zpool.c: * zpool_register_driver() - register a zpool implementation.
zpool.c:	atomic_set(&driver->refcount, 0);
zpool.c:	list_add(&driver->list, &drivers_head);
zpool.c: * zpool_unregister_driver() - unregister a zpool implementation.
zpool.c:	refcount = atomic_read(&driver->refcount);
zpool.c:		ret = -EBUSY;
zpool.c:		list_del(&driver->list);
zpool.c:/* this assumes @type is null-terminated. */
zpool.c:		if (!strcmp(driver->type, type)) {
zpool.c:			bool got = try_module_get(driver->owner);
zpool.c:				atomic_inc(&driver->refcount);
zpool.c:	atomic_dec(&driver->refcount);
zpool.c:	module_put(driver->owner);
zpool.c: * zpool_has_pool() - Check if the pool driver is available
zpool.c: * The @type string must be null-terminated.
zpool.c:		request_module("zpool-%s", type);
zpool.c: * zpool_create_pool() - Create a new zpool
zpool.c: * Implementations must guarantee this to be thread-safe.
zpool.c: * The @type and @name strings must be null-terminated.
zpool.c:		request_module("zpool-%s", type);
zpool.c:		pr_err("couldn't create zpool - out of memory\n");
zpool.c:	zpool->driver = driver;
zpool.c:	zpool->pool = driver->create(name, gfp, ops, zpool);
zpool.c:	zpool->ops = ops;
zpool.c:	if (!zpool->pool) {
zpool.c:	list_add(&zpool->list, &pools_head);
zpool.c: * zpool_destroy_pool() - Destroy a zpool
zpool.c: * Implementations must guarantee this to be thread-safe,
zpool.c:	pr_debug("destroying pool type %s\n", zpool->driver->type);
zpool.c:	list_del(&zpool->list);
zpool.c:	zpool->driver->destroy(zpool->pool);
zpool.c:	zpool_put_driver(zpool->driver);
zpool.c: * zpool_get_type() - Get the type of the zpool
zpool.c: * Implementations must guarantee this to be thread-safe.
zpool.c:	return zpool->driver->type;
zpool.c: * zpool_malloc() - Allocate memory
zpool.c: * Implementations must guarantee this to be thread-safe.
zpool.c:	return zpool->driver->malloc(zpool->pool, size, gfp, handle);
zpool.c: * zpool_free() - Free previously allocated memory
zpool.c: * Implementations must guarantee this to be thread-safe,
zpool.c:	zpool->driver->free(zpool->pool, handle);
zpool.c: * zpool_shrink() - Shrink the pool size
zpool.c: * of the handles, this will fail.  If non-NULL, the @reclaimed
zpool.c: * Implementations must guarantee this to be thread-safe.
zpool.c:	return zpool->driver->shrink(zpool->pool, pages, reclaimed);
zpool.c: * zpool_map_handle() - Map a previously allocated handle into memory
zpool.c: * used, i.e. read-only, write-only, read-write.  If the
zpool.c: * as read-write.
zpool.c: * as soon as possible.  As the implementation may use per-cpu
zpool.c:	return zpool->driver->map(zpool->pool, handle, mapmode);
zpool.c: * zpool_unmap_handle() - Unmap a previously mapped handle
zpool.c:	zpool->driver->unmap(zpool->pool, handle);
zpool.c: * zpool_get_total_size() - The total size of the pool
zpool.c:	return zpool->driver->total_size(zpool->pool);
mmu_notifier.c: *  the COPYING file in the top-level directory.
mmu_notifier.c: * because mm->mm_users > 0 during mmu_notifier_register and exit_mmap
mmu_notifier.c: * the mmu_notifier_mm->lock in addition to SRCU and it serializes
mmu_notifier.c:	 * ->release returns.
mmu_notifier.c:	hlist_for_each_entry_rcu(mn, &mm->mmu_notifier_mm->list, hlist)
mmu_notifier.c:		 * If ->release runs before mmu_notifier_unregister it must be
mmu_notifier.c:		if (mn->ops->release)
mmu_notifier.c:			mn->ops->release(mn, mm);
mmu_notifier.c:	spin_lock(&mm->mmu_notifier_mm->lock);
mmu_notifier.c:	while (unlikely(!hlist_empty(&mm->mmu_notifier_mm->list))) {
mmu_notifier.c:		mn = hlist_entry(mm->mmu_notifier_mm->list.first,
mmu_notifier.c:		 * for ->release to finish and for mmu_notifier_unregister to
mmu_notifier.c:		hlist_del_init_rcu(&mn->hlist);
mmu_notifier.c:	spin_unlock(&mm->mmu_notifier_mm->lock);
mmu_notifier.c:	 * until the ->release method returns, if it was invoked by
mmu_notifier.c: * If no young bitflag is supported by the hardware, ->clear_flush_young can
mmu_notifier.c:	hlist_for_each_entry_rcu(mn, &mm->mmu_notifier_mm->list, hlist) {
mmu_notifier.c:		if (mn->ops->clear_flush_young)
mmu_notifier.c:			young |= mn->ops->clear_flush_young(mn, mm, start, end);
mmu_notifier.c:	hlist_for_each_entry_rcu(mn, &mm->mmu_notifier_mm->list, hlist) {
mmu_notifier.c:		if (mn->ops->clear_young)
mmu_notifier.c:			young |= mn->ops->clear_young(mn, mm, start, end);
mmu_notifier.c:	hlist_for_each_entry_rcu(mn, &mm->mmu_notifier_mm->list, hlist) {
mmu_notifier.c:		if (mn->ops->test_young) {
mmu_notifier.c:			young = mn->ops->test_young(mn, mm, address);
mmu_notifier.c:	hlist_for_each_entry_rcu(mn, &mm->mmu_notifier_mm->list, hlist) {
mmu_notifier.c:		if (mn->ops->change_pte)
mmu_notifier.c:			mn->ops->change_pte(mn, mm, address, pte);
mmu_notifier.c:	hlist_for_each_entry_rcu(mn, &mm->mmu_notifier_mm->list, hlist) {
mmu_notifier.c:		if (mn->ops->invalidate_range_start)
mmu_notifier.c:			mn->ops->invalidate_range_start(mn, mm, start, end);
mmu_notifier.c:	hlist_for_each_entry_rcu(mn, &mm->mmu_notifier_mm->list, hlist) {
mmu_notifier.c:		 * call-back when there is invalidate_range already. Usually a
mmu_notifier.c:		if (mn->ops->invalidate_range)
mmu_notifier.c:			mn->ops->invalidate_range(mn, mm, start, end);
mmu_notifier.c:		if (mn->ops->invalidate_range_end)
mmu_notifier.c:			mn->ops->invalidate_range_end(mn, mm, start, end);
mmu_notifier.c:	hlist_for_each_entry_rcu(mn, &mm->mmu_notifier_mm->list, hlist) {
mmu_notifier.c:		if (mn->ops->invalidate_range)
mmu_notifier.c:			mn->ops->invalidate_range(mn, mm, start, end);
mmu_notifier.c:	BUG_ON(atomic_read(&mm->mm_users) <= 0);
mmu_notifier.c:	ret = -ENOMEM;
mmu_notifier.c:		down_write(&mm->mmap_sem);
mmu_notifier.c:		INIT_HLIST_HEAD(&mmu_notifier_mm->list);
mmu_notifier.c:		spin_lock_init(&mmu_notifier_mm->lock);
mmu_notifier.c:		mm->mmu_notifier_mm = mmu_notifier_mm;
mmu_notifier.c:	 * current->mm or explicitly with get_task_mm() or similar).
mmu_notifier.c:	spin_lock(&mm->mmu_notifier_mm->lock);
mmu_notifier.c:	hlist_add_head(&mn->hlist, &mm->mmu_notifier_mm->list);
mmu_notifier.c:	spin_unlock(&mm->mmu_notifier_mm->lock);
mmu_notifier.c:		up_write(&mm->mmap_sem);
mmu_notifier.c:	BUG_ON(atomic_read(&mm->mm_users) <= 0);
mmu_notifier.c: * so mm has to be current->mm or the mm should be pinned safely such
mmu_notifier.c: * as with get_task_mm(). If the mm is not current->mm, the mm_users
mmu_notifier.c: * after exit_mmap. ->release will always be called before exit_mmap
mmu_notifier.c:	BUG_ON(!hlist_empty(&mm->mmu_notifier_mm->list));
mmu_notifier.c:	kfree(mm->mmu_notifier_mm);
mmu_notifier.c:	mm->mmu_notifier_mm = LIST_POISON1; /* debug */
mmu_notifier.c: * calling mmu_notifier_unregister. ->release or any other notifier
mmu_notifier.c: * that ->release or any other method can't run anymore.
mmu_notifier.c:	BUG_ON(atomic_read(&mm->mm_count) <= 0);
mmu_notifier.c:	if (!hlist_unhashed(&mn->hlist)) {
mmu_notifier.c:		 * SRCU here will force exit_mmap to wait for ->release to
mmu_notifier.c:		 * that ->release is called before freeing the pages.
mmu_notifier.c:		if (mn->ops->release)
mmu_notifier.c:			mn->ops->release(mn, mm);
mmu_notifier.c:		spin_lock(&mm->mmu_notifier_mm->lock);
mmu_notifier.c:		hlist_del_init_rcu(&mn->hlist);
mmu_notifier.c:		spin_unlock(&mm->mmu_notifier_mm->lock);
mmu_notifier.c:	 * ->release if it was run by mmu_notifier_release instead of us.
mmu_notifier.c:	BUG_ON(atomic_read(&mm->mm_count) <= 0);
mmu_notifier.c:	spin_lock(&mm->mmu_notifier_mm->lock);
mmu_notifier.c:	hlist_del_init_rcu(&mn->hlist);
mmu_notifier.c:	spin_unlock(&mm->mmu_notifier_mm->lock);
mmu_notifier.c:	BUG_ON(atomic_read(&mm->mm_count) <= 0);
vmacache.c:// SPDX-License-Identifier: GPL-2.0
vmacache.c:	if (atomic_read(&mm->mm_users) == 1)
vmacache.c:		if (mm == p->mm)
vmacache.c: * get_user_pages()->find_vma().  The vmacache is task-local and this
vmacache.c:	return current->mm == mm && !(current->flags & PF_KTHREAD);
vmacache.c:	if (vmacache_valid_mm(newvma->vm_mm))
vmacache.c:		current->vmacache.vmas[VMACACHE_HASH(addr)] = newvma;
vmacache.c:	if (mm->vmacache_seqnum != curr->vmacache.seqnum) {
vmacache.c:		curr->vmacache.seqnum = mm->vmacache_seqnum;
vmacache.c:		struct vm_area_struct *vma = current->vmacache.vmas[i];
vmacache.c:		if (WARN_ON_ONCE(vma->vm_mm != mm))
vmacache.c:		if (vma->vm_start <= addr && vma->vm_end > addr) {
vmacache.c:		struct vm_area_struct *vma = current->vmacache.vmas[i];
vmacache.c:		if (vma && vma->vm_start == start && vma->vm_end == end) {
slab.h:/* SPDX-License-Identifier: GPL-2.0 */
slab.h:#include <linux/fault-inject.h>
slab.h:	list_for_each_entry(iter, &(root)->memcg_params.children, \
slab.h:	return !s->memcg_params.root_cache;
slab.h:	return p == s || p == s->memcg_params.root_cache;
slab.h:		s = s->memcg_params.root_cache;
slab.h:	return s->name;
slab.h: * Note, we protect with RCU only the memcg_caches array, not per-memcg caches.
slab.h:	arr = rcu_dereference(s->memcg_params.memcg_caches);
slab.h:	 * Make sure we will access the up-to-date value. The code updating
slab.h:	cachep = READ_ONCE(arr->entries[idx]);
slab.h:	return s->memcg_params.root_cache;
slab.h:	return memcg_kmem_charge_memcg(page, gfp, order, s->memcg_params.memcg);
slab.h:	return s->name;
slab.h:	    !unlikely(s->flags & SLAB_CONSISTENCY_CHECKS))
slab.h:	cachep = page->slab_cache;
slab.h:	       __func__, s->name, cachep->name);
slab.h:	return s->object_size;
slab.h:	if (s->flags & (SLAB_RED_ZONE | SLAB_POISON))
slab.h:		return s->object_size;
slab.h:	if (s->flags & SLAB_KASAN)
slab.h:		return s->object_size;
slab.h:	if (s->flags & (SLAB_TYPESAFE_BY_RCU | SLAB_STORE_USER))
slab.h:		return s->inuse;
slab.h:	return s->size;
slab.h:	    ((flags & __GFP_ACCOUNT) || (s->flags & SLAB_ACCOUNT)))
slab.h:		kmemleak_alloc_recursive(object, s->object_size, 1,
slab.h:					 s->flags, flags);
slab.h:	unsigned int colour_next;	/* Per-node cache coloring */
slab.h:	return s->node[node];
vmalloc.c: *  SMP-safe vmalloc/vfree/ioremap, Tigran Aivazian <tigran@veritas.com>, May 2000
vmalloc.c:#include <linux/radix-tree.h>
vmalloc.c:	llist_for_each_safe(llnode, t, llist_del_all(&p->list))
vmalloc.c:		return -ENOMEM;
vmalloc.c:			return -EBUSY;
vmalloc.c:			return -ENOMEM;
vmalloc.c:		return -ENOMEM;
vmalloc.c:			return -ENOMEM;
vmalloc.c:		return -ENOMEM;
vmalloc.c:			return -ENOMEM;
vmalloc.c:		return -ENOMEM;
vmalloc.c:			return -ENOMEM;
vmalloc.c:	 * ARM, x86-64 and sparc64 put modules in a special place,
vmalloc.c: * Map a vmalloc()-space virtual address to the physical page frame number.
vmalloc.c:		if (addr < va->va_start)
vmalloc.c:			n = n->rb_left;
vmalloc.c:		else if (addr >= va->va_end)
vmalloc.c:			n = n->rb_right;
vmalloc.c:		if (va->va_start < tmp_va->va_end)
vmalloc.c:			p = &(*p)->rb_left;
vmalloc.c:		else if (va->va_end > tmp_va->va_start)
vmalloc.c:			p = &(*p)->rb_right;
vmalloc.c:	rb_link_node(&va->rb_node, parent, p);
vmalloc.c:	rb_insert_color(&va->rb_node, &vmap_area_root);
vmalloc.c:	/* address-sort this list */
vmalloc.c:	tmp = rb_prev(&va->rb_node);
vmalloc.c:		list_add_rcu(&va->list, &prev->list);
vmalloc.c:		list_add_rcu(&va->list, &vmap_area_list);
vmalloc.c:		return ERR_PTR(-ENOMEM);
vmalloc.c:	kmemleak_scan_area(&va->rb_node, SIZE_MAX, gfp_mask & GFP_RECLAIM_MASK);
vmalloc.c:		addr = ALIGN(first->va_end, align);
vmalloc.c:			if (tmp->va_end >= addr) {
vmalloc.c:				if (tmp->va_start <= addr)
vmalloc.c:				n = n->rb_left;
vmalloc.c:				n = n->rb_right;
vmalloc.c:	while (addr + size > first->va_start && addr + size <= vend) {
vmalloc.c:		if (addr + cached_hole_size < first->va_start)
vmalloc.c:			cached_hole_size = first->va_start - addr;
vmalloc.c:		addr = ALIGN(first->va_end, align);
vmalloc.c:		if (list_is_last(&first->list, &vmap_area_list))
vmalloc.c:	va->va_start = addr;
vmalloc.c:	va->va_end = addr + size;
vmalloc.c:	va->flags = 0;
vmalloc.c:	free_vmap_cache = &va->rb_node;
vmalloc.c:	BUG_ON(!IS_ALIGNED(va->va_start, align));
vmalloc.c:	BUG_ON(va->va_start < vstart);
vmalloc.c:	BUG_ON(va->va_end > vend);
vmalloc.c:	return ERR_PTR(-EBUSY);
vmalloc.c:	BUG_ON(RB_EMPTY_NODE(&va->rb_node));
vmalloc.c:		if (va->va_end < cached_vstart) {
vmalloc.c:			if (va->va_start <= cache->va_start) {
vmalloc.c:				free_vmap_cache = rb_prev(&va->rb_node);
vmalloc.c:	rb_erase(&va->rb_node, &vmap_area_root);
vmalloc.c:	RB_CLEAR_NODE(&va->rb_node);
vmalloc.c:	list_del_rcu(&va->list);
vmalloc.c:	if (va->va_end > VMALLOC_START && va->va_end <= VMALLOC_END)
vmalloc.c:		vmap_area_pcpu_hole = max(vmap_area_pcpu_hole, va->va_end);
vmalloc.c:	vunmap_page_range(va->va_start, va->va_end);
vmalloc.c:/* for per-CPU blocks */
vmalloc.c: * Purges all lazily-freed vmap areas.
vmalloc.c:		if (va->va_start < start)
vmalloc.c:			start = va->va_start;
vmalloc.c:		if (va->va_end > end)
vmalloc.c:			end = va->va_end;
vmalloc.c:		int nr = (va->va_end - va->va_start) >> PAGE_SHIFT;
vmalloc.c:	nr_lazy = atomic_add_return((va->va_end - va->va_start) >> PAGE_SHIFT,
vmalloc.c:	llist_add(&va->purge_list, &vmap_purge_list);
vmalloc.c:	flush_cache_vunmap(va->va_start, va->va_end);
vmalloc.c: * to #define VMALLOC_SPACE		(VMALLOC_END-VMALLOC_START). Guess
vmalloc.c:	addr -= VMALLOC_START & ~(VMAP_BLOCK_SIZE-1);
vmalloc.c: * new_vmap_block - allocates new vmap_block and occupies 2^order pages in this
vmalloc.c: * Returns: virtual address in a newly allocated block or ERR_PTR(-errno)
vmalloc.c:		return ERR_PTR(-ENOMEM);
vmalloc.c:	vaddr = vmap_block_vaddr(va->va_start, 0);
vmalloc.c:	spin_lock_init(&vb->lock);
vmalloc.c:	vb->va = va;
vmalloc.c:	vb->free = VMAP_BBMAP_BITS - (1UL << order);
vmalloc.c:	vb->dirty = 0;
vmalloc.c:	vb->dirty_min = VMAP_BBMAP_BITS;
vmalloc.c:	vb->dirty_max = 0;
vmalloc.c:	INIT_LIST_HEAD(&vb->free_list);
vmalloc.c:	vb_idx = addr_to_vb_idx(va->va_start);
vmalloc.c:	spin_lock(&vbq->lock);
vmalloc.c:	list_add_tail_rcu(&vb->free_list, &vbq->free);
vmalloc.c:	spin_unlock(&vbq->lock);
vmalloc.c:	vb_idx = addr_to_vb_idx(vb->va->va_start);
vmalloc.c:	free_vmap_area_noflush(vb->va);
vmalloc.c:	list_for_each_entry_rcu(vb, &vbq->free, free_list) {
vmalloc.c:		if (!(vb->free + vb->dirty == VMAP_BBMAP_BITS && vb->dirty != VMAP_BBMAP_BITS))
vmalloc.c:		spin_lock(&vb->lock);
vmalloc.c:		if (vb->free + vb->dirty == VMAP_BBMAP_BITS && vb->dirty != VMAP_BBMAP_BITS) {
vmalloc.c:			vb->free = 0; /* prevent further allocs after releasing lock */
vmalloc.c:			vb->dirty = VMAP_BBMAP_BITS; /* prevent purging it again */
vmalloc.c:			vb->dirty_min = 0;
vmalloc.c:			vb->dirty_max = VMAP_BBMAP_BITS;
vmalloc.c:			spin_lock(&vbq->lock);
vmalloc.c:			list_del_rcu(&vb->free_list);
vmalloc.c:			spin_unlock(&vbq->lock);
vmalloc.c:			spin_unlock(&vb->lock);
vmalloc.c:			list_add_tail(&vb->purge, &purge);
vmalloc.c:			spin_unlock(&vb->lock);
vmalloc.c:		list_del(&vb->purge);
vmalloc.c:	list_for_each_entry_rcu(vb, &vbq->free, free_list) {
vmalloc.c:		spin_lock(&vb->lock);
vmalloc.c:		if (vb->free < (1UL << order)) {
vmalloc.c:			spin_unlock(&vb->lock);
vmalloc.c:		pages_off = VMAP_BBMAP_BITS - vb->free;
vmalloc.c:		vaddr = vmap_block_vaddr(vb->va->va_start, pages_off);
vmalloc.c:		vb->free -= 1UL << order;
vmalloc.c:		if (vb->free == 0) {
vmalloc.c:			spin_lock(&vbq->lock);
vmalloc.c:			list_del_rcu(&vb->free_list);
vmalloc.c:			spin_unlock(&vbq->lock);
vmalloc.c:		spin_unlock(&vb->lock);
vmalloc.c:	offset = (unsigned long)addr & (VMAP_BLOCK_SIZE - 1);
vmalloc.c:	spin_lock(&vb->lock);
vmalloc.c:	vb->dirty_min = min(vb->dirty_min, offset);
vmalloc.c:	vb->dirty_max = max(vb->dirty_max, offset + (1UL << order));
vmalloc.c:	vb->dirty += 1UL << order;
vmalloc.c:	if (vb->dirty == VMAP_BBMAP_BITS) {
vmalloc.c:		BUG_ON(vb->free);
vmalloc.c:		spin_unlock(&vb->lock);
vmalloc.c:		spin_unlock(&vb->lock);
vmalloc.c: * vm_unmap_aliases - unmap outstanding lazy aliases in the vmap layer
vmalloc.c:		list_for_each_entry_rcu(vb, &vbq->free, free_list) {
vmalloc.c:			spin_lock(&vb->lock);
vmalloc.c:			if (vb->dirty) {
vmalloc.c:				unsigned long va_start = vb->va->va_start;
vmalloc.c:				s = va_start + (vb->dirty_min << PAGE_SHIFT);
vmalloc.c:				e = va_start + (vb->dirty_max << PAGE_SHIFT);
vmalloc.c:			spin_unlock(&vb->lock);
vmalloc.c: * vm_unmap_ram - unmap linear kernel address space set up by vm_map_ram
vmalloc.c: * vm_map_ram - map pages linearly into kernel virtual address (vmalloc space)
vmalloc.c: * faster than vmap so it's good.  But if you mix long-life and short-life
vmalloc.c: * the end.  Please use this function for short-lived objects.
vmalloc.c:		addr = va->va_start;
vmalloc.c: * vm_area_add_early - add vmap area early during boot
vmalloc.c: * vmalloc_init() is called.  @vm->addr, @vm->size, and @vm->flags
vmalloc.c:	for (p = &vmlist; (tmp = *p) != NULL; p = &tmp->next) {
vmalloc.c:		if (tmp->addr >= vm->addr) {
vmalloc.c:			BUG_ON(tmp->addr < vm->addr + vm->size);
vmalloc.c:			BUG_ON(tmp->addr + tmp->size > vm->addr);
vmalloc.c:	vm->next = *p;
vmalloc.c: * vm_area_register_early - register vmap area early during boot
vmalloc.c: * vmalloc_init() is called.  @vm->size and @vm->flags should contain
vmalloc.c: * vm->addr contains the allocated address.
vmalloc.c:	vm_init_off = PFN_ALIGN(addr + vm->size) - VMALLOC_START;
vmalloc.c:	vm->addr = (void *)addr;
vmalloc.c:		spin_lock_init(&vbq->lock);
vmalloc.c:		INIT_LIST_HEAD(&vbq->free);
vmalloc.c:		init_llist_head(&p->list);
vmalloc.c:		INIT_WORK(&p->wq, free_work);
vmalloc.c:	for (tmp = vmlist; tmp; tmp = tmp->next) {
vmalloc.c:		va->flags = VM_VM_AREA;
vmalloc.c:		va->va_start = (unsigned long)tmp->addr;
vmalloc.c:		va->va_end = va->va_start + tmp->size;
vmalloc.c:		va->vm = tmp;
vmalloc.c: * map_kernel_range_noflush - map kernel VM area with the specified pages
vmalloc.c: * responsible for calling flush_cache_vmap() on to-be-mapped areas
vmalloc.c: * The number of pages mapped on success, -errno on failure.
vmalloc.c: * unmap_kernel_range_noflush - unmap kernel VM area
vmalloc.c: * responsible for calling flush_cache_vunmap() on to-be-mapped areas
vmalloc.c: * unmap_kernel_range - unmap kernel VM area and flush cache and TLB
vmalloc.c:	unsigned long addr = (unsigned long)area->addr;
vmalloc.c:	vm->flags = flags;
vmalloc.c:	vm->addr = (void *)va->va_start;
vmalloc.c:	vm->size = va->va_end - va->va_start;
vmalloc.c:	vm->caller = caller;
vmalloc.c:	va->vm = vm;
vmalloc.c:	va->flags |= VM_VM_AREA;
vmalloc.c:	vm->flags &= ~VM_UNINITIALIZED;
vmalloc.c: *	get_vm_area  -  reserve a contiguous kernel virtual area
vmalloc.c: *	find_vm_area  -  find a continuous kernel virtual area
vmalloc.c:	if (va && va->flags & VM_VM_AREA)
vmalloc.c:		return va->vm;
vmalloc.c: *	remove_vm_area  -  find and remove a continuous kernel virtual area
vmalloc.c:	if (va && va->flags & VM_VM_AREA) {
vmalloc.c:		struct vm_struct *vm = va->vm;
vmalloc.c:		va->vm = NULL;
vmalloc.c:		va->flags &= ~VM_VM_AREA;
vmalloc.c:		va->flags |= VM_LAZY_FREE;
vmalloc.c:		vmap_debug_free_range(va->va_start, va->va_end);
vmalloc.c:		for (i = 0; i < area->nr_pages; i++) {
vmalloc.c:			struct page *page = area->pages[i];
vmalloc.c:		kvfree(area->pages);
vmalloc.c:	if (llist_add((struct llist_node *)addr, &p->list))
vmalloc.c:		schedule_work(&p->wq);
vmalloc.c: *	vfree_atomic  -  release memory allocated by vmalloc()
vmalloc.c: *	vfree  -  release memory allocated by vmalloc()
vmalloc.c: *	conventions for vfree() arch-depenedent would be a really bad idea)
vmalloc.c: *	vunmap  -  release virtual mapping obtained by vmap()
vmalloc.c: *	vmap  -  map an array of pages into virtually contiguous space
vmalloc.c: *	@flags:		vm_area->flags
vmalloc.c:		vunmap(area->addr);
vmalloc.c:	return area->addr;
vmalloc.c:	area->nr_pages = nr_pages;
vmalloc.c:				PAGE_KERNEL, node, area->caller);
vmalloc.c:	area->pages = pages;
vmalloc.c:	if (!area->pages) {
vmalloc.c:		remove_vm_area(area->addr);
vmalloc.c:	for (i = 0; i < area->nr_pages; i++) {
vmalloc.c:			area->nr_pages = i;
vmalloc.c:		area->pages[i] = page;
vmalloc.c:	return area->addr;
vmalloc.c:			  (area->nr_pages*PAGE_SIZE), area->size);
vmalloc.c:	vfree(area->addr);
vmalloc.c: *	__vmalloc_node_range  -  allocate virtually contiguous memory
vmalloc.c: *	__vmalloc_node  -  allocate virtually contiguous memory
vmalloc.c: *	Reclaim modifiers in @gfp_mask - __GFP_NORETRY, __GFP_RETRY_MAYFAIL
vmalloc.c: *	vmalloc  -  allocate virtually contiguous memory
vmalloc.c: *	vzalloc - allocate virtually contiguous memory with zero fill
vmalloc.c: * vmalloc_user - allocate zeroed virtually contiguous memory for userspace
vmalloc.c:		area->flags |= VM_USERMAP;
vmalloc.c: *	vmalloc_node  -  allocate memory on a specific node
vmalloc.c: * vzalloc_node - allocate memory on a specific node with zero fill
vmalloc.c: *	vmalloc_exec  -  allocate virtually contiguous, executable memory
vmalloc.c: *	Kernel-internal function to allocate enough pages to cover @size
vmalloc.c: *	vmalloc_32  -  allocate virtually contiguous memory (32bit addressable)
vmalloc.c: * vmalloc_32_user - allocate zeroed virtually contiguous 32bit memory
vmalloc.c:		area->flags |= VM_USERMAP;
vmalloc.c:		length = PAGE_SIZE - offset;
vmalloc.c:		count -= length;
vmalloc.c:		length = PAGE_SIZE - offset;
vmalloc.c:		count -= length;
vmalloc.c: *	vread() -  read vmalloc area in a safe way.
vmalloc.c: *	proper area of @buf. If there are memory holes, they'll be zero-filled.
vmalloc.c:		count = -(unsigned long) addr;
vmalloc.c:		if (!(va->flags & VM_VM_AREA))
vmalloc.c:		vm = va->vm;
vmalloc.c:		vaddr = (char *) vm->addr;
vmalloc.c:			count--;
vmalloc.c:		n = vaddr + get_vm_area_size(vm) - addr;
vmalloc.c:		if (!(vm->flags & VM_IOREMAP))
vmalloc.c:		count -= n;
vmalloc.c:	/* zero-fill memory holes */
vmalloc.c:		memset(buf, 0, buflen - (buf - buf_start));
vmalloc.c: *	vwrite() -  write vmalloc area in a safe way.
vmalloc.c:		count = -(unsigned long) addr;
vmalloc.c:		if (!(va->flags & VM_VM_AREA))
vmalloc.c:		vm = va->vm;
vmalloc.c:		vaddr = (char *) vm->addr;
vmalloc.c:			count--;
vmalloc.c:		n = vaddr + get_vm_area_size(vm) - addr;
vmalloc.c:		if (!(vm->flags & VM_IOREMAP)) {
vmalloc.c:		count -= n;
vmalloc.c: *	remap_vmalloc_range_partial  -  map vmalloc pages to userspace
vmalloc.c: *	Returns:	0 for success, -Exxx on failure
vmalloc.c:		return -EINVAL;
vmalloc.c:		return -EINVAL;
vmalloc.c:	if (!(area->flags & VM_USERMAP))
vmalloc.c:		return -EINVAL;
vmalloc.c:	if (kaddr + size > area->addr + area->size)
vmalloc.c:		return -EINVAL;
vmalloc.c:		size -= PAGE_SIZE;
vmalloc.c:	vma->vm_flags |= VM_DONTEXPAND | VM_DONTDUMP;
vmalloc.c: *	remap_vmalloc_range  -  map vmalloc pages to userspace
vmalloc.c: *	Returns:	0 for success, -Exxx on failure
vmalloc.c:	return remap_vmalloc_range_partial(vma, vma->vm_start,
vmalloc.c:					   vma->vm_end - vma->vm_start);
vmalloc.c: *	alloc_vm_area - allocate a range of kernel address space
vmalloc.c: *	If @ptes is non-NULL, pointers to the PTEs (in init_mm)
vmalloc.c:	if (apply_to_page_range(&init_mm, (unsigned long)area->addr,
vmalloc.c:	ret = remove_vm_area(area->addr);
vmalloc.c: * pvm_find_next_prev - find the next and prev vmap_area surrounding @end
vmalloc.c: * NULL, *pnext->va_end > @end and *pprev->va_end <= @end.
vmalloc.c:		if (end < va->va_end)
vmalloc.c:			n = n->rb_left;
vmalloc.c:		else if (end > va->va_end)
vmalloc.c:			n = n->rb_right;
vmalloc.c:	if (va->va_end > end) {
vmalloc.c:		*pprev = node_to_va(rb_prev(&(*pnext)->rb_node));
vmalloc.c:		*pnext = node_to_va(rb_next(&(*pprev)->rb_node));
vmalloc.c: * pvm_determine_end - find the highest aligned address between two vmap_areas
vmalloc.c:	const unsigned long vmalloc_end = VMALLOC_END & ~(align - 1);
vmalloc.c:		addr = min((*pnext)->va_start & ~(align - 1), vmalloc_end);
vmalloc.c:	while (*pprev && (*pprev)->va_end > addr) {
vmalloc.c:		*pprev = node_to_va(rb_prev(&(*pnext)->rb_node));
vmalloc.c: * pcpu_get_vm_areas - allocate vmalloc areas for percpu allocator
vmalloc.c: * does everything top-down and scans areas from the end looking for
vmalloc.c:	const unsigned long vmalloc_end = VMALLOC_END & ~(align - 1);
vmalloc.c:	if (vmalloc_end - vmalloc_start < last_end) {
vmalloc.c:	/* start scanning - we scan from the top, begin with the last area */
vmalloc.c:		base = vmalloc_end - last_end;
vmalloc.c:	base = pvm_determine_end(&next, &prev, align) - end;
vmalloc.c:		BUG_ON(next && next->va_end <= base + end);
vmalloc.c:		BUG_ON(prev && prev->va_end > base + end);
vmalloc.c:		if (next && next->va_start < base + end) {
vmalloc.c:			base = pvm_determine_end(&next, &prev, align) - end;
vmalloc.c:		if (prev && prev->va_end > base + start)  {
vmalloc.c:			prev = node_to_va(rb_prev(&next->rb_node));
vmalloc.c:			base = pvm_determine_end(&next, &prev, align) - end;
vmalloc.c:		area = (area + nr_vms - 1) % nr_vms;
vmalloc.c:		va->va_start = base + offsets[area];
vmalloc.c:		va->va_end = va->va_start + sizes[area];
vmalloc.c: * pcpu_free_vm_areas - free vmalloc areas for percpu allocator
vmalloc.c:		unsigned int nr, *counters = m->private;
vmalloc.c:		if (v->flags & VM_UNINITIALIZED)
vmalloc.c:		for (nr = 0; nr < v->nr_pages; nr++)
vmalloc.c:			counters[page_to_nid(v->pages[nr])]++;
vmalloc.c:	if (!(va->flags & VM_VM_AREA)) {
vmalloc.c:		seq_printf(m, "0x%pK-0x%pK %7ld %s\n",
vmalloc.c:			(void *)va->va_start, (void *)va->va_end,
vmalloc.c:			va->va_end - va->va_start,
vmalloc.c:			va->flags & VM_LAZY_FREE ? "unpurged vm_area" : "vm_map_ram");
vmalloc.c:	v = va->vm;
vmalloc.c:	seq_printf(m, "0x%pK-0x%pK %7ld",
vmalloc.c:		v->addr, v->addr + v->size, v->size);
vmalloc.c:	if (v->caller)
vmalloc.c:		seq_printf(m, " %pS", v->caller);
vmalloc.c:	if (v->nr_pages)
vmalloc.c:		seq_printf(m, " pages=%d", v->nr_pages);
vmalloc.c:	if (v->phys_addr)
vmalloc.c:		seq_printf(m, " phys=%pa", &v->phys_addr);
vmalloc.c:	if (v->flags & VM_IOREMAP)
vmalloc.c:	if (v->flags & VM_ALLOC)
vmalloc.c:	if (v->flags & VM_MAP)
vmalloc.c:	if (v->flags & VM_USERMAP)
vmalloc.c:	if (is_vmalloc_addr(v->pages))
sparse-vmemmap.c:// SPDX-License-Identifier: GPL-2.0
sparse-vmemmap.c: * architectures already map their physical space using 1-1 mappings
sparse-vmemmap.c: * for free if we use the same page size as the 1-1 mappings. In that
sparse-vmemmap.c:	return altmap->base_pfn + altmap->reserve + altmap->alloc
sparse-vmemmap.c:		+ altmap->align;
sparse-vmemmap.c:	unsigned long allocated = altmap->alloc + altmap->align;
sparse-vmemmap.c:	if (altmap->free > allocated)
sparse-vmemmap.c:		return altmap->free - allocated;
sparse-vmemmap.c: * vmem_altmap_alloc - allocate pages from the vmem_altmap reservation
sparse-vmemmap.c: * @altmap - reserved page pool for the allocation
sparse-vmemmap.c: * @nr_pfns - size (in pages) of the allocation
sparse-vmemmap.c:	nr_align = ALIGN(pfn, nr_align) - pfn;
sparse-vmemmap.c:	altmap->alloc += nr_pfns;
sparse-vmemmap.c:	altmap->align += nr_align;
sparse-vmemmap.c:			__func__, pfn, altmap->alloc, altmap->align, nr_pfns);
sparse-vmemmap.c:		pr_warn("[%lx-%lx] potential offnode page_structs\n",
sparse-vmemmap.c:			start, end - 1);
sparse-vmemmap.c:			return -ENOMEM;
sparse-vmemmap.c:			return -ENOMEM;
sparse-vmemmap.c:			return -ENOMEM;
sparse-vmemmap.c:			return -ENOMEM;
sparse-vmemmap.c:			return -ENOMEM;
sparse-vmemmap.c:		ms->section_mem_map = 0;
sparse-vmemmap.c:				    vmemmap_buf_end - vmemmap_buf);
process_vm_access.c: * Copyright (C) 2010-2011 Christopher Yeoh <cyeoh@au1.ibm.com>, IBM Corp.
process_vm_access.c: * process_vm_rw_pages - read/write pages from task specified
process_vm_access.c:		size_t copy = PAGE_SIZE - offset;
process_vm_access.c:		len -= copied;
process_vm_access.c:			return -EFAULT;
process_vm_access.c: * process_vm_rw_single_vec - read/write pages from task specified
process_vm_access.c:	unsigned long start_offset = addr - pa;
process_vm_access.c:	nr_pages = (addr + len - 1) / PAGE_SIZE - addr / PAGE_SIZE + 1;
process_vm_access.c:		 * current/current->mm
process_vm_access.c:		down_read(&mm->mmap_sem);
process_vm_access.c:			up_read(&mm->mmap_sem);
process_vm_access.c:			return -EFAULT;
process_vm_access.c:		bytes = pages * PAGE_SIZE - start_offset;
process_vm_access.c:		len -= bytes;
process_vm_access.c:		nr_pages -= pages;
process_vm_access.c:			put_page(process_pages[--pages]);
process_vm_access.c: * process_vm_rw_core - core of reading/writing pages from task specified
process_vm_access.c:				/ PAGE_SIZE - (unsigned long)rvec[i].iov_base
process_vm_access.c:			return -ENOMEM;
process_vm_access.c:		rc = -ESRCH;
process_vm_access.c:		rc = IS_ERR(mm) ? PTR_ERR(mm) : -ESRCH;
process_vm_access.c:		if (rc == -EACCES)
process_vm_access.c:			rc = -EPERM;
process_vm_access.c:	/* copied = space before - space after */
process_vm_access.c:	total_len -= iov_iter_count(iter);
process_vm_access.c: * process_vm_rw - check iovecs before calling core routine
process_vm_access.c:		return -EINVAL;
process_vm_access.c:	ssize_t rc = -EFAULT;
process_vm_access.c:		return -EINVAL;
page_idle.c:// SPDX-License-Identifier: GPL-2.0
page_idle.c:			 * For PTE-mapped THP, one sub page is referenced,
page_idle.c:			/* unexpected pmd-mapped page? */
page_idle.c:		return -EINVAL;
page_idle.c:		if (bit == BITMAP_CHUNK_BITS - 1)
page_idle.c:	return (char *)out - buf;
page_idle.c:		return -EINVAL;
page_idle.c:		return -ENXIO;
page_idle.c:		if (bit == BITMAP_CHUNK_BITS - 1)
page_idle.c:	return (char *)in - buf;
percpu-internal.h:/* SPDX-License-Identifier: GPL-2.0 */
percpu-internal.h: * pcpu_chunk_nr_blocks - converts nr_pages to # of md_blocks
percpu-internal.h:	return chunk->nr_pages * PAGE_SIZE / PCPU_BITMAP_BLOCK_SIZE;
percpu-internal.h: * pcpu_nr_pages_to_map_bits - converts the pages to size of bitmap
percpu-internal.h: * pcpu_chunk_map_bits - helper to convert nr_pages to size of bitmap
percpu-internal.h:	return pcpu_nr_pages_to_map_bits(chunk->nr_pages);
percpu-internal.h: * pcpu_stats_area_alloc - increment area allocation stats
percpu-internal.h:	chunk->nr_alloc++;
percpu-internal.h:	chunk->max_alloc_size = max(chunk->max_alloc_size, size);
percpu-internal.h: * pcpu_stats_area_dealloc - decrement allocation stats
percpu-internal.h:	pcpu_stats.nr_cur_alloc--;
percpu-internal.h:	chunk->nr_alloc--;
percpu-internal.h: * pcpu_stats_chunk_alloc - increment chunk stats
percpu-internal.h: * pcpu_stats_chunk_dealloc - decrement chunk stats
percpu-internal.h:	pcpu_stats.nr_chunks--;
msync.c:// SPDX-License-Identifier: GPL-2.0
msync.c: * Copyright (C) 1994-1999  Linus Torvalds
msync.c: * MS_SYNC syncs the entire file - including mappings.
msync.c:	struct mm_struct *mm = current->mm;
msync.c:	int error = -EINVAL;
msync.c:	error = -ENOMEM;
msync.c:	 * just ignore them, but return -ENOMEM at the end.
msync.c:	down_read(&mm->mmap_sem);
msync.c:		error = -ENOMEM;
msync.c:		/* Here start < vma->vm_end. */
msync.c:		if (start < vma->vm_start) {
msync.c:			start = vma->vm_start;
msync.c:			unmapped_error = -ENOMEM;
msync.c:		/* Here vma->vm_start <= start < vma->vm_end. */
msync.c:				(vma->vm_flags & VM_LOCKED)) {
msync.c:			error = -EBUSY;
msync.c:		file = vma->vm_file;
msync.c:		fstart = (start - vma->vm_start) +
msync.c:			 ((loff_t)vma->vm_pgoff << PAGE_SHIFT);
msync.c:		fend = fstart + (min(end, vma->vm_end) - start) - 1;
msync.c:		start = vma->vm_end;
msync.c:				(vma->vm_flags & VM_SHARED)) {
msync.c:			up_read(&mm->mmap_sem);
msync.c:			down_read(&mm->mmap_sem);
msync.c:			vma = vma->vm_next;
msync.c:	up_read(&mm->mmap_sem);
highmem.c:// SPDX-License-Identifier: GPL-2.0
highmem.c: * Redesigned the x86 32-bit VM architecture to deal with
highmem.c: * 64-bit physical space. With current x86 CPUs this
highmem.c: *    since a TLB flush - it is usable.
highmem.c: *    since the last TLB flush - so we can't use it.
highmem.c: *  n means that there are (n-1) current users of it.
highmem.c:		 * Don't need an atomic fetch-and-clear op here;
highmem.c:		 * no-one has the page mapped, and cannot get at
highmem.c: * kmap_flush_unused - flush all unused kmap mappings in order to remove stray mappings
highmem.c:		if (--count)
highmem.c:			/* Re-start */
highmem.c: * kmap_high - map a highmem page into memory
highmem.c: * kmap_high_get - pin a highmem page into memory
highmem.c: * kunmap_high - unmap a highmem page into memory
highmem.c:	switch (--pkmap_count[nr]) {
highmem.c:		 * The tasks queued in the wait-queue are guarded
highmem.c:		 * by both the lock in the wait-queue-head and by
highmem.c:		 * no need for the wait-queue-head's lock.  Simply
highmem.c:	/* do wake-up, if needed, race-free outside of the spin lock */
highmem.c: * Describes one page->virtual association
highmem.c: * page_address - get the mapped virtual address of a page
highmem.c:	spin_lock_irqsave(&pas->lock, flags);
highmem.c:	if (!list_empty(&pas->lh)) {
highmem.c:		list_for_each_entry(pam, &pas->lh, list) {
highmem.c:			if (pam->page == page) {
highmem.c:				ret = pam->virtual;
highmem.c:	spin_unlock_irqrestore(&pas->lock, flags);
highmem.c: * set_page_address - set a page's virtual address
highmem.c:		pam->page = page;
highmem.c:		pam->virtual = virtual;
highmem.c:		spin_lock_irqsave(&pas->lock, flags);
highmem.c:		list_add_tail(&pam->list, &pas->lh);
highmem.c:		spin_unlock_irqrestore(&pas->lock, flags);
highmem.c:		spin_lock_irqsave(&pas->lock, flags);
highmem.c:		list_for_each_entry(pam, &pas->lh, list) {
highmem.c:			if (pam->page == page) {
highmem.c:				list_del(&pam->list);
highmem.c:				spin_unlock_irqrestore(&pas->lock, flags);
highmem.c:		spin_unlock_irqrestore(&pas->lock, flags);
madvise.c:// SPDX-License-Identifier: GPL-2.0
madvise.c:#include <linux/page-isolation.h>
madvise.c:#include <linux/backing-dev.h>
madvise.c: * Any behaviour which results in changes to the vma->vm_flags needs to
madvise.c:	struct mm_struct *mm = vma->vm_mm;
madvise.c:	unsigned long new_flags = vma->vm_flags;
madvise.c:		if (vma->vm_flags & VM_IO) {
madvise.c:			error = -EINVAL;
madvise.c:		if (vma->vm_file || vma->vm_flags & VM_SHARED) {
madvise.c:			error = -EINVAL;
madvise.c:			error = -EINVAL;
madvise.c:			if (error == -ENOMEM)
madvise.c:				error = -EAGAIN;
madvise.c:			if (error == -ENOMEM)
madvise.c:				error = -EAGAIN;
madvise.c:	if (new_flags == vma->vm_flags) {
madvise.c:	pgoff = vma->vm_pgoff + ((start - vma->vm_start) >> PAGE_SHIFT);
madvise.c:	*prev = vma_merge(mm, *prev, start, end, new_flags, vma->anon_vma,
madvise.c:			  vma->vm_file, pgoff, vma_policy(vma),
madvise.c:			  vma->vm_userfaultfd_ctx);
madvise.c:	if (start != vma->vm_start) {
madvise.c:		if (unlikely(mm->map_count >= sysctl_max_map_count)) {
madvise.c:			error = -ENOMEM;
madvise.c:			if (error == -ENOMEM)
madvise.c:				error = -EAGAIN;
madvise.c:	if (end != vma->vm_end) {
madvise.c:		if (unlikely(mm->map_count >= sysctl_max_map_count)) {
madvise.c:			error = -ENOMEM;
madvise.c:			if (error == -ENOMEM)
madvise.c:				error = -EAGAIN;
madvise.c:	vma->vm_flags = new_flags;
madvise.c:	struct vm_area_struct *vma = walk->private;
madvise.c:		orig_pte = pte_offset_map_lock(vma->vm_mm, pmd, start, &ptl);
madvise.c:		pte = *(orig_pte + ((index - start) / PAGE_SIZE));
madvise.c:		.mm = vma->vm_mm,
madvise.c:		index = ((start - vma->vm_start) >> PAGE_SHIFT) + vma->vm_pgoff;
madvise.c:	struct file *file = vma->vm_file;
madvise.c:	if (shmem_mapping(file->f_mapping)) {
madvise.c:					file->f_mapping);
madvise.c:		return -EBADF;
madvise.c:	start = ((start - vma->vm_start) >> PAGE_SHIFT) + vma->vm_pgoff;
madvise.c:	if (end > vma->vm_end)
madvise.c:		end = vma->vm_end;
madvise.c:	end = ((end - vma->vm_start) >> PAGE_SHIFT) + vma->vm_pgoff;
madvise.c:	force_page_cache_readahead(file->f_mapping, file, start, end - start);
madvise.c:	struct mmu_gather *tlb = walk->private;
madvise.c:	struct mm_struct *mm = tlb->mm;
madvise.c:	struct vm_area_struct *vma = walk->vma;
madvise.c:		 * prevent swap-in which is more expensive rather than
madvise.c:			nr_swap--;
madvise.c:			pte_clear_not_present_full(mm, addr, pte, tlb->fullmm);
madvise.c:			pte--;
madvise.c:			addr -= PAGE_SIZE;
madvise.c:							tlb->fullmm);
madvise.c:		if (current->mm == mm)
madvise.c:		.mm = vma->vm_mm,
madvise.c:	struct mm_struct *mm = vma->vm_mm;
madvise.c:		return -EINVAL;
madvise.c:	start = max(vma->vm_start, start_addr);
madvise.c:	if (start >= vma->vm_end)
madvise.c:		return -EINVAL;
madvise.c:	end = min(vma->vm_end, end_addr);
madvise.c:	if (end <= vma->vm_start)
madvise.c:		return -EINVAL;
madvise.c:	zap_page_range(vma, start, end - start);
madvise.c:		return -EINVAL;
madvise.c:		down_read(&current->mm->mmap_sem);
madvise.c:		vma = find_vma(current->mm, start);
madvise.c:			return -ENOMEM;
madvise.c:		if (start < vma->vm_start) {
madvise.c:			 * with the lowest vma->vm_start where start
madvise.c:			 * is also < vma->vm_end. If start <
madvise.c:			 * vma->vm_start it means an hole materialized
madvise.c:			return -ENOMEM;
madvise.c:			return -EINVAL;
madvise.c:		if (end > vma->vm_end) {
madvise.c:			 * Don't fail if end > vma->vm_end. If the old
madvise.c:			 * end-vma->vm_end range, but the manager can
madvise.c:			end = vma->vm_end;
madvise.c:		return -EINVAL;
madvise.c:	if (vma->vm_flags & VM_LOCKED)
madvise.c:		return -EINVAL;
madvise.c:	f = vma->vm_file;
madvise.c:	if (!f || !f->f_mapping || !f->f_mapping->host) {
madvise.c:			return -EINVAL;
madvise.c:	if ((vma->vm_flags & (VM_SHARED|VM_WRITE)) != (VM_SHARED|VM_WRITE))
madvise.c:		return -EACCES;
madvise.c:	offset = (loff_t)(start - vma->vm_start)
madvise.c:			+ ((loff_t)vma->vm_pgoff << PAGE_SHIFT);
madvise.c:		up_read(&current->mm->mmap_sem);
madvise.c:				offset, end - start);
madvise.c:	down_read(&current->mm->mmap_sem);
madvise.c:		return -EPERM;
madvise.c:	/* Ensure that all poisoned pages are removed from per-cpu lists */
madvise.c: * use appropriate read-ahead and caching techniques.  The information
madvise.c: *  MADV_NORMAL - the default behavior is to read clusters.  This
madvise.c: *		results in some read-ahead and read-behind.
madvise.c: *  MADV_RANDOM - the system should read the minimum amount of data
madvise.c: *		on any access, since it is unlikely that the appli-
madvise.c: *  MADV_SEQUENTIAL - pages in the given range will probably be accessed
madvise.c: *  MADV_WILLNEED - the application is notifying the system to read
madvise.c: *  MADV_DONTNEED - the application is finished with the given range,
madvise.c: *  MADV_FREE - the application marks pages in the given range as lazy free,
madvise.c: *  MADV_REMOVE - the application wants to free up the given range of
madvise.c: *  MADV_DONTFORK - omit this area from child's address space when forking:
madvise.c: *  MADV_DOFORK - cancel MADV_DONTFORK: no longer omit this area when forking.
madvise.c: *  MADV_WIPEONFORK - present the child process with zero-filled memory in this
madvise.c: *  MADV_KEEPONFORK - undo the effect of MADV_WIPEONFORK
madvise.c: *  MADV_HWPOISON - trigger memory error handler as if the given memory range
madvise.c: *  MADV_SOFT_OFFLINE - try to soft-offline the given range of memory.
madvise.c: *  MADV_MERGEABLE - the application recommends that KSM try to merge pages in
madvise.c: *  MADV_UNMERGEABLE- cancel MADV_MERGEABLE: no longer merge pages with others.
madvise.c: *  MADV_HUGEPAGE - the application wants to back the given range by transparent
madvise.c: *  MADV_NOHUGEPAGE - mark the given range as not worth being backed by
madvise.c: *  MADV_DONTDUMP - the application wants to prevent pages in the given range
madvise.c: *  MADV_DODUMP - cancel MADV_DONTDUMP: no longer exclude from core dump.
madvise.c: *  zero    - success
madvise.c: *  -EINVAL - start + len < 0, start is not page-aligned,
madvise.c: *  -ENOMEM - addresses in the specified range are not currently
madvise.c: *  -EIO    - an I/O error occurred while paging in data.
madvise.c: *  -EBADF  - map exists, but area maps something that isn't a file.
madvise.c: *  -EAGAIN - a kernel resource was temporarily unavailable.
madvise.c:	int error = -EINVAL;
madvise.c:	/* Check to see whether len was rounded up from small -ve to zero */
madvise.c:		if (down_write_killable(&current->mm->mmap_sem))
madvise.c:			return -EINTR;
madvise.c:		down_read(&current->mm->mmap_sem);
madvise.c:	 * ranges, just ignore them, but return -ENOMEM at the end.
madvise.c:	 * - different from the way of handling in mlock etc.
madvise.c:	vma = find_vma_prev(current->mm, start, &prev);
madvise.c:	if (vma && start > vma->vm_start)
madvise.c:		error = -ENOMEM;
madvise.c:		/* Here start < (end|vma->vm_end). */
madvise.c:		if (start < vma->vm_start) {
madvise.c:			unmapped_error = -ENOMEM;
madvise.c:			start = vma->vm_start;
madvise.c:		/* Here vma->vm_start <= start < (end|vma->vm_end) */
madvise.c:		tmp = vma->vm_end;
madvise.c:		/* Here vma->vm_start <= start < tmp <= (end|vma->vm_end). */
madvise.c:		if (prev && start < prev->vm_end)
madvise.c:			start = prev->vm_end;
madvise.c:			vma = prev->vm_next;
madvise.c:			vma = find_vma(current->mm, start);
madvise.c:		up_write(&current->mm->mmap_sem);
madvise.c:		up_read(&current->mm->mmap_sem);
dmapool.c: * This allocator returns small blocks of a given size which are DMA-able by
dmapool.c: * represented by the 'struct dma_pool' which keeps a doubly-linked list of
dmapool.c: * least 'size' bytes.  Free blocks are tracked in an unsorted singly-linked
dmapool.c:#include <linux/dma-mapping.h>
dmapool.c:	temp = scnprintf(next, size, "poolinfo - 0.1\n");
dmapool.c:	size -= temp;
dmapool.c:	list_for_each_entry(pool, &dev->dma_pools, pools) {
dmapool.c:		spin_lock_irq(&pool->lock);
dmapool.c:		list_for_each_entry(page, &pool->page_list, page_list) {
dmapool.c:			blocks += page->in_use;
dmapool.c:		spin_unlock_irq(&pool->lock);
dmapool.c:		/* per-pool info, no real statistics yet */
dmapool.c:		temp = scnprintf(next, size, "%-16s %4u %4zu %4zu %2u\n",
dmapool.c:				 pool->name, blocks,
dmapool.c:				 pages * (pool->allocation / pool->size),
dmapool.c:				 pool->size, pages);
dmapool.c:		size -= temp;
dmapool.c:	return PAGE_SIZE - size;
dmapool.c: * dma_pool_create - Creates a pool of consistent memory blocks, for dma.
dmapool.c:	else if (align & (align - 1))
dmapool.c:	else if ((boundary < size) || (boundary & (boundary - 1)))
dmapool.c:	strlcpy(retval->name, name, sizeof(retval->name));
dmapool.c:	retval->dev = dev;
dmapool.c:	INIT_LIST_HEAD(&retval->page_list);
dmapool.c:	spin_lock_init(&retval->lock);
dmapool.c:	retval->size = size;
dmapool.c:	retval->boundary = boundary;
dmapool.c:	retval->allocation = allocation;
dmapool.c:	INIT_LIST_HEAD(&retval->pools);
dmapool.c:	 * pools_lock ensures that the ->dma_pools list does not get corrupted.
dmapool.c:	if (list_empty(&dev->dma_pools))
dmapool.c:	list_add(&retval->pools, &dev->dma_pools);
dmapool.c:			list_del(&retval->pools);
dmapool.c:	unsigned int next_boundary = pool->boundary;
dmapool.c:		unsigned int next = offset + pool->size;
dmapool.c:		if (unlikely((next + pool->size) >= next_boundary)) {
dmapool.c:			next_boundary += pool->boundary;
dmapool.c:		*(int *)(page->vaddr + offset) = next;
dmapool.c:	} while (offset < pool->allocation);
dmapool.c:	page->vaddr = dma_alloc_coherent(pool->dev, pool->allocation,
dmapool.c:					 &page->dma, mem_flags);
dmapool.c:	if (page->vaddr) {
dmapool.c:		memset(page->vaddr, POOL_POISON_FREED, pool->allocation);
dmapool.c:		page->in_use = 0;
dmapool.c:		page->offset = 0;
dmapool.c:	return page->in_use != 0;
dmapool.c:	dma_addr_t dma = page->dma;
dmapool.c:	memset(page->vaddr, POOL_POISON_FREED, pool->allocation);
dmapool.c:	dma_free_coherent(pool->dev, pool->allocation, page->vaddr, dma);
dmapool.c:	list_del(&page->page_list);
dmapool.c: * dma_pool_destroy - destroys a pool of dma memory blocks.
dmapool.c:	list_del(&pool->pools);
dmapool.c:	if (pool->dev && list_empty(&pool->dev->dma_pools))
dmapool.c:		device_remove_file(pool->dev, &dev_attr_pools);
dmapool.c:	while (!list_empty(&pool->page_list)) {
dmapool.c:		page = list_entry(pool->page_list.next,
dmapool.c:			if (pool->dev)
dmapool.c:				dev_err(pool->dev,
dmapool.c:					pool->name, page->vaddr);
dmapool.c:				       pool->name, page->vaddr);
dmapool.c:			/* leak the still-in-use consistent memory */
dmapool.c:			list_del(&page->page_list);
dmapool.c: * dma_pool_alloc - get a block of consistent memory
dmapool.c:	spin_lock_irqsave(&pool->lock, flags);
dmapool.c:	list_for_each_entry(page, &pool->page_list, page_list) {
dmapool.c:		if (page->offset < pool->allocation)
dmapool.c:	/* pool_alloc_page() might sleep, so temporarily drop &pool->lock */
dmapool.c:	spin_unlock_irqrestore(&pool->lock, flags);
dmapool.c:	spin_lock_irqsave(&pool->lock, flags);
dmapool.c:	list_add(&page->page_list, &pool->page_list);
dmapool.c:	page->in_use++;
dmapool.c:	offset = page->offset;
dmapool.c:	page->offset = *(int *)(page->vaddr + offset);
dmapool.c:	retval = offset + page->vaddr;
dmapool.c:	*handle = offset + page->dma;
dmapool.c:		/* page->offset is stored in first 4 bytes */
dmapool.c:		for (i = sizeof(page->offset); i < pool->size; i++) {
dmapool.c:			if (pool->dev)
dmapool.c:				dev_err(pool->dev,
dmapool.c:					pool->name, retval);
dmapool.c:					pool->name, retval);
dmapool.c:					data, pool->size, 1);
dmapool.c:		memset(retval, POOL_POISON_ALLOCATED, pool->size);
dmapool.c:	spin_unlock_irqrestore(&pool->lock, flags);
dmapool.c:		memset(retval, 0, pool->size);
dmapool.c:	list_for_each_entry(page, &pool->page_list, page_list) {
dmapool.c:		if (dma < page->dma)
dmapool.c:		if ((dma - page->dma) < pool->allocation)
dmapool.c: * dma_pool_free - put block back into dma pool
dmapool.c: * unless it is first re-allocated.
dmapool.c:	spin_lock_irqsave(&pool->lock, flags);
dmapool.c:		spin_unlock_irqrestore(&pool->lock, flags);
dmapool.c:		if (pool->dev)
dmapool.c:			dev_err(pool->dev,
dmapool.c:				pool->name, vaddr, (unsigned long)dma);
dmapool.c:			       pool->name, vaddr, (unsigned long)dma);
dmapool.c:	offset = vaddr - page->vaddr;
dmapool.c:	if ((dma - page->dma) != offset) {
dmapool.c:		spin_unlock_irqrestore(&pool->lock, flags);
dmapool.c:		if (pool->dev)
dmapool.c:			dev_err(pool->dev,
dmapool.c:				pool->name, vaddr, &dma);
dmapool.c:			       pool->name, vaddr, &dma);
dmapool.c:		unsigned int chain = page->offset;
dmapool.c:		while (chain < pool->allocation) {
dmapool.c:				chain = *(int *)(page->vaddr + chain);
dmapool.c:			spin_unlock_irqrestore(&pool->lock, flags);
dmapool.c:			if (pool->dev)
dmapool.c:				dev_err(pool->dev, "dma_pool_free %s, dma %pad already free\n",
dmapool.c:					pool->name, &dma);
dmapool.c:				       pool->name, &dma);
dmapool.c:	memset(vaddr, POOL_POISON_FREED, pool->size);
dmapool.c:	page->in_use--;
dmapool.c:	*(int *)vaddr = page->offset;
dmapool.c:	page->offset = offset;
dmapool.c:	spin_unlock_irqrestore(&pool->lock, flags);
dmapool.c: * dmam_pool_create - Managed dma_pool_create()
dmapool.c: * dmam_pool_destroy - Managed dma_pool_destroy()
dmapool.c:	struct device *dev = pool->dev;
cma.c: * Copyright (c) 2010-2011 by Samsung Electronics.
cma.c:	return PFN_PHYS(cma->base_pfn);
cma.c:	return cma->count << PAGE_SHIFT;
cma.c:	return cma->name ? cma->name : "(undefined)";
cma.c:	if (align_order <= cma->order_per_bit)
cma.c:	return (1UL << (align_order - cma->order_per_bit)) - 1;
cma.c:	return (cma->base_pfn & ((1UL << align_order) - 1))
cma.c:		>> cma->order_per_bit;
cma.c:	return ALIGN(pages, 1UL << cma->order_per_bit) >> cma->order_per_bit;
cma.c:	bitmap_no = (pfn - cma->base_pfn) >> cma->order_per_bit;
cma.c:	mutex_lock(&cma->lock);
cma.c:	bitmap_clear(cma->bitmap, bitmap_no, bitmap_count);
cma.c:	mutex_unlock(&cma->lock);
cma.c:	unsigned long base_pfn = cma->base_pfn, pfn = base_pfn;
cma.c:	unsigned i = cma->count >> pageblock_order;
cma.c:	cma->bitmap = kzalloc(bitmap_size, GFP_KERNEL);
cma.c:	if (!cma->bitmap)
cma.c:		return -ENOMEM;
cma.c:		for (j = pageblock_nr_pages; j; --j, pfn++) {
cma.c:	} while (--i);
cma.c:	mutex_init(&cma->lock);
cma.c:	INIT_HLIST_HEAD(&cma->mem_head);
cma.c:	spin_lock_init(&cma->mem_head_lock);
cma.c:	pr_err("CMA area %s could not be activated\n", cma->name);
cma.c:	kfree(cma->bitmap);
cma.c:	cma->count = 0;
cma.c:	return -EINVAL;
cma.c: * cma_init_reserved_mem() - create custom contiguous area from reserved memory
cma.c:		return -ENOSPC;
cma.c:		return -EINVAL;
cma.c:			max_t(unsigned long, MAX_ORDER - 1, pageblock_order);
cma.c:		return -EINVAL;
cma.c:		return -EINVAL;
cma.c:		cma->name = name;
cma.c:		cma->name = kasprintf(GFP_KERNEL, "cma%d\n", cma_area_count);
cma.c:		if (!cma->name)
cma.c:			return -ENOMEM;
cma.c:	cma->base_pfn = PFN_DOWN(base);
cma.c:	cma->count = size >> PAGE_SHIFT;
cma.c:	cma->order_per_bit = order_per_bit;
cma.c: * cma_declare_contiguous() - reserve custom contiguous area
cma.c:	highmem_start = __pa(high_memory - 1) + 1;
cma.c:		return -ENOSPC;
cma.c:		return -EINVAL;
cma.c:		return -EINVAL;
cma.c:			  max_t(unsigned long, MAX_ORDER - 1, pageblock_order));
cma.c:	limit &= ~(alignment - 1);
cma.c:		return -EINVAL;
cma.c:		ret = -EINVAL;
cma.c:			ret = -EBUSY;
cma.c:				ret = -ENOMEM;
cma.c:	mutex_lock(&cma->lock);
cma.c:		next_zero_bit = find_next_zero_bit(cma->bitmap, cma->count, start);
cma.c:		if (next_zero_bit >= cma->count)
cma.c:		next_set_bit = find_next_bit(cma->bitmap, cma->count, next_zero_bit);
cma.c:		nr_zero = next_set_bit - next_zero_bit;
cma.c:	pr_cont("=> %u free of %lu total pages\n", nr_total, cma->count);
cma.c:	mutex_unlock(&cma->lock);
cma.c: * cma_alloc() - allocate pages from contiguous area
cma.c:	unsigned long pfn = -1;
cma.c:	int ret = -ENOMEM;
cma.c:	if (!cma || !cma->count)
cma.c:		mutex_lock(&cma->lock);
cma.c:		bitmap_no = bitmap_find_next_zero_area_off(cma->bitmap,
cma.c:			mutex_unlock(&cma->lock);
cma.c:		bitmap_set(cma->bitmap, bitmap_no, bitmap_count);
cma.c:		mutex_unlock(&cma->lock);
cma.c:		pfn = cma->base_pfn + (bitmap_no << cma->order_per_bit);
cma.c:		if (ret != -EBUSY)
cma.c:		pr_info("%s: alloc failed, req-size: %zu pages, ret: %d\n",
cma.c: * cma_release() - release allocated pages
cma.c:	if (pfn < cma->base_pfn || pfn >= cma->base_pfn + cma->count)
cma.c:	VM_BUG_ON(pfn + count > cma->base_pfn + cma->count);
mremap.c:// SPDX-License-Identifier: GPL-2.0
mremap.c:#include <linux/mm-arch-hooks.h>
mremap.c:	if (vma->vm_file)
mremap.c:		i_mmap_lock_write(vma->vm_file->f_mapping);
mremap.c:	if (vma->anon_vma)
mremap.c:		anon_vma_lock_write(vma->anon_vma);
mremap.c:	if (vma->anon_vma)
mremap.c:		anon_vma_unlock_write(vma->anon_vma);
mremap.c:	if (vma->vm_file)
mremap.c:		i_mmap_unlock_write(vma->vm_file->f_mapping);
mremap.c:	struct mm_struct *mm = vma->vm_mm;
mremap.c:	unsigned long len = old_end - old_addr;
mremap.c:	 * - During exec() shift_arg_pages(), we use a specially tagged vma
mremap.c:	 * - During mremap(), new_vma is often known to be placed after vma
mremap.c:	flush_tlb_batched_pending(vma->vm_mm);
mremap.c:		pte = move_pte(pte, new_vma->vm_page_prot, old_addr, new_addr);
mremap.c:	pte_unmap(new_pte - 1);
mremap.c:		flush_tlb_range(vma, old_end - len, old_end);
mremap.c:	pte_unmap_unlock(old_pte - 1, old_ptl);
mremap.c:	mmu_notifier_invalidate_range_start(vma->vm_mm, mmun_start, mmun_end);
mremap.c:		extent = next - old_addr;
mremap.c:		if (extent > old_end - old_addr)
mremap.c:			extent = old_end - old_addr;
mremap.c:		old_pmd = get_old_pmd(vma->vm_mm, old_addr);
mremap.c:		new_pmd = alloc_new_pmd(vma->vm_mm, vma, new_addr);
mremap.c:		if (pte_alloc(new_vma->vm_mm, new_pmd, new_addr))
mremap.c:		if (extent > next - new_addr)
mremap.c:			extent = next - new_addr;
mremap.c:		flush_tlb_range(vma, old_end-len, old_addr);
mremap.c:	mmu_notifier_invalidate_range_end(vma->vm_mm, mmun_start, mmun_end);
mremap.c:	return len + old_addr - old_end;	/* how much done */
mremap.c:	struct mm_struct *mm = vma->vm_mm;
mremap.c:	unsigned long vm_flags = vma->vm_flags;
mremap.c:	if (mm->map_count >= sysctl_max_map_count - 3)
mremap.c:		return -ENOMEM;
mremap.c:	 * pages recently unmapped.  But leave vma->vm_flags as it was,
mremap.c:	new_pgoff = vma->vm_pgoff + ((old_addr - vma->vm_start) >> PAGE_SHIFT);
mremap.c:		return -ENOMEM;
mremap.c:		err = -ENOMEM;
mremap.c:	} else if (vma->vm_ops && vma->vm_ops->mremap) {
mremap.c:		err = vma->vm_ops->mremap(new_vma);
mremap.c:		vma->vm_flags &= ~VM_ACCOUNT;
mremap.c:		excess = vma->vm_end - vma->vm_start - old_len;
mremap.c:		if (old_addr > vma->vm_start &&
mremap.c:		    old_addr + old_len < vma->vm_end)
mremap.c:	hiwater_vm = mm->hiwater_vm;
mremap.c:	vm_stat_account(mm, vma->vm_flags, new_len >> PAGE_SHIFT);
mremap.c:	if (unlikely(vma->vm_flags & VM_PFNMAP))
mremap.c:	mm->hiwater_vm = hiwater_vm;
mremap.c:		vma->vm_flags |= VM_ACCOUNT;
mremap.c:			vma->vm_next->vm_flags |= VM_ACCOUNT;
mremap.c:		mm->locked_vm += new_len >> PAGE_SHIFT;
mremap.c:	struct mm_struct *mm = current->mm;
mremap.c:	if (!vma || vma->vm_start > addr)
mremap.c:		return ERR_PTR(-EFAULT);
mremap.c:	if (!old_len && !(vma->vm_flags & (VM_SHARED | VM_MAYSHARE))) {
mremap.c:		pr_warn_once("%s (%d): attempted to duplicate a private mapping with mremap.  This is not supported.\n", current->comm, current->pid);
mremap.c:		return ERR_PTR(-EINVAL);
mremap.c:		return ERR_PTR(-EINVAL);
mremap.c:	if (old_len > vma->vm_end - addr)
mremap.c:		return ERR_PTR(-EFAULT);
mremap.c:	pgoff = (addr - vma->vm_start) >> PAGE_SHIFT;
mremap.c:	pgoff += vma->vm_pgoff;
mremap.c:		return ERR_PTR(-EINVAL);
mremap.c:	if (vma->vm_flags & (VM_DONTEXPAND | VM_PFNMAP))
mremap.c:		return ERR_PTR(-EFAULT);
mremap.c:	if (vma->vm_flags & VM_LOCKED) {
mremap.c:		locked = mm->locked_vm << PAGE_SHIFT;
mremap.c:		locked += new_len - old_len;
mremap.c:			return ERR_PTR(-EAGAIN);
mremap.c:	if (!may_expand_vm(mm, vma->vm_flags,
mremap.c:				(new_len - old_len) >> PAGE_SHIFT))
mremap.c:		return ERR_PTR(-ENOMEM);
mremap.c:	if (vma->vm_flags & VM_ACCOUNT) {
mremap.c:		unsigned long charged = (new_len - old_len) >> PAGE_SHIFT;
mremap.c:			return ERR_PTR(-ENOMEM);
mremap.c:	struct mm_struct *mm = current->mm;
mremap.c:	unsigned long ret = -EINVAL;
mremap.c:	if (new_len > TASK_SIZE || new_addr > TASK_SIZE - new_len)
mremap.c:		ret = do_munmap(mm, addr+new_len, old_len - new_len, uf_unmap);
mremap.c:	if (vma->vm_flags & VM_MAYSHARE)
mremap.c:	ret = get_unmapped_area(vma->vm_file, new_addr, new_len, vma->vm_pgoff +
mremap.c:				((addr - vma->vm_start) >> PAGE_SHIFT),
mremap.c:	unsigned long end = vma->vm_end + delta;
mremap.c:	if (end < vma->vm_end) /* overflow */
mremap.c:	if (vma->vm_next && vma->vm_next->vm_start < end) /* intersection */
mremap.c:	if (get_unmapped_area(NULL, vma->vm_start, end - vma->vm_start,
mremap.c: * MREMAP_FIXED option added 5-Dec-1999 by Benjamin LaHaise
mremap.c:	struct mm_struct *mm = current->mm;
mremap.c:	unsigned long ret = -EINVAL;
mremap.c:	 * We allow a zero old-len as a special case
mremap.c:	 * for DOS-emu "duplicate shm area" thing. But
mremap.c:	 * a zero new-len is nonsensical.
mremap.c:	if (down_write_killable(&current->mm->mmap_sem))
mremap.c:		return -EINTR;
mremap.c:		ret = do_munmap(mm, addr+new_len, old_len - new_len, &uf_unmap);
mremap.c:	if (old_len == vma->vm_end - addr) {
mremap.c:		if (vma_expandable(vma, new_len - old_len)) {
mremap.c:			int pages = (new_len - old_len) >> PAGE_SHIFT;
mremap.c:			if (vma_adjust(vma, vma->vm_start, addr + new_len,
mremap.c:				       vma->vm_pgoff, NULL)) {
mremap.c:				ret = -ENOMEM;
mremap.c:			vm_stat_account(mm, vma->vm_flags, pages);
mremap.c:			if (vma->vm_flags & VM_LOCKED) {
mremap.c:				mm->locked_vm += pages;
mremap.c:	ret = -ENOMEM;
mremap.c:		if (vma->vm_flags & VM_MAYSHARE)
mremap.c:		new_addr = get_unmapped_area(vma->vm_file, 0, new_len,
mremap.c:					vma->vm_pgoff +
mremap.c:					((addr - vma->vm_start) >> PAGE_SHIFT),
mremap.c:	up_write(&current->mm->mmap_sem);
mremap.c:		mm_populate(new_addr + old_len, new_len - old_len);
init-mm.c:// SPDX-License-Identifier: GPL-2.0
memory_hotplug.c:#include <linux/page-isolation.h>
memory_hotplug.c:#include <linux/firmware-map.h>
memory_hotplug.c:		return ERR_PTR(-ENOMEM);
memory_hotplug.c:	res->name = "System RAM";
memory_hotplug.c:	res->start = start;
memory_hotplug.c:	res->end = start + size - 1;
memory_hotplug.c:	res->flags = IORESOURCE_SYSTEM_RAM | IORESOURCE_BUSY;
memory_hotplug.c:		if (conflict->desc == IORES_DESC_DEVICE_PRIVATE_MEMORY) {
memory_hotplug.c:		return ERR_PTR(-EEXIST);
memory_hotplug.c:	page->freelist = (void *)type;
memory_hotplug.c:	type = (unsigned long) page->freelist;
memory_hotplug.c:		page->freelist = NULL;
memory_hotplug.c:		INIT_LIST_HEAD(&page->lru);
memory_hotplug.c:	memmap = sparse_decode_mem_map(ms->section_mem_map, section_nr);
memory_hotplug.c:	usemap = __nr_to_section(section_nr)->pageblock_flags;
memory_hotplug.c:	memmap = sparse_decode_mem_map(ms->section_mem_map, section_nr);
memory_hotplug.c:	usemap = __nr_to_section(section_nr)->pageblock_flags;
memory_hotplug.c:	int node = pgdat->node_id;
memory_hotplug.c:	pfn = pgdat->node_start_pfn;
memory_hotplug.c:		 * Some platforms can assign the same pfn to multiple nodes - on
memory_hotplug.c:		return -EEXIST;
memory_hotplug.c:	/* during initialize mem_map, align hot-added range to section */
memory_hotplug.c:	end_sec = pfn_to_section_nr(phys_start_pfn + nr_pages - 1);
memory_hotplug.c:		if (altmap->base_pfn != phys_start_pfn
memory_hotplug.c:			err = -EINVAL;
memory_hotplug.c:		altmap->alloc = 0;
memory_hotplug.c:		if (err && (err != -EEXIST))
memory_hotplug.c:	pfn = end_pfn - 1;
memory_hotplug.c:	for (; pfn >= start_pfn; pfn -= PAGES_PER_SECTION) {
memory_hotplug.c:	unsigned long zone_start_pfn = zone->zone_start_pfn;
memory_hotplug.c:		 * shrink zone->zone_start_pfn and zone->zone_spanned_pages.
memory_hotplug.c:			zone->zone_start_pfn = pfn;
memory_hotplug.c:			zone->spanned_pages = zone_end_pfn - pfn;
memory_hotplug.c:		 * shrink zone->spanned_pages.
memory_hotplug.c:			zone->spanned_pages = pfn - zone_start_pfn + 1;
memory_hotplug.c:	zone->zone_start_pfn = 0;
memory_hotplug.c:	zone->spanned_pages = 0;
memory_hotplug.c:	unsigned long pgdat_start_pfn = pgdat->node_start_pfn;
memory_hotplug.c:	int nid = pgdat->node_id;
memory_hotplug.c:		 * shrink pgdat->node_start_pfn and pgdat->node_spanned_pages.
memory_hotplug.c:			pgdat->node_start_pfn = pfn;
memory_hotplug.c:			pgdat->node_spanned_pages = pgdat_end_pfn - pfn;
memory_hotplug.c:		 * shrink pgdat->node_spanned_pages.
memory_hotplug.c:			pgdat->node_spanned_pages = pfn - pgdat_start_pfn + 1;
memory_hotplug.c:	pgdat->node_start_pfn = 0;
memory_hotplug.c:	pgdat->node_spanned_pages = 0;
memory_hotplug.c:	struct pglist_data *pgdat = zone->zone_pgdat;
memory_hotplug.c:	pgdat_resize_lock(zone->zone_pgdat, &flags);
memory_hotplug.c:	pgdat_resize_unlock(zone->zone_pgdat, &flags);
memory_hotplug.c:	int ret = -EINVAL;
memory_hotplug.c: * __remove_pages() - remove sections of pages from a zone
memory_hotplug.c:			resource_size_t endres = start + size - 1;
memory_hotplug.c:			pr_warn("Unable to release resource <%pa-%pa> (%d)\n",
memory_hotplug.c:	int rc = -EINVAL;
memory_hotplug.c:	int rc = -EINVAL;
memory_hotplug.c:		arg->status_change_nid_normal = nid;
memory_hotplug.c:		arg->status_change_nid_normal = -1;
memory_hotplug.c:		arg->status_change_nid_high = nid;
memory_hotplug.c:		arg->status_change_nid_high = -1;
memory_hotplug.c:	arg->status_change_nid_high = arg->status_change_nid_normal;
memory_hotplug.c:		arg->status_change_nid = nid;
memory_hotplug.c:		arg->status_change_nid = -1;
memory_hotplug.c:	if (arg->status_change_nid_normal >= 0)
memory_hotplug.c:	if (arg->status_change_nid_high >= 0)
memory_hotplug.c:	if (zone_is_empty(zone) || start_pfn < zone->zone_start_pfn)
memory_hotplug.c:		zone->zone_start_pfn = start_pfn;
memory_hotplug.c:	zone->spanned_pages = max(start_pfn + nr_pages, old_end_pfn) - zone->zone_start_pfn;
memory_hotplug.c:	if (!pgdat->node_spanned_pages || start_pfn < pgdat->node_start_pfn)
memory_hotplug.c:		pgdat->node_start_pfn = start_pfn;
memory_hotplug.c:	pgdat->node_spanned_pages = max(start_pfn + nr_pages, old_end_pfn) - pgdat->node_start_pfn;
memory_hotplug.c:	struct pglist_data *pgdat = zone->zone_pgdat;
memory_hotplug.c:	int nid = pgdat->node_id;
memory_hotplug.c:		struct zone *zone = &pgdat->node_zones[zid];
memory_hotplug.c:	return &pgdat->node_zones[ZONE_NORMAL];
memory_hotplug.c:	struct zone *movable_zone = &NODE_DATA(nid)->node_zones[ZONE_MOVABLE];
memory_hotplug.c:		return &NODE_DATA(nid)->node_zones[ZONE_MOVABLE];
memory_hotplug.c:	zone->present_pages += onlined_pages;
memory_hotplug.c:	pgdat_resize_lock(zone->zone_pgdat, &flags);
memory_hotplug.c:	zone->zone_pgdat->node_present_pages += onlined_pages;
memory_hotplug.c:	pgdat_resize_unlock(zone->zone_pgdat, &flags);
memory_hotplug.c:	pr_debug("online_pages [mem %#010llx-%#010llx] failed\n",
memory_hotplug.c:		 (((unsigned long long) pfn + nr_pages) << PAGE_SHIFT) - 1);
memory_hotplug.c:	for (z = pgdat->node_zones; z < pgdat->node_zones + MAX_NR_ZONES; z++)
memory_hotplug.c:		z->present_pages = 0;
memory_hotplug.c:	pgdat->node_present_pages = 0;
memory_hotplug.c:/* we are OK calling __meminit stuff here - we have CONFIG_MEMORY_HOTPLUG */
memory_hotplug.c:		pgdat->nr_zones = 0;
memory_hotplug.c:		pgdat->kswapd_order = 0;
memory_hotplug.c:		pgdat->kswapd_classzone_idx = 0;
memory_hotplug.c:	pgdat->per_cpu_nodestats = alloc_percpu(struct per_cpu_nodestat);
memory_hotplug.c:	 * to access not-initialized zonelist, build here.
memory_hotplug.c:	 * zone->managed_pages is set to an approximate value in
memory_hotplug.c:	 * When memory is hot-added, all the memory is in offline state. So
memory_hotplug.c:	free_percpu(pgdat->per_cpu_nodestats);
memory_hotplug.c: * try_online_node - online a node if offlined
memory_hotplug.c:		ret = -ENOMEM;
memory_hotplug.c:		pr_err("Section-unaligned hotplug range: start 0x%llx, size 0x%llx\n",
memory_hotplug.c:		return -EINVAL;
memory_hotplug.c:	return device_online(&mem->dev);
memory_hotplug.c:/* we are OK calling __meminit stuff here - we have CONFIG_MEMORY_HOTPLUG */
memory_hotplug.c:	start = res->start;
memory_hotplug.c:	{	/* Stupid hack to suppress address-never-null warning */
memory_hotplug.c:	 * be removed at hot-remove time.
memory_hotplug.c:		ret = -ENOMEM;
memory_hotplug.c:		 * can't be hot-added. There is no rollback way now.
memory_hotplug.c:		walk_memory_range(PFN_DOWN(start), PFN_UP(start + size - 1),
memory_hotplug.c: * A free page on the buddy free lists (not the per-cpu lists) has PageBuddy
memory_hotplug.c:	/* Ensure the starting page is pageblock-aligned */
memory_hotplug.c:	BUG_ON(page_to_pfn(page) & (pageblock_nr_pages - 1));
memory_hotplug.c:/* Checks if this range of memory is likely to be hot-removable. */
memory_hotplug.c:	/* All pageblocks in the memory block are likely to be hot-removable */
memory_hotplug.c: * non-lru movable pages and hugepages). We scan pfn because it's much
memory_hotplug.c:						1 << compound_order(page)) - 1;
memory_hotplug.c:			pfn = page_to_pfn(head) + (1<<compound_order(head)) - 1;
memory_hotplug.c:				ret = -EBUSY;
memory_hotplug.c:				move_pages -= 1 << compound_order(head);
memory_hotplug.c:				+ hpage_nr_pages(page) - 1;
memory_hotplug.c:		 * LRU and non-lru movable pages.
memory_hotplug.c:			list_add_tail(&page->lru, &source);
memory_hotplug.c:			move_pages--;
memory_hotplug.c:			/* Because we don't have big zone->lock. we should
memory_hotplug.c:				ret = -EBUSY;
memory_hotplug.c:	walk_system_ram_range(start_pfn, end_pfn - start_pfn, NULL,
memory_hotplug.c:	ret = walk_system_ram_range(start_pfn, end_pfn - start_pfn, &offlined,
memory_hotplug.c:	struct pglist_data *pgdat = zone->zone_pgdat;
memory_hotplug.c:		present_pages += pgdat->node_zones[zt].present_pages;
memory_hotplug.c:		arg->status_change_nid_normal = zone_to_nid(zone);
memory_hotplug.c:		arg->status_change_nid_normal = -1;
memory_hotplug.c:		present_pages += pgdat->node_zones[zt].present_pages;
memory_hotplug.c:		arg->status_change_nid_high = zone_to_nid(zone);
memory_hotplug.c:		arg->status_change_nid_high = -1;
memory_hotplug.c:	arg->status_change_nid_high = arg->status_change_nid_normal;
memory_hotplug.c:		present_pages += pgdat->node_zones[zt].present_pages;
memory_hotplug.c:		arg->status_change_nid = zone_to_nid(zone);
memory_hotplug.c:		arg->status_change_nid = -1;
memory_hotplug.c:	if (arg->status_change_nid_normal >= 0)
memory_hotplug.c:	    (arg->status_change_nid_high >= 0))
memory_hotplug.c:	    (arg->status_change_nid >= 0))
memory_hotplug.c:		return -EINVAL;
memory_hotplug.c:		return -EINVAL;
memory_hotplug.c:		return -EINVAL;
memory_hotplug.c:	nr_pages = end_pfn - start_pfn;
memory_hotplug.c:	ret = -EAGAIN;
memory_hotplug.c:	ret = -EINTR;
memory_hotplug.c:				if (--retry_max == 0)
memory_hotplug.c:		ret = -EBUSY;
memory_hotplug.c:	adjust_managed_page_count(pfn_to_page(start_pfn), -offlined_pages);
memory_hotplug.c:	zone->present_pages -= offlined_pages;
memory_hotplug.c:	pgdat_resize_lock(zone->zone_pgdat, &flags);
memory_hotplug.c:	zone->zone_pgdat->node_present_pages -= offlined_pages;
memory_hotplug.c:	pgdat_resize_unlock(zone->zone_pgdat, &flags);
memory_hotplug.c:	pr_debug("memory offlining [mem %#010llx-%#010llx] failed\n",
memory_hotplug.c:		 ((unsigned long long) end_pfn << PAGE_SHIFT) - 1);
memory_hotplug.c: * walk_memory_range - walks through all mem sections in [start_pfn, end_pfn)
memory_hotplug.c:			if ((section_nr >= mem->start_section_nr) &&
memory_hotplug.c:			    (section_nr <= mem->end_section_nr))
memory_hotplug.c:			kobject_put(&mem->dev.kobj);
memory_hotplug.c:		kobject_put(&mem->dev.kobj);
memory_hotplug.c:		beginpa = PFN_PHYS(section_nr_to_pfn(mem->start_section_nr));
memory_hotplug.c:		endpa = PFN_PHYS(section_nr_to_pfn(mem->end_section_nr + 1))-1;
memory_hotplug.c:		pr_warn("removing memory fails, because memory [%pa-%pa] is onlined\n",
memory_hotplug.c:		if (cpu_to_node(cpu) == pgdat->node_id)
memory_hotplug.c:			return -EBUSY;
memory_hotplug.c:		if (cpu_to_node(cpu) == pgdat->node_id)
memory_hotplug.c:	unsigned long start_pfn = pgdat->node_start_pfn;
memory_hotplug.c:	unsigned long end_pfn = start_pfn + pgdat->node_spanned_pages;
memory_hotplug.c:	ret = walk_memory_range(PFN_DOWN(start), PFN_UP(start + size - 1), NULL,
bootmem.c:// SPDX-License-Identifier: GPL-2.0
bootmem.c: *  bootmem - A boot-time physical memory allocator and configurator
bootmem.c: * bootmem_bootmap_pages - calculate bitmap size in pages
bootmem.c:		if (bdata->node_min_pfn < ent->node_min_pfn) {
bootmem.c:			list_add_tail(&bdata->list, &ent->list);
bootmem.c:	list_add_tail(&bdata->list, &bdata_list);
bootmem.c:	bdata->node_bootmem_map = phys_to_virt(PFN_PHYS(mapstart));
bootmem.c:	bdata->node_min_pfn = start;
bootmem.c:	bdata->node_low_pfn = end;
bootmem.c:	 * Initially all pages are reserved - setup_arch() has to
bootmem.c:	mapsize = bootmap_bytes(end - start);
bootmem.c:	memset(bdata->node_bootmem_map, 0xff, mapsize);
bootmem.c:		bdata - bootmem_node_data, start, mapstart, end, mapsize);
bootmem.c: * init_bootmem_node - register a node as boot memory
bootmem.c:	return init_bootmem_core(pgdat->bdata, freepfn, startpfn, endpfn);
bootmem.c: * init_bootmem - register boot memory
bootmem.c:	return init_bootmem_core(NODE_DATA(0)->bdata, start, 0, pages);
bootmem.c: * free_bootmem_late - free bootmem pages directly to page allocator
bootmem.c:	if (!bdata->node_bootmem_map)
bootmem.c:	map = bdata->node_bootmem_map;
bootmem.c:	start = bdata->node_min_pfn;
bootmem.c:	end = bdata->node_low_pfn;
bootmem.c:		bdata - bootmem_node_data, start, end);
bootmem.c:		idx = start - bdata->node_min_pfn;
bootmem.c:		shift = idx & (BITS_PER_LONG - 1);
bootmem.c:			if (end - start >= BITS_PER_LONG)
bootmem.c:					(BITS_PER_LONG - shift);
bootmem.c:	cur = bdata->node_min_pfn;
bootmem.c:	page = virt_to_page(bdata->node_bootmem_map);
bootmem.c:	pages = bdata->node_low_pfn - bdata->node_min_pfn;
bootmem.c:	while (pages--)
bootmem.c:	bdata->node_bootmem_map = NULL;
bootmem.c:	bdebug("nid=%td released=%lx\n", bdata - bootmem_node_data, count);
bootmem.c:	for (z = pgdat->node_zones; z < pgdat->node_zones + MAX_NR_ZONES; z++)
bootmem.c:		z->managed_pages = 0;
bootmem.c: * free_all_bootmem - release free pages to the buddy allocator
bootmem.c:	bdebug("nid=%td start=%lx end=%lx\n", bdata - bootmem_node_data,
bootmem.c:		sidx + bdata->node_min_pfn,
bootmem.c:		eidx + bdata->node_min_pfn);
bootmem.c:	if (WARN_ON(bdata->node_bootmem_map == NULL))
bootmem.c:	if (bdata->hint_idx > sidx)
bootmem.c:		bdata->hint_idx = sidx;
bootmem.c:		if (!test_and_clear_bit(idx, bdata->node_bootmem_map))
bootmem.c:		bdata - bootmem_node_data,
bootmem.c:		sidx + bdata->node_min_pfn,
bootmem.c:		eidx + bdata->node_min_pfn,
bootmem.c:	if (WARN_ON(bdata->node_bootmem_map == NULL))
bootmem.c:		if (test_and_set_bit(idx, bdata->node_bootmem_map)) {
bootmem.c:				return -EBUSY;
bootmem.c:				idx + bdata->node_min_pfn);
bootmem.c:		bdata - bootmem_node_data, start, end, reserve, flags);
bootmem.c:	BUG_ON(start < bdata->node_min_pfn);
bootmem.c:	BUG_ON(end > bdata->node_low_pfn);
bootmem.c:	sidx = start - bdata->node_min_pfn;
bootmem.c:	eidx = end - bdata->node_min_pfn;
bootmem.c:		if (pos < bdata->node_min_pfn ||
bootmem.c:		    pos >= bdata->node_low_pfn) {
bootmem.c:		max = min(bdata->node_low_pfn, end);
bootmem.c:		pos = bdata->node_low_pfn;
bootmem.c: * free_bootmem_node - mark a page range as usable
bootmem.c:	mark_bootmem_node(pgdat->bdata, start, end, 0, 0);
bootmem.c: * free_bootmem - mark a page range as usable
bootmem.c: * reserve_bootmem_node - mark a page range as reserved
bootmem.c:	return mark_bootmem_node(pgdat->bdata, start, end, 1, flags);
bootmem.c: * reserve_bootmem - mark a page range as reserved
bootmem.c:	unsigned long base = bdata->node_min_pfn;
bootmem.c:	return ALIGN(base + idx, step) - base;
bootmem.c:	unsigned long base = PFN_PHYS(bdata->node_min_pfn);
bootmem.c:	return ALIGN(base + off, align) - base;
bootmem.c:		bdata - bootmem_node_data, size, PAGE_ALIGN(size) >> PAGE_SHIFT,
bootmem.c:	BUG_ON(align & (align - 1));
bootmem.c:	if (!bdata->node_bootmem_map)
bootmem.c:	min = bdata->node_min_pfn;
bootmem.c:	max = bdata->node_low_pfn;
bootmem.c:	sidx = start - bdata->node_min_pfn;
bootmem.c:	midx = max - bdata->node_min_pfn;
bootmem.c:	if (bdata->hint_idx > sidx) {
bootmem.c:		sidx = align_idx(bdata, bdata->hint_idx, step);
bootmem.c:		sidx = find_next_zero_bit(bdata->node_bootmem_map, midx, sidx);
bootmem.c:			if (test_bit(i, bdata->node_bootmem_map)) {
bootmem.c:		if (bdata->last_end_off & (PAGE_SIZE - 1) &&
bootmem.c:				PFN_DOWN(bdata->last_end_off) + 1 == sidx)
bootmem.c:			start_off = align_off(bdata, bdata->last_end_off, align);
bootmem.c:		bdata->last_end_off = end_off;
bootmem.c:		bdata->hint_idx = PFN_UP(end_off);
bootmem.c:		region = phys_to_virt(PFN_PHYS(bdata->node_min_pfn) +
bootmem.c:		sidx = align_idx(bdata, fallback - 1, step);
bootmem.c:		if (goal && bdata->node_low_pfn <= PFN_DOWN(goal))
bootmem.c:		if (limit && bdata->node_min_pfn >= PFN_DOWN(limit))
bootmem.c: * __alloc_bootmem_nopanic - allocate boot memory without panicking
bootmem.c: * __alloc_bootmem - allocate boot memory
bootmem.c:		return kzalloc_node(size, GFP_NOWAIT, pgdat->node_id);
bootmem.c:	ptr = alloc_bootmem_bdata(pgdat->bdata, size, align, goal, limit);
bootmem.c: * __alloc_bootmem_node - allocate boot memory from a specific node
bootmem.c:		return kzalloc_node(size, GFP_NOWAIT, pgdat->node_id);
bootmem.c:		return kzalloc_node(size, GFP_NOWAIT, pgdat->node_id);
bootmem.c:	if (end_pfn > MAX_DMA32_PFN + (128 >> (20 - PAGE_SHIFT)) &&
bootmem.c:		ptr = alloc_bootmem_bdata(pgdat->bdata, size, align,
bootmem.c: * __alloc_bootmem_low - allocate low boot memory
bootmem.c: * __alloc_bootmem_low_node - allocate low boot memory from a specific node
bootmem.c:		return kzalloc_node(size, GFP_NOWAIT, pgdat->node_id);
failslab.c:// SPDX-License-Identifier: GPL-2.0
failslab.c:#include <linux/fault-inject.h>
failslab.c:	/* No fault-injection for bootstrap cache */
failslab.c:	if (failslab.cache_filter && !(s->flags & SLAB_FAILSLAB))
failslab.c:	return should_fail(&failslab.attr, s->object_size);
failslab.c:	if (!debugfs_create_bool("ignore-gfp-wait", mode, dir,
failslab.c:	if (!debugfs_create_bool("cache-filter", mode, dir,
failslab.c:	return -ENOMEM;
percpu-vm.c: * mm/percpu-vm.c - vmalloc area based chunk allocation
percpu-vm.c:	/* must not be used on pre-mapped chunk */
percpu-vm.c:	WARN_ON(chunk->immutable);
percpu-vm.c: * pcpu_get_pages - get temp pages array
percpu-vm.c: * pcpu_free_pages - free pages which were allocated for @chunk
percpu-vm.c: * pcpu_alloc_pages - allocates pages for @chunk
percpu-vm.c:	while (--i >= page_start)
percpu-vm.c:	return -ENOMEM;
percpu-vm.c: * pcpu_pre_unmap_flush - flush cache prior to unmapping
percpu-vm.c: * pcpu_unmap_pages - unmap pages out of a pcpu_chunk
percpu-vm.c:				   page_end - page_start);
percpu-vm.c: * pcpu_post_unmap_tlb_flush - flush TLB after unmapping
percpu-vm.c: * pcpu_map_pages - map pages into a pcpu_chunk
percpu-vm.c: * reverse lookup (addr -> chunk).
percpu-vm.c:				       page_end - page_start);
percpu-vm.c:				   page_end - page_start);
percpu-vm.c: * pcpu_post_map_flush - flush cache after mapping
percpu-vm.c: * pcpu_populate_chunk - populate and map an area of a pcpu_chunk
percpu-vm.c:		return -ENOMEM;
percpu-vm.c:		return -ENOMEM;
percpu-vm.c:		return -ENOMEM;
percpu-vm.c: * pcpu_depopulate_chunk - depopulate and unmap an area of a pcpu_chunk
percpu-vm.c:	chunk->data = vms;
percpu-vm.c:	chunk->base_addr = vms[0]->addr - pcpu_group_offsets[0];
percpu-vm.c:	trace_percpu_create_chunk(chunk->base_addr);
percpu-vm.c:	trace_percpu_destroy_chunk(chunk->base_addr);
percpu-vm.c:	if (chunk->data)
percpu-vm.c:		pcpu_free_vm_areas(chunk->data, pcpu_nr_groups);
quicklist.c:// SPDX-License-Identifier: GPL-2.0
quicklist.c:	struct zone *zones = NODE_DATA(node)->node_zones;
quicklist.c:	pages_to_free = q->nr_pages - max_pages(min_pages);
quicklist.c:	if (q->nr_pages > min_pages) {
quicklist.c:			pages_to_free--;
quicklist.c:			count += q->nr_pages;
shmem.c: *		 2000-2001 Christoph Rohland
shmem.c: *		 2000-2001 SAP AG
shmem.c: * Copyright (C) 2002-2011 Hugh Dickins.
shmem.c: * Copyright (C) 2002-2005 VERITAS Software Corporation.
shmem.c: * tiny-shmem:
shmem.c:#include <linux/backing-dev.h>
shmem.c: * inode->i_private (with i_mutex making sure that it has only one user at
shmem.c:	return min(totalram_pages - totalhigh_pages, totalram_pages / 2);
shmem.c:		mapping_gfp_mask(inode->i_mapping), NULL, NULL, NULL);
shmem.c:	return sb->s_fs_info;
shmem.c: * shmem_file_setup pre-accounts the whole fixed size of a VM object,
shmem.c: * consistent with the pre-accounting of private mappings ...
shmem.c:		0 : security_vm_enough_memory_mm(current->mm, VM_ACCT(size));
shmem.c:			return security_vm_enough_memory_mm(current->mm,
shmem.c:					VM_ACCT(newsize) - VM_ACCT(oldsize));
shmem.c:			vm_unacct_memory(VM_ACCT(oldsize) - VM_ACCT(newsize));
shmem.c: * shmem_getpage reports shmem_acct_block failure as -ENOSPC not -ENOMEM,
shmem.c:	return security_vm_enough_memory_mm(current->mm,
shmem.c:	struct shmem_sb_info *sbinfo = SHMEM_SB(inode->i_sb);
shmem.c:	if (shmem_acct_block(info->flags, pages))
shmem.c:	if (sbinfo->max_blocks) {
shmem.c:		if (percpu_counter_compare(&sbinfo->used_blocks,
shmem.c:					   sbinfo->max_blocks - pages) > 0)
shmem.c:		percpu_counter_add(&sbinfo->used_blocks, pages);
shmem.c:	shmem_unacct_blocks(info->flags, pages);
shmem.c:	struct shmem_sb_info *sbinfo = SHMEM_SB(inode->i_sb);
shmem.c:	if (sbinfo->max_blocks)
shmem.c:		percpu_counter_sub(&sbinfo->used_blocks, pages);
shmem.c:	shmem_unacct_blocks(info->flags, pages);
shmem.c:	return vma->vm_ops == &shmem_vm_ops;
shmem.c:	if (sbinfo->max_inodes) {
shmem.c:		spin_lock(&sbinfo->stat_lock);
shmem.c:		if (!sbinfo->free_inodes) {
shmem.c:			spin_unlock(&sbinfo->stat_lock);
shmem.c:			return -ENOSPC;
shmem.c:		sbinfo->free_inodes--;
shmem.c:		spin_unlock(&sbinfo->stat_lock);
shmem.c:	if (sbinfo->max_inodes) {
shmem.c:		spin_lock(&sbinfo->stat_lock);
shmem.c:		sbinfo->free_inodes++;
shmem.c:		spin_unlock(&sbinfo->stat_lock);
shmem.c: * shmem_recalc_inode - recalculate the block usage of an inode
shmem.c: * But normally   info->alloced == inode->i_mapping->nrpages + info->swapped
shmem.c: * So mm freed is info->alloced - (inode->i_mapping->nrpages + info->swapped)
shmem.c:	freed = info->alloced - info->swapped - inode->i_mapping->nrpages;
shmem.c:		info->alloced -= freed;
shmem.c:		inode->i_blocks -= freed * BLOCKS_PER_PAGE;
shmem.c:	spin_lock_irqsave(&info->lock, flags);
shmem.c:	info->alloced += pages;
shmem.c:	inode->i_blocks += pages * BLOCKS_PER_PAGE;
shmem.c:	spin_unlock_irqrestore(&info->lock, flags);
shmem.c:	inode->i_mapping->nrpages += pages;
shmem.c:	spin_lock_irqsave(&info->lock, flags);
shmem.c:	info->alloced -= pages;
shmem.c:	inode->i_blocks -= pages * BLOCKS_PER_PAGE;
shmem.c:	spin_unlock_irqrestore(&info->lock, flags);
shmem.c:	item = __radix_tree_lookup(&mapping->page_tree, index, &node, &pslot);
shmem.c:		return -ENOENT;
shmem.c:		return -ENOENT;
shmem.c:	__radix_tree_replace(&mapping->page_tree, node, pslot,
shmem.c:	item = radix_tree_lookup(&mapping->page_tree, index);
shmem.c:#define SHMEM_HUGE_DENY		(-1)
shmem.c:#define SHMEM_HUGE_FORCE	(-2)
shmem.c:	return -EINVAL;
shmem.c:	unsigned long batch = sc ? sc->nr_to_scan : 128;
shmem.c:	if (list_empty(&sbinfo->shrinklist))
shmem.c:	spin_lock(&sbinfo->shrinklist_lock);
shmem.c:	list_for_each_safe(pos, next, &sbinfo->shrinklist) {
shmem.c:		inode = igrab(&info->vfs_inode);
shmem.c:			list_del_init(&info->shrinklist);
shmem.c:		if (round_up(inode->i_size, PAGE_SIZE) ==
shmem.c:				round_up(inode->i_size, HPAGE_PMD_SIZE)) {
shmem.c:			list_move(&info->shrinklist, &to_remove);
shmem.c:		list_move(&info->shrinklist, &list);
shmem.c:		if (!--batch)
shmem.c:	spin_unlock(&sbinfo->shrinklist_lock);
shmem.c:		inode = &info->vfs_inode;
shmem.c:		list_del_init(&info->shrinklist);
shmem.c:		inode = &info->vfs_inode;
shmem.c:		page = find_get_page(inode->i_mapping,
shmem.c:				(inode->i_size & HPAGE_PMD_MASK) >> PAGE_SHIFT);
shmem.c:		list_del_init(&info->shrinklist);
shmem.c:	spin_lock(&sbinfo->shrinklist_lock);
shmem.c:	list_splice_tail(&list, &sbinfo->shrinklist);
shmem.c:	sbinfo->shrinklist_len -= removed;
shmem.c:	spin_unlock(&sbinfo->shrinklist_lock);
shmem.c:	if (!READ_ONCE(sbinfo->shrinklist_len))
shmem.c:	return READ_ONCE(sbinfo->shrinklist_len);
shmem.c:	page->mapping = mapping;
shmem.c:	page->index = index;
shmem.c:	spin_lock_irq(&mapping->tree_lock);
shmem.c:		if (radix_tree_gang_lookup_slot(&mapping->page_tree,
shmem.c:			error = -EEXIST;
shmem.c:				error = radix_tree_insert(&mapping->page_tree,
shmem.c:		error = radix_tree_insert(&mapping->page_tree, index, page);
shmem.c:		mapping->nrpages += nr;
shmem.c:		spin_unlock_irq(&mapping->tree_lock);
shmem.c:		page->mapping = NULL;
shmem.c:		spin_unlock_irq(&mapping->tree_lock);
shmem.c:	struct address_space *mapping = page->mapping;
shmem.c:	spin_lock_irq(&mapping->tree_lock);
shmem.c:	error = shmem_radix_tree_replace(mapping, page->index, page, radswap);
shmem.c:	page->mapping = NULL;
shmem.c:	mapping->nrpages--;
shmem.c:	spin_unlock_irq(&mapping->tree_lock);
shmem.c:	spin_lock_irq(&mapping->tree_lock);
shmem.c:	old = radix_tree_delete_item(&mapping->page_tree, index, radswap);
shmem.c:	spin_unlock_irq(&mapping->tree_lock);
shmem.c:		return -ENOENT;
shmem.c: * This is safe to call without i_mutex or mapping->tree_lock thanks to RCU,
shmem.c:	radix_tree_for_each_slot(slot, &mapping->page_tree, &iter, start) {
shmem.c: * This is safe to call without i_mutex or mapping->tree_lock thanks to RCU,
shmem.c:	struct inode *inode = file_inode(vma->vm_file);
shmem.c:	struct address_space *mapping = inode->i_mapping;
shmem.c:	/* Be careful as we don't hold info->lock */
shmem.c:	swapped = READ_ONCE(info->swapped);
shmem.c:	if (!vma->vm_pgoff && vma->vm_end - vma->vm_start >= inode->i_size)
shmem.c:			linear_page_index(vma, vma->vm_start),
shmem.c:			linear_page_index(vma, vma->vm_end));
shmem.c:		index = indices[pvec.nr - 1] + 1;
shmem.c:	struct address_space *mapping = inode->i_mapping;
shmem.c:	pgoff_t start = (lstart + PAGE_SIZE - 1) >> PAGE_SHIFT;
shmem.c:	unsigned int partial_start = lstart & (PAGE_SIZE - 1);
shmem.c:	unsigned int partial_end = (lend + 1) & (PAGE_SIZE - 1);
shmem.c:	if (lend == -1)
shmem.c:		end = -1;	/* unsigned, so actually very big */
shmem.c:			min(end - index, (pgoff_t)PAGEVEC_SIZE),
shmem.c:				index += HPAGE_PMD_NR - 1;
shmem.c:				i += HPAGE_PMD_NR - 1;
shmem.c:		shmem_getpage(inode, start - 1, &page, SGP_READ);
shmem.c:				min(end - index, (pgoff_t)PAGEVEC_SIZE),
shmem.c:			/* If all gone or hole-punch or unfalloc, we're done */
shmem.c:			if (index == start || end != -1)
shmem.c:					index--;
shmem.c:				index += HPAGE_PMD_NR - 1;
shmem.c:				i += HPAGE_PMD_NR - 1;
shmem.c:					index--;
shmem.c:	spin_lock_irq(&info->lock);
shmem.c:	info->swapped -= nr_swaps_freed;
shmem.c:	spin_unlock_irq(&info->lock);
shmem.c:	inode->i_ctime = inode->i_mtime = current_time(inode);
shmem.c:	struct inode *inode = path->dentry->d_inode;
shmem.c:	if (info->alloced - info->swapped != inode->i_mapping->nrpages) {
shmem.c:		spin_lock_irq(&info->lock);
shmem.c:		spin_unlock_irq(&info->lock);
shmem.c:	struct shmem_sb_info *sbinfo = SHMEM_SB(inode->i_sb);
shmem.c:	if (S_ISREG(inode->i_mode) && (attr->ia_valid & ATTR_SIZE)) {
shmem.c:		loff_t oldsize = inode->i_size;
shmem.c:		loff_t newsize = attr->ia_size;
shmem.c:		if ((newsize < oldsize && (info->seals & F_SEAL_SHRINK)) ||
shmem.c:		    (newsize > oldsize && (info->seals & F_SEAL_GROW)))
shmem.c:			return -EPERM;
shmem.c:			error = shmem_reacct_size(SHMEM_I(inode)->flags,
shmem.c:			inode->i_ctime = inode->i_mtime = current_time(inode);
shmem.c:				unmap_mapping_range(inode->i_mapping,
shmem.c:			if (info->alloced)
shmem.c:							newsize, (loff_t)-1);
shmem.c:				unmap_mapping_range(inode->i_mapping,
shmem.c:				spin_lock(&sbinfo->shrinklist_lock);
shmem.c:				 * ->shrink_list in shmem_unused_huge_shrink()
shmem.c:				if (list_empty_careful(&info->shrinklist)) {
shmem.c:					list_add_tail(&info->shrinklist,
shmem.c:							&sbinfo->shrinklist);
shmem.c:					sbinfo->shrinklist_len++;
shmem.c:				spin_unlock(&sbinfo->shrinklist_lock);
shmem.c:	if (attr->ia_valid & ATTR_MODE)
shmem.c:		error = posix_acl_chmod(inode, inode->i_mode);
shmem.c:	struct shmem_sb_info *sbinfo = SHMEM_SB(inode->i_sb);
shmem.c:	if (inode->i_mapping->a_ops == &shmem_aops) {
shmem.c:		shmem_unacct_size(info->flags, inode->i_size);
shmem.c:		inode->i_size = 0;
shmem.c:		shmem_truncate_range(inode, 0, (loff_t)-1);
shmem.c:		if (!list_empty(&info->shrinklist)) {
shmem.c:			spin_lock(&sbinfo->shrinklist_lock);
shmem.c:			if (!list_empty(&info->shrinklist)) {
shmem.c:				list_del_init(&info->shrinklist);
shmem.c:				sbinfo->shrinklist_len--;
shmem.c:			spin_unlock(&sbinfo->shrinklist_lock);
shmem.c:		if (!list_empty(&info->swaplist)) {
shmem.c:			list_del_init(&info->swaplist);
shmem.c:	simple_xattrs_free(&info->xattrs);
shmem.c:	WARN_ON(inode->i_blocks);
shmem.c:	shmem_free_inode(inode->i_sb);
shmem.c:	unsigned long found = -1;
shmem.c:	struct address_space *mapping = info->vfs_inode.i_mapping;
shmem.c:	index = find_swap_entry(&mapping->page_tree, radswap);
shmem.c:	if (index == -1)
shmem.c:		return -EAGAIN;	/* tell shmem_unuse we found nothing */
shmem.c:	 * mutex, and there's an instant in list_move_tail when info->swaplist
shmem.c:	if (shmem_swaplist.next != &info->swaplist)
shmem.c:		list_move_tail(&shmem_swaplist, &info->swaplist);
shmem.c:		 * on this swapcache page is not enough to prevent that -
shmem.c:			error = -ENOENT;
shmem.c:	if (error != -ENOMEM) {
shmem.c:			spin_lock_irq(&info->lock);
shmem.c:			info->swapped--;
shmem.c:			spin_unlock_irq(&info->lock);
shmem.c:	error = mem_cgroup_try_charge(page, current->mm, GFP_KERNEL, &memcg,
shmem.c:	error = -EAGAIN;
shmem.c:		if (info->swapped)
shmem.c:			list_del_init(&info->swaplist);
shmem.c:		if (error != -EAGAIN)
shmem.c:		if (error != -ENOMEM)
shmem.c:	mapping = page->mapping;
shmem.c:	index = page->index;
shmem.c:	inode = mapping->host;
shmem.c:	if (info->flags & VM_LOCKED)
shmem.c:	 * shmem_writepage; but a stacking filesystem might use ->writepage of
shmem.c:	if (!wbc->for_reclaim) {
shmem.c:		if (inode->i_private) {
shmem.c:			spin_lock(&inode->i_lock);
shmem.c:			shmem_falloc = inode->i_private;
shmem.c:			    !shmem_falloc->waitq &&
shmem.c:			    index >= shmem_falloc->start &&
shmem.c:			    index < shmem_falloc->next)
shmem.c:				shmem_falloc->nr_unswapped++;
shmem.c:			spin_unlock(&inode->i_lock);
shmem.c:	 * Add inode to shmem_unuse()'s list of swapped-out inodes,
shmem.c:	if (list_empty(&info->swaplist))
shmem.c:		list_add_tail(&info->swaplist, &shmem_swaplist);
shmem.c:		spin_lock_irq(&info->lock);
shmem.c:		info->swapped++;
shmem.c:		spin_unlock_irq(&info->lock);
shmem.c:	if (wbc->for_reclaim)
shmem.c:	if (!mpol || mpol->mode == MPOL_DEFAULT)
shmem.c:	if (sbinfo->mpol) {
shmem.c:		spin_lock(&sbinfo->stat_lock);	/* prevent replace/use races */
shmem.c:		mpol = sbinfo->mpol;
shmem.c:		spin_unlock(&sbinfo->stat_lock);
shmem.c:	vma->vm_start = 0;
shmem.c:	vma->vm_pgoff = index + info->vfs_inode.i_ino;
shmem.c:	vma->vm_ops = NULL;
shmem.c:	vma->vm_policy = mpol_shared_policy_lookup(&info->policy, index);
shmem.c:	mpol_cond_put(vma->vm_policy);
shmem.c:	struct inode *inode = &info->vfs_inode;
shmem.c:	struct address_space *mapping = inode->i_mapping;
shmem.c:	if (radix_tree_gang_lookup_slot(&mapping->page_tree, &results, &idx,
shmem.c:	int err = -ENOSPC;
shmem.c:	err = -ENOMEM;
shmem.c:		return -ENOMEM;
shmem.c:	spin_lock_irq(&swap_mapping->tree_lock);
shmem.c:	spin_unlock_irq(&swap_mapping->tree_lock);
shmem.c: * shmem_getpage_gfp - find page in cache, or get from swap, or allocate
shmem.c:	struct address_space *mapping = inode->i_mapping;
shmem.c:		return -EFBIG;
shmem.c:		error = -EINVAL;
shmem.c:	sbinfo = SHMEM_SB(inode->i_sb);
shmem.c:	charge_mm = vma ? vma->vm_mm : current->mm;
shmem.c:				error = -ENOMEM;
shmem.c:			error = -EEXIST;	/* try again */
shmem.c:			error = -EIO;
shmem.c:		spin_lock_irq(&info->lock);
shmem.c:		info->swapped--;
shmem.c:		spin_unlock_irq(&info->lock);
shmem.c:		if (mapping->a_ops != &shmem_aops)
shmem.c:		switch (sbinfo->huge) {
shmem.c:			if (error != -ENOSPC)
shmem.c:			while (retry--) {
shmem.c:		spin_lock_irq(&info->lock);
shmem.c:		info->alloced += 1 << compound_order(page);
shmem.c:		inode->i_blocks += BLOCKS_PER_PAGE << compound_order(page);
shmem.c:		spin_unlock_irq(&info->lock);
shmem.c:				hindex + HPAGE_PMD_NR - 1) {
shmem.c:			spin_lock(&sbinfo->shrinklist_lock);
shmem.c:			 * ->shrink_list in shmem_unused_huge_shrink()
shmem.c:			if (list_empty_careful(&info->shrinklist)) {
shmem.c:				list_add_tail(&info->shrinklist,
shmem.c:						&sbinfo->shrinklist);
shmem.c:				sbinfo->shrinklist_len++;
shmem.c:			spin_unlock(&sbinfo->shrinklist_lock);
shmem.c:			spin_lock_irq(&info->lock);
shmem.c:			spin_unlock_irq(&info->lock);
shmem.c:		error = -EINVAL;
shmem.c:	*pagep = page + index - hindex;
shmem.c:		error = -EEXIST;
shmem.c:	if (error == -ENOSPC && !once++) {
shmem.c:		spin_lock_irq(&info->lock);
shmem.c:		spin_unlock_irq(&info->lock);
shmem.c:	if (error == -EEXIST)	/* from above or from radix_tree_insert */
shmem.c: * entry unconditionally - even if something else had already woken the
shmem.c:	list_del_init(&wait->entry);
shmem.c:	struct vm_area_struct *vma = vmf->vma;
shmem.c:	struct inode *inode = file_inode(vma->vm_file);
shmem.c:	gfp_t gfp = mapping_gfp_mask(inode->i_mapping);
shmem.c:	 * prevent the hole-punch from ever completing: which in turn
shmem.c:	 * hole-punch begins, so that one fault then races with the punch:
shmem.c:	if (unlikely(inode->i_private)) {
shmem.c:		spin_lock(&inode->i_lock);
shmem.c:		shmem_falloc = inode->i_private;
shmem.c:		    shmem_falloc->waitq &&
shmem.c:		    vmf->pgoff >= shmem_falloc->start &&
shmem.c:		    vmf->pgoff < shmem_falloc->next) {
shmem.c:			if ((vmf->flags & FAULT_FLAG_ALLOW_RETRY) &&
shmem.c:			   !(vmf->flags & FAULT_FLAG_RETRY_NOWAIT)) {
shmem.c:				up_read(&vma->vm_mm->mmap_sem);
shmem.c:			shmem_falloc_waitq = shmem_falloc->waitq;
shmem.c:			spin_unlock(&inode->i_lock);
shmem.c:			 * stack of the hole-punching task: shmem_falloc_waitq
shmem.c:			spin_lock(&inode->i_lock);
shmem.c:			spin_unlock(&inode->i_lock);
shmem.c:		spin_unlock(&inode->i_lock);
shmem.c:	if ((vma->vm_flags & VM_NOHUGEPAGE) ||
shmem.c:	    test_bit(MMF_DISABLE_THP, &vma->vm_mm->flags))
shmem.c:	else if (vma->vm_flags & VM_HUGEPAGE)
shmem.c:	error = shmem_getpage_gfp(inode, vmf->pgoff, &vmf->page, sgp,
shmem.c:		return ((error == -ENOMEM) ? VM_FAULT_OOM : VM_FAULT_SIGBUS);
shmem.c:		return -ENOMEM;
shmem.c:	get_area = current->mm->get_unmapped_area;
shmem.c:	if (addr > TASK_SIZE - len)
shmem.c:			VM_BUG_ON(file->f_op != &shmem_file_operations);
shmem.c:			sb = file_inode(file)->i_sb;
shmem.c:			sb = shm_mnt->mnt_sb;
shmem.c:		if (SHMEM_SB(sb)->huge == SHMEM_HUGE_NEVER)
shmem.c:	offset = (pgoff << PAGE_SHIFT) & (HPAGE_PMD_SIZE-1);
shmem.c:	if ((addr & (HPAGE_PMD_SIZE-1)) == offset)
shmem.c:	inflated_len = len + HPAGE_PMD_SIZE - PAGE_SIZE;
shmem.c:	inflated_offset = inflated_addr & (HPAGE_PMD_SIZE-1);
shmem.c:	inflated_addr += offset - inflated_offset;
shmem.c:	if (inflated_addr > TASK_SIZE - len)
shmem.c:	struct inode *inode = file_inode(vma->vm_file);
shmem.c:	return mpol_set_shared_policy(&SHMEM_I(inode)->policy, vma, mpol);
shmem.c:	struct inode *inode = file_inode(vma->vm_file);
shmem.c:	index = ((addr - vma->vm_start) >> PAGE_SHIFT) + vma->vm_pgoff;
shmem.c:	return mpol_shared_policy_lookup(&SHMEM_I(inode)->policy, index);
shmem.c:	int retval = -ENOMEM;
shmem.c:	spin_lock_irq(&info->lock);
shmem.c:	if (lock && !(info->flags & VM_LOCKED)) {
shmem.c:		if (!user_shm_lock(inode->i_size, user))
shmem.c:		info->flags |= VM_LOCKED;
shmem.c:		mapping_set_unevictable(file->f_mapping);
shmem.c:	if (!lock && (info->flags & VM_LOCKED) && user) {
shmem.c:		user_shm_unlock(inode->i_size, user);
shmem.c:		info->flags &= ~VM_LOCKED;
shmem.c:		mapping_clear_unevictable(file->f_mapping);
shmem.c:	spin_unlock_irq(&info->lock);
shmem.c:	vma->vm_ops = &shmem_vm_ops;
shmem.c:			((vma->vm_start + ~HPAGE_PMD_MASK) & HPAGE_PMD_MASK) <
shmem.c:			(vma->vm_end & HPAGE_PMD_MASK)) {
shmem.c:		khugepaged_enter(vma, vma->vm_flags);
shmem.c:		inode->i_ino = get_next_ino();
shmem.c:		inode->i_blocks = 0;
shmem.c:		inode->i_atime = inode->i_mtime = inode->i_ctime = current_time(inode);
shmem.c:		inode->i_generation = get_seconds();
shmem.c:		memset(info, 0, (char *)inode - (char *)info);
shmem.c:		spin_lock_init(&info->lock);
shmem.c:		info->seals = F_SEAL_SEAL;
shmem.c:		info->flags = flags & VM_NORESERVE;
shmem.c:		INIT_LIST_HEAD(&info->shrinklist);
shmem.c:		INIT_LIST_HEAD(&info->swaplist);
shmem.c:		simple_xattrs_init(&info->xattrs);
shmem.c:			inode->i_op = &shmem_special_inode_operations;
shmem.c:			inode->i_mapping->a_ops = &shmem_aops;
shmem.c:			inode->i_op = &shmem_inode_operations;
shmem.c:			inode->i_fop = &shmem_file_operations;
shmem.c:			mpol_shared_policy_init(&info->policy,
shmem.c:			inode->i_size = 2 * BOGO_DIRENT_SIZE;
shmem.c:			inode->i_op = &shmem_dir_inode_operations;
shmem.c:			inode->i_fop = &simple_dir_operations;
shmem.c:			mpol_shared_policy_init(&info->policy, NULL);
shmem.c:	return mapping->a_ops == &shmem_aops;
shmem.c:	struct inode *inode = file_inode(dst_vma->vm_file);
shmem.c:	struct address_space *mapping = inode->i_mapping;
shmem.c:	ret = -ENOMEM;
shmem.c:				return -EFAULT;
shmem.c:	_dst_pte = mk_pte(page, dst_vma->vm_page_prot);
shmem.c:	if (dst_vma->vm_flags & VM_WRITE)
shmem.c:	ret = -EEXIST;
shmem.c:	spin_lock(&info->lock);
shmem.c:	info->alloced++;
shmem.c:	inode->i_blocks += BLOCKS_PER_PAGE;
shmem.c:	spin_unlock(&info->lock);
shmem.c:	/* No need to invalidate - it was non-present before */
shmem.c:	struct inode *inode = mapping->host;
shmem.c:	if (unlikely(info->seals & (F_SEAL_WRITE | F_SEAL_GROW))) {
shmem.c:		if (info->seals & F_SEAL_WRITE)
shmem.c:			return -EPERM;
shmem.c:		if ((info->seals & F_SEAL_GROW) && pos + len > inode->i_size)
shmem.c:			return -EPERM;
shmem.c:	struct inode *inode = mapping->host;
shmem.c:	if (pos + copied > inode->i_size)
shmem.c:			unsigned from = pos & (PAGE_SIZE - 1);
shmem.c:	struct file *file = iocb->ki_filp;
shmem.c:	struct address_space *mapping = inode->i_mapping;
shmem.c:	loff_t *ppos = &iocb->ki_pos;
shmem.c:			if (error == -EINVAL)
shmem.c:		nr -= offset;
shmem.c:		 * Ok, we have the page, and it's up-to-date, so
shmem.c:			error = -EFAULT;
shmem.c:	struct address_space *mapping = file->f_mapping;
shmem.c:	struct inode *inode = mapping->host;
shmem.c:		offset = -EINVAL;
shmem.c:	else if (offset >= inode->i_size)
shmem.c:		offset = -ENXIO;
shmem.c:		end = (inode->i_size + PAGE_SIZE - 1) >> PAGE_SHIFT;
shmem.c:			if (new_offset < inode->i_size)
shmem.c:				offset = -ENXIO;
shmem.c:				offset = inode->i_size;
shmem.c:	radix_tree_for_each_slot(slot, &mapping->page_tree, &iter, start) {
shmem.c:		} else if (page_count(page) - page_mapcount(page) > 1) {
shmem.c:			spin_lock_irq(&mapping->tree_lock);
shmem.c:			radix_tree_tag_set(&mapping->page_tree, iter.index,
shmem.c:			spin_unlock_irq(&mapping->tree_lock);
shmem.c: * user-space mappings (eg., direct-IO, AIO). Therefore, we look at all pages
shmem.c: * and see whether it has an elevated ref-count. If so, we tag them and wait for
shmem.c:		if (!radix_tree_tagged(&mapping->page_tree, SHMEM_TAG_PINNED))
shmem.c:		radix_tree_for_each_tagged(slot, &mapping->page_tree, &iter,
shmem.c:			    page_count(page) - page_mapcount(page) != 1) {
shmem.c:				error = -EBUSY;
shmem.c:			spin_lock_irq(&mapping->tree_lock);
shmem.c:			radix_tree_tag_clear(&mapping->page_tree,
shmem.c:			spin_unlock_irq(&mapping->tree_lock);
shmem.c:	 * Sealing allows multiple parties to share a shmem-file but restrict
shmem.c:	 * share common memory regions with a well-defined policy. A malicious
shmem.c:	 * Seals are only supported on special shmem-files and always affect
shmem.c:	 * existing seals. Furthermore, the "setting seals"-operation can be
shmem.c:	if (file->f_op != &shmem_file_operations)
shmem.c:		return -EINVAL;
shmem.c:	if (!(file->f_mode & FMODE_WRITE))
shmem.c:		return -EPERM;
shmem.c:		return -EINVAL;
shmem.c:	if (info->seals & F_SEAL_SEAL) {
shmem.c:		error = -EPERM;
shmem.c:	if ((seals & F_SEAL_WRITE) && !(info->seals & F_SEAL_WRITE)) {
shmem.c:		error = mapping_deny_writable(file->f_mapping);
shmem.c:		error = shmem_wait_for_pins(file->f_mapping);
shmem.c:			mapping_allow_writable(file->f_mapping);
shmem.c:	info->seals |= seals;
shmem.c:	if (file->f_op != &shmem_file_operations)
shmem.c:		return -EINVAL;
shmem.c:	return SHMEM_I(file_inode(file))->seals;
shmem.c:			return -EINVAL;
shmem.c:		error = -EINVAL;
shmem.c:	struct shmem_sb_info *sbinfo = SHMEM_SB(inode->i_sb);
shmem.c:		return -EOPNOTSUPP;
shmem.c:		struct address_space *mapping = file->f_mapping;
shmem.c:		loff_t unmap_end = round_down(offset + len, PAGE_SIZE) - 1;
shmem.c:		if (info->seals & F_SEAL_WRITE) {
shmem.c:			error = -EPERM;
shmem.c:		spin_lock(&inode->i_lock);
shmem.c:		inode->i_private = &shmem_falloc;
shmem.c:		spin_unlock(&inode->i_lock);
shmem.c:					    1 + unmap_end - unmap_start, 0);
shmem.c:		shmem_truncate_range(inode, offset, offset + len - 1);
shmem.c:		/* No need to unmap again: hole-punching leaves COWed pages */
shmem.c:		spin_lock(&inode->i_lock);
shmem.c:		inode->i_private = NULL;
shmem.c:		spin_unlock(&inode->i_lock);
shmem.c:	if ((info->seals & F_SEAL_GROW) && offset + len > inode->i_size) {
shmem.c:		error = -EPERM;
shmem.c:	end = (offset + len + PAGE_SIZE - 1) >> PAGE_SHIFT;
shmem.c:	if (sbinfo->max_blocks && end - start > sbinfo->max_blocks) {
shmem.c:		error = -ENOSPC;
shmem.c:	spin_lock(&inode->i_lock);
shmem.c:	inode->i_private = &shmem_falloc;
shmem.c:	spin_unlock(&inode->i_lock);
shmem.c:			error = -EINTR;
shmem.c:			error = -ENOMEM;
shmem.c:				    ((loff_t)index << PAGE_SHIFT) - 1, true);
shmem.c:	if (!(mode & FALLOC_FL_KEEP_SIZE) && offset + len > inode->i_size)
shmem.c:	inode->i_ctime = current_time(inode);
shmem.c:	spin_lock(&inode->i_lock);
shmem.c:	inode->i_private = NULL;
shmem.c:	spin_unlock(&inode->i_lock);
shmem.c:	struct shmem_sb_info *sbinfo = SHMEM_SB(dentry->d_sb);
shmem.c:	buf->f_type = TMPFS_MAGIC;
shmem.c:	buf->f_bsize = PAGE_SIZE;
shmem.c:	buf->f_namelen = NAME_MAX;
shmem.c:	if (sbinfo->max_blocks) {
shmem.c:		buf->f_blocks = sbinfo->max_blocks;
shmem.c:		buf->f_bavail =
shmem.c:		buf->f_bfree  = sbinfo->max_blocks -
shmem.c:				percpu_counter_sum(&sbinfo->used_blocks);
shmem.c:	if (sbinfo->max_inodes) {
shmem.c:		buf->f_files = sbinfo->max_inodes;
shmem.c:		buf->f_ffree = sbinfo->free_inodes;
shmem.c:	int error = -ENOSPC;
shmem.c:	inode = shmem_get_inode(dir->i_sb, dir, mode, dev, VM_NORESERVE);
shmem.c:						     &dentry->d_name,
shmem.c:		if (error && error != -EOPNOTSUPP)
shmem.c:		dir->i_size += BOGO_DIRENT_SIZE;
shmem.c:		dir->i_ctime = dir->i_mtime = current_time(dir);
shmem.c:		dget(dentry); /* Extra count - pin the dentry in core */
shmem.c:	int error = -ENOSPC;
shmem.c:	inode = shmem_get_inode(dir->i_sb, dir, mode, 0, VM_NORESERVE);
shmem.c:		if (error && error != -EOPNOTSUPP)
shmem.c:	ret = shmem_reserve_inode(inode->i_sb);
shmem.c:	dir->i_size += BOGO_DIRENT_SIZE;
shmem.c:	inode->i_ctime = dir->i_ctime = dir->i_mtime = current_time(inode);
shmem.c:	if (inode->i_nlink > 1 && !S_ISDIR(inode->i_mode))
shmem.c:		shmem_free_inode(inode->i_sb);
shmem.c:	dir->i_size -= BOGO_DIRENT_SIZE;
shmem.c:	inode->i_ctime = dir->i_ctime = dir->i_mtime = current_time(inode);
shmem.c:	dput(dentry);	/* Undo the count from "create" - this does all the work */
shmem.c:		return -ENOTEMPTY;
shmem.c:	old_dir->i_ctime = old_dir->i_mtime =
shmem.c:	new_dir->i_ctime = new_dir->i_mtime =
shmem.c:	d_inode(old_dentry)->i_ctime =
shmem.c:	d_inode(new_dentry)->i_ctime = current_time(old_dir);
shmem.c:	whiteout = d_alloc(old_dentry->d_parent, &old_dentry->d_name);
shmem.c:		return -ENOMEM;
shmem.c:	int they_are_dirs = S_ISDIR(inode->i_mode);
shmem.c:		return -EINVAL;
shmem.c:		return -ENOTEMPTY;
shmem.c:	old_dir->i_size -= BOGO_DIRENT_SIZE;
shmem.c:	new_dir->i_size += BOGO_DIRENT_SIZE;
shmem.c:	old_dir->i_ctime = old_dir->i_mtime =
shmem.c:	new_dir->i_ctime = new_dir->i_mtime =
shmem.c:	inode->i_ctime = current_time(old_dir);
shmem.c:		return -ENAMETOOLONG;
shmem.c:	inode = shmem_get_inode(dir->i_sb, dir, S_IFLNK|S_IRWXUGO, 0, VM_NORESERVE);
shmem.c:		return -ENOSPC;
shmem.c:	error = security_inode_init_security(inode, dir, &dentry->d_name,
shmem.c:		if (error != -EOPNOTSUPP) {
shmem.c:	inode->i_size = len-1;
shmem.c:		inode->i_link = kmemdup(symname, len, GFP_KERNEL);
shmem.c:		if (!inode->i_link) {
shmem.c:			return -ENOMEM;
shmem.c:		inode->i_op = &shmem_short_symlink_operations;
shmem.c:		inode->i_mapping->a_ops = &shmem_aops;
shmem.c:		inode->i_op = &shmem_symlink_inode_operations;
shmem.c:	dir->i_size += BOGO_DIRENT_SIZE;
shmem.c:	dir->i_ctime = dir->i_mtime = current_time(dir);
shmem.c:		page = find_get_page(inode->i_mapping, 0);
shmem.c:			return ERR_PTR(-ECHILD);
shmem.c:			return ERR_PTR(-ECHILD);
shmem.c:	for (xattr = xattr_array; xattr->name != NULL; xattr++) {
shmem.c:		new_xattr = simple_xattr_alloc(xattr->value, xattr->value_len);
shmem.c:			return -ENOMEM;
shmem.c:		len = strlen(xattr->name) + 1;
shmem.c:		new_xattr->name = kmalloc(XATTR_SECURITY_PREFIX_LEN + len,
shmem.c:		if (!new_xattr->name) {
shmem.c:			return -ENOMEM;
shmem.c:		memcpy(new_xattr->name, XATTR_SECURITY_PREFIX,
shmem.c:		memcpy(new_xattr->name + XATTR_SECURITY_PREFIX_LEN,
shmem.c:		       xattr->name, len);
shmem.c:		simple_xattr_list_add(&info->xattrs, new_xattr);
shmem.c:	return simple_xattr_get(&info->xattrs, name, buffer, size);
shmem.c:	return simple_xattr_set(&info->xattrs, name, value, size, flags);
shmem.c:	return simple_xattr_list(d_inode(dentry), &info->xattrs, buffer, size);
shmem.c:	return ERR_PTR(-ESTALE);
shmem.c:	return ino->i_ino == inum && fh[0] == ino->i_generation;
shmem.c:	inum = fid->raw[2];
shmem.c:	inum = (inum << 32) | fid->raw[1];
shmem.c:	inode = ilookup5(sb, (unsigned long)(inum + fid->raw[0]),
shmem.c:			shmem_match, fid->raw);
shmem.c:					    inode->i_ino + inode->i_generation);
shmem.c:	fh[0] = inode->i_generation;
shmem.c:	fh[1] = inode->i_ino;
shmem.c:	fh[2] = ((__u64)inode->i_ino) >> 32;
shmem.c:			 * NUL-terminate this option: unfortunately,
shmem.c:			 * mount options form a comma-separated list,
shmem.c:				options[-1] = '\0';
shmem.c:			sbinfo->max_blocks =
shmem.c:			sbinfo->max_blocks = memparse(value, &rest);
shmem.c:			sbinfo->max_inodes = memparse(value, &rest);
shmem.c:			sbinfo->mode = simple_strtoul(value, &rest, 8) & 07777;
shmem.c:			sbinfo->uid = make_kuid(current_user_ns(), uid);
shmem.c:			if (!uid_valid(sbinfo->uid))
shmem.c:			sbinfo->gid = make_kgid(current_user_ns(), gid);
shmem.c:			if (!gid_valid(sbinfo->gid))
shmem.c:			sbinfo->huge = huge;
shmem.c:	sbinfo->mpol = mpol;
shmem.c:	int error = -EINVAL;
shmem.c:	spin_lock(&sbinfo->stat_lock);
shmem.c:	inodes = sbinfo->max_inodes - sbinfo->free_inodes;
shmem.c:	if (percpu_counter_compare(&sbinfo->used_blocks, config.max_blocks) > 0)
shmem.c:	 * Those tests disallow limited->unlimited while any are in use;
shmem.c:	 * but we must separately disallow unlimited->limited, because
shmem.c:	if (config.max_blocks && !sbinfo->max_blocks)
shmem.c:	if (config.max_inodes && !sbinfo->max_inodes)
shmem.c:	sbinfo->huge = config.huge;
shmem.c:	sbinfo->max_blocks  = config.max_blocks;
shmem.c:	sbinfo->max_inodes  = config.max_inodes;
shmem.c:	sbinfo->free_inodes = config.max_inodes - inodes;
shmem.c:		mpol_put(sbinfo->mpol);
shmem.c:		sbinfo->mpol = config.mpol;	/* transfers initial ref */
shmem.c:	spin_unlock(&sbinfo->stat_lock);
shmem.c:	struct shmem_sb_info *sbinfo = SHMEM_SB(root->d_sb);
shmem.c:	if (sbinfo->max_blocks != shmem_default_max_blocks())
shmem.c:			sbinfo->max_blocks << (PAGE_SHIFT - 10));
shmem.c:	if (sbinfo->max_inodes != shmem_default_max_inodes())
shmem.c:		seq_printf(seq, ",nr_inodes=%lu", sbinfo->max_inodes);
shmem.c:	if (sbinfo->mode != (S_IRWXUGO | S_ISVTX))
shmem.c:		seq_printf(seq, ",mode=%03ho", sbinfo->mode);
shmem.c:	if (!uid_eq(sbinfo->uid, GLOBAL_ROOT_UID))
shmem.c:				from_kuid_munged(&init_user_ns, sbinfo->uid));
shmem.c:	if (!gid_eq(sbinfo->gid, GLOBAL_ROOT_GID))
shmem.c:				from_kgid_munged(&init_user_ns, sbinfo->gid));
shmem.c:	if (sbinfo->huge)
shmem.c:		seq_printf(seq, ",huge=%s", shmem_format_huge(sbinfo->huge));
shmem.c:	shmem_show_mpol(seq, sbinfo->mpol);
shmem.c:#define MFD_NAME_PREFIX_LEN (sizeof(MFD_NAME_PREFIX) - 1)
shmem.c:#define MFD_NAME_MAX_LEN (NAME_MAX - MFD_NAME_PREFIX_LEN)
shmem.c:			return -EINVAL;
shmem.c:			return -EINVAL;
shmem.c:			return -EINVAL;
shmem.c:		return -EFAULT;
shmem.c:		return -EINVAL;
shmem.c:		return -ENOMEM;
shmem.c:		error = -EFAULT;
shmem.c:	/* terminating-zero may have changed after strnlen_user() returned */
shmem.c:	if (name[len + MFD_NAME_PREFIX_LEN - 1]) {
shmem.c:		error = -EFAULT;
shmem.c:	file->f_mode |= FMODE_LSEEK | FMODE_PREAD | FMODE_PWRITE;
shmem.c:	file->f_flags |= O_RDWR | O_LARGEFILE;
shmem.c:		info->seals &= ~F_SEAL_SEAL;
shmem.c:	percpu_counter_destroy(&sbinfo->used_blocks);
shmem.c:	mpol_put(sbinfo->mpol);
shmem.c:	sb->s_fs_info = NULL;
shmem.c:	int err = -ENOMEM;
shmem.c:		return -ENOMEM;
shmem.c:	sbinfo->mode = S_IRWXUGO | S_ISVTX;
shmem.c:	sbinfo->uid = current_fsuid();
shmem.c:	sbinfo->gid = current_fsgid();
shmem.c:	sb->s_fs_info = sbinfo;
shmem.c:	if (!(sb->s_flags & MS_KERNMOUNT)) {
shmem.c:		sbinfo->max_blocks = shmem_default_max_blocks();
shmem.c:		sbinfo->max_inodes = shmem_default_max_inodes();
shmem.c:			err = -EINVAL;
shmem.c:		sb->s_flags |= MS_NOUSER;
shmem.c:	sb->s_export_op = &shmem_export_ops;
shmem.c:	sb->s_flags |= MS_NOSEC;
shmem.c:	sb->s_flags |= MS_NOUSER;
shmem.c:	spin_lock_init(&sbinfo->stat_lock);
shmem.c:	if (percpu_counter_init(&sbinfo->used_blocks, 0, GFP_KERNEL))
shmem.c:	sbinfo->free_inodes = sbinfo->max_inodes;
shmem.c:	spin_lock_init(&sbinfo->shrinklist_lock);
shmem.c:	INIT_LIST_HEAD(&sbinfo->shrinklist);
shmem.c:	sb->s_maxbytes = MAX_LFS_FILESIZE;
shmem.c:	sb->s_blocksize = PAGE_SIZE;
shmem.c:	sb->s_blocksize_bits = PAGE_SHIFT;
shmem.c:	sb->s_magic = TMPFS_MAGIC;
shmem.c:	sb->s_op = &shmem_ops;
shmem.c:	sb->s_time_gran = 1;
shmem.c:	sb->s_xattr = shmem_xattr_handlers;
shmem.c:	sb->s_flags |= MS_POSIXACL;
shmem.c:	uuid_gen(&sb->s_uuid);
shmem.c:	inode = shmem_get_inode(sb, NULL, S_IFDIR | sbinfo->mode, 0, VM_NORESERVE);
shmem.c:	inode->i_uid = sbinfo->uid;
shmem.c:	inode->i_gid = sbinfo->gid;
shmem.c:	sb->s_root = d_make_root(inode);
shmem.c:	if (!sb->s_root)
shmem.c:	return &info->vfs_inode;
shmem.c:	if (S_ISLNK(inode->i_mode))
shmem.c:		kfree(inode->i_link);
shmem.c:	if (S_ISREG(inode->i_mode))
shmem.c:		mpol_free_shared_policy(&SHMEM_I(inode)->policy);
shmem.c:	call_rcu(&inode->i_rcu, shmem_destroy_callback);
shmem.c:	inode_init_once(&info->vfs_inode);
shmem.c:	/* If rootfs called this, don't re-init */
shmem.c:		SHMEM_SB(shm_mnt->mnt_sb)->huge = shmem_huge;
shmem.c:	buf[count - 1] = '\n';
shmem.c:		return -EINVAL;
shmem.c:	if (count && tmp[count - 1] == '\n')
shmem.c:		tmp[count - 1] = '\0';
shmem.c:	if (huge == -EINVAL)
shmem.c:		return -EINVAL;
shmem.c:		return -EINVAL;
shmem.c:		SHMEM_SB(shm_mnt->mnt_sb)->huge = shmem_huge;
shmem.c:	struct inode *inode = file_inode(vma->vm_file);
shmem.c:	struct shmem_sb_info *sbinfo = SHMEM_SB(inode->i_sb);
shmem.c:	switch (sbinfo->huge) {
shmem.c:			off = round_up(vma->vm_pgoff, HPAGE_PMD_NR);
shmem.c:			return (vma->vm_flags & VM_HUGEPAGE);
shmem.c: * tiny-shmem: simple shmemfs and tmpfs using ramfs code
shmem.c: * shmem code (swap-backed and resource-limited) are outweighed by
shmem.c:	return current->mm->get_unmapped_area(file, addr, len, pgoff, flags);
shmem.c:	truncate_inode_pages_range(inode->i_mapping, lstart, lend);
shmem.c:		return ERR_PTR(-EINVAL);
shmem.c:		return ERR_PTR(-ENOMEM);
shmem.c:	res = ERR_PTR(-ENOMEM);
shmem.c:	sb = shm_mnt->mnt_sb;
shmem.c:	res = ERR_PTR(-ENOSPC);
shmem.c:	inode->i_flags |= i_flags;
shmem.c:	inode->i_size = size;
shmem.c: * shmem_kernel_file_setup - get an unlinked file living in tmpfs which must be
shmem.c: * @flags: VM_NORESERVE suppresses pre-accounting of the entire object size
shmem.c: * shmem_file_setup - get an unlinked file living in tmpfs
shmem.c: * @flags: VM_NORESERVE suppresses pre-accounting of the entire object size
shmem.c: * shmem_zero_setup - setup a shared anonymous mapping
shmem.c:	loff_t size = vma->vm_end - vma->vm_start;
shmem.c:	file = __shmem_file_setup("dev/zero", size, vma->vm_flags, S_PRIVATE);
shmem.c:	if (vma->vm_file)
shmem.c:		fput(vma->vm_file);
shmem.c:	vma->vm_file = file;
shmem.c:	vma->vm_ops = &shmem_vm_ops;
shmem.c:			((vma->vm_start + ~HPAGE_PMD_MASK) & HPAGE_PMD_MASK) <
shmem.c:			(vma->vm_end & HPAGE_PMD_MASK)) {
shmem.c:		khugepaged_enter(vma, vma->vm_flags);
shmem.c: * shmem_read_mapping_page_gfp - read into page cache, using specified page allocation flags.
shmem.c: * But read_cache_page_gfp() uses the ->readpage() method: which does not
shmem.c:	struct inode *inode = mapping->host;
shmem.c:	BUG_ON(mapping->a_ops != &shmem_aops);
mprotect.c:// SPDX-License-Identifier: GPL-2.0
mprotect.c:	struct mm_struct *mm = vma->vm_mm;
mprotect.c:	pte = pte_offset_map_lock(vma->vm_mm, pmd, addr, &ptl);
mprotect.c:	if (prot_numa && !(vma->vm_flags & VM_SHARED) &&
mprotect.c:	    atomic_read(&vma->vm_mm->mm_users) == 1)
mprotect.c:	flush_tlb_batched_pending(vma->vm_mm);
mprotect.c:				 * a single-threaded process is running on.
mprotect.c:					 !(vma->vm_flags & VM_SOFTDIRTY))) {
mprotect.c:				 * We do not preserve soft-dirtiness. See
mprotect.c:	pte_unmap_unlock(pte - 1, ptl);
mprotect.c:	struct mm_struct *mm = vma->vm_mm;
mprotect.c:			if (next - addr != HPAGE_PMD_SIZE) {
mprotect.c:	struct mm_struct *mm = vma->vm_mm;
mprotect.c:	struct mm_struct *mm = vma->vm_mm;
mprotect.c:	unsigned long oldflags = vma->vm_flags;
mprotect.c:	long nrpages = (end - start) >> PAGE_SHIFT;
mprotect.c:	 * even if read-only so there is no need to account for them here
mprotect.c:			return -ENOMEM;
mprotect.c:				return -ENOMEM;
mprotect.c:	pgoff = vma->vm_pgoff + ((start - vma->vm_start) >> PAGE_SHIFT);
mprotect.c:			   vma->anon_vma, vma->vm_file, pgoff, vma_policy(vma),
mprotect.c:			   vma->vm_userfaultfd_ctx);
mprotect.c:		VM_WARN_ON((vma->vm_flags ^ newflags) & ~VM_SOFTDIRTY);
mprotect.c:	if (start != vma->vm_start) {
mprotect.c:	if (end != vma->vm_end) {
mprotect.c:	vma->vm_flags = newflags;
mprotect.c:	dirty_accountable = vma_wants_writenotify(vma, vma->vm_page_prot);
mprotect.c:	change_protection(vma, start, end, vma->vm_page_prot,
mprotect.c:	vm_stat_account(mm, oldflags, -nrpages);
mprotect.c: * pkey==-1 when doing a legacy mprotect()
mprotect.c:	int error = -EINVAL;
mprotect.c:	const bool rier = (current->personality & READ_IMPLIES_EXEC) &&
mprotect.c:		return -EINVAL;
mprotect.c:		return -EINVAL;
mprotect.c:		return -ENOMEM;
mprotect.c:		return -EINVAL;
mprotect.c:	if (down_write_killable(&current->mm->mmap_sem))
mprotect.c:		return -EINTR;
mprotect.c:	error = -EINVAL;
mprotect.c:	if ((pkey != -1) && !mm_pkey_is_allocated(current->mm, pkey))
mprotect.c:	vma = find_vma(current->mm, start);
mprotect.c:	error = -ENOMEM;
mprotect.c:	prev = vma->vm_prev;
mprotect.c:		if (vma->vm_start >= end)
mprotect.c:		start = vma->vm_start;
mprotect.c:		error = -EINVAL;
mprotect.c:		if (!(vma->vm_flags & VM_GROWSDOWN))
mprotect.c:		if (vma->vm_start > start)
mprotect.c:			end = vma->vm_end;
mprotect.c:			error = -EINVAL;
mprotect.c:			if (!(vma->vm_flags & VM_GROWSUP))
mprotect.c:	if (start > vma->vm_start)
mprotect.c:		/* Here we know that vma->vm_start <= nstart < vma->vm_end. */
mprotect.c:		if (rier && (vma->vm_flags & VM_MAYEXEC))
mprotect.c:		newflags |= (vma->vm_flags & ~mask_off_old_flags);
mprotect.c:			error = -EACCES;
mprotect.c:		tmp = vma->vm_end;
mprotect.c:		if (nstart < prev->vm_end)
mprotect.c:			nstart = prev->vm_end;
mprotect.c:		vma = prev->vm_next;
mprotect.c:		if (!vma || vma->vm_start != nstart) {
mprotect.c:			error = -ENOMEM;
mprotect.c:	up_write(&current->mm->mmap_sem);
mprotect.c:	return do_mprotect_pkey(start, len, prot, -1);
mprotect.c:		return -EINVAL;
mprotect.c:		return -EINVAL;
mprotect.c:	down_write(&current->mm->mmap_sem);
mprotect.c:	pkey = mm_pkey_alloc(current->mm);
mprotect.c:	ret = -ENOSPC;
mprotect.c:	if (pkey == -1)
mprotect.c:		mm_pkey_free(current->mm, pkey);
mprotect.c:	up_write(&current->mm->mmap_sem);
mprotect.c:	down_write(&current->mm->mmap_sem);
mprotect.c:	ret = mm_pkey_free(current->mm, pkey);
mprotect.c:	up_write(&current->mm->mmap_sem);
vmscan.c:// SPDX-License-Identifier: GPL-2.0
vmscan.c:#include <linux/backing-dev.h>
vmscan.c:		if ((_page)->lru.prev != _base) {			\
vmscan.c:			prev = lru_to_page(&(_page->lru));		\
vmscan.c:			prefetch(&prev->_field);			\
vmscan.c:		if ((_page)->lru.prev != _base) {			\
vmscan.c:			prev = lru_to_page(&(_page->lru));		\
vmscan.c:			prefetchw(&prev->_field);			\
vmscan.c:	return !sc->target_mem_cgroup;
vmscan.c: * sane_reclaim - is the usual dirty throttling mechanism operational?
vmscan.c:	struct mem_cgroup *memcg = sc->target_mem_cgroup;
vmscan.c: * lruvec_lru_size -  Returns the number of pages on the given LRU list.
vmscan.c:		struct zone *zone = &lruvec_pgdat(lruvec)->node_zones[zid];
vmscan.c:			size = zone_page_state(&lruvec_pgdat(lruvec)->node_zones[zid],
vmscan.c:		lru_size -= min(size, lru_size);
vmscan.c:	size_t size = sizeof(*shrinker->nr_deferred);
vmscan.c:	if (shrinker->flags & SHRINKER_NUMA_AWARE)
vmscan.c:	shrinker->nr_deferred = kzalloc(size, GFP_KERNEL);
vmscan.c:	if (!shrinker->nr_deferred)
vmscan.c:		return -ENOMEM;
vmscan.c:	list_add_tail(&shrinker->list, &shrinker_list);
vmscan.c:	if (!shrinker->nr_deferred)
vmscan.c:	list_del(&shrinker->list);
vmscan.c:	kfree(shrinker->nr_deferred);
vmscan.c:	shrinker->nr_deferred = NULL;
vmscan.c:	int nid = shrinkctl->nid;
vmscan.c:	long batch_size = shrinker->batch ? shrinker->batch
vmscan.c:	freeable = shrinker->count_objects(shrinker, shrinkctl);
vmscan.c:	nr = atomic_long_xchg(&shrinker->nr_deferred[nid], 0);
vmscan.c:	delta = (4 * nr_scanned) / shrinker->seeks;
vmscan.c:		       shrinker->scan_objects, total_scan);
vmscan.c:	 * shrinkers to return -1 all the time. This results in a large
vmscan.c:		shrinkctl->nr_to_scan = nr_to_scan;
vmscan.c:		shrinkctl->nr_scanned = nr_to_scan;
vmscan.c:		ret = shrinker->scan_objects(shrinker, shrinkctl);
vmscan.c:		count_vm_events(SLABS_SCANNED, shrinkctl->nr_scanned);
vmscan.c:		total_scan -= shrinkctl->nr_scanned;
vmscan.c:		scanned += shrinkctl->nr_scanned;
vmscan.c:		next_deferred -= scanned;
vmscan.c:						&shrinker->nr_deferred[nid]);
vmscan.c:		new_nr = atomic_long_read(&shrinker->nr_deferred[nid]);
vmscan.c: * shrink_slab - shrink slab caches
vmscan.c: * the ->seeks setting of the shrink function, which indicates the
vmscan.c:		    !!memcg != !!(shrinker->flags & SHRINKER_MEMCG_AWARE))
vmscan.c:		if (!(shrinker->flags & SHRINKER_NUMA_AWARE))
vmscan.c:	 * optional buffer heads at page->private.
vmscan.c:	return page_count(page) - page_has_private(page) == 1 + radix_pins;
vmscan.c:	if (current->flags & PF_SWAPWRITE)
vmscan.c:	if (inode_to_bdi(inode) == current->backing_dev_info)
vmscan.c: * -ENOSPC.  We need to propagate that into the address_space for a subsequent
vmscan.c: * Calls ->writepage().
vmscan.c:	 * will be non-blocking.  To prevent this allocation from being
vmscan.c:		 * page->mapping == NULL while being dirty with clean buffers.
vmscan.c:	if (mapping->a_ops->writepage == NULL)
vmscan.c:	if (!may_write_to_inode(mapping->host, sc))
vmscan.c:		res = mapping->a_ops->writepage(page, &wbc);
vmscan.c:	spin_lock_irqsave(&mapping->tree_lock, flags);
vmscan.c:	 * escape unnoticed. The smp_rmb is needed to ensure the page->flags
vmscan.c:	 * load is not satisfied before that of page->_refcount.
vmscan.c:		spin_unlock_irqrestore(&mapping->tree_lock, flags);
vmscan.c:		freepage = mapping->a_ops->freepage;
vmscan.c:		spin_unlock_irqrestore(&mapping->tree_lock, flags);
vmscan.c:	spin_unlock_irqrestore(&mapping->tree_lock, flags);
vmscan.c: * Attempt to detach a locked page from its ->mapping.  If it is dirty or if
vmscan.c: * putback_lru_page - put previously isolated page onto appropriate LRU list
vmscan.c:	referenced_ptes = page_referenced(page, 1, sc->target_mem_cgroup,
vmscan.c:		 * Activate file-backed executable pages after first usage.
vmscan.c:	if (mapping && mapping->a_ops->is_dirty_writeback)
vmscan.c:		mapping->a_ops->is_dirty_writeback(page, dirty, writeback);
vmscan.c:		list_del(&page->lru);
vmscan.c:		sc->nr_scanned++;
vmscan.c:		if (!sc->may_unmap && page_mapped(page))
vmscan.c:			sc->nr_scanned++;
vmscan.c:		may_enter_fs = (sc->gfp_mask & __GFP_FS) ||
vmscan.c:			(PageSwapCache(page) && (sc->gfp_mask & __GFP_IO));
vmscan.c:		     inode_write_congested(mapping->host)) ||
vmscan.c:			    test_bit(PGDAT_WRITEBACK, &pgdat->flags)) {
vmscan.c:				 * This is slightly racy - end_page_writeback()
vmscan.c:				 * as PageReadahead - but that does not matter
vmscan.c:				list_add_tail(&page->lru, page_list);
vmscan.c:				if (!(sc->gfp_mask & __GFP_IO))
vmscan.c:			 * injecting inefficient single-page IO into
vmscan.c:			     !test_bit(PGDAT_DIRTY, &pgdat->flags))) {
vmscan.c:			if (!sc->may_writepage)
vmscan.c:				 * A synchronous write - probably a ramdisk.  Go
vmscan.c:		 * drop the buffers and mark the page clean - it can be freed.
vmscan.c:		 * Rarely, pages can have buffers and no ->mapping.  These are
vmscan.c:			if (!try_to_release_page(page, sc->gfp_mask))
vmscan.c:		 * from pagecache). Can use non-atomic bitops now (and
vmscan.c:			list_add(&page->lru, &free_pages);
vmscan.c:		list_add(&page->lru, &ret_pages);
vmscan.c:		stat->nr_dirty = nr_dirty;
vmscan.c:		stat->nr_congested = nr_congested;
vmscan.c:		stat->nr_unqueued_dirty = nr_unqueued_dirty;
vmscan.c:		stat->nr_writeback = nr_writeback;
vmscan.c:		stat->nr_immediate = nr_immediate;
vmscan.c:		stat->nr_activate = pgactivate;
vmscan.c:		stat->nr_ref_keep = nr_ref_keep;
vmscan.c:		stat->nr_unmap_fail = nr_unmap_fail;
vmscan.c:			list_move(&page->lru, &clean_pages);
vmscan.c:	ret = shrink_page_list(&clean_pages, zone->zone_pgdat, &sc,
vmscan.c:	mod_node_page_state(zone->zone_pgdat, NR_ISOLATED_FILE, -ret);
vmscan.c: * returns 0 on success, -ve errno on failure.
vmscan.c:	int ret = -EINVAL;
vmscan.c:	ret = -EBUSY;
vmscan.c:	 * blocking - clean pages for the most part.
vmscan.c:			 * ->migratepage callback are possible to migrate
vmscan.c:			migrate_dirty = !mapping || mapping->a_ops->migratepage;
vmscan.c:		 * sure the page is not being freed elsewhere -- the
vmscan.c:		__update_lru_size(lruvec, lru, zid, -nr_zone_taken[zid]);
vmscan.c:		mem_cgroup_update_lru_size(lruvec, lru, zid, -nr_zone_taken[zid]);
vmscan.c:	struct list_head *src = &lruvec->lists[lru];
vmscan.c:		if (page_zonenum(page) > sc->reclaim_idx) {
vmscan.c:			list_move(&page->lru, &pages_skipped);
vmscan.c:			list_move(&page->lru, dst);
vmscan.c:		case -EBUSY:
vmscan.c:			list_move(&page->lru, src);
vmscan.c:	trace_mm_vmscan_lru_isolate(sc->reclaim_idx, sc->order, nr_to_scan,
vmscan.c: * isolate_lru_page - tries to isolate a page from its LRU list
vmscan.c: * Returns -EBUSY if the page was not on an LRU list.
vmscan.c:	int ret = -EBUSY;
vmscan.c:		lruvec = mem_cgroup_page_lruvec(page, zone->zone_pgdat);
vmscan.c:	 * won't get blocked by normal direct-reclaimers, forming a circular
vmscan.c:	if ((sc->gfp_mask & (__GFP_IO | __GFP_FS)) == (__GFP_IO | __GFP_FS))
vmscan.c:	struct zone_reclaim_stat *reclaim_stat = &lruvec->reclaim_stat;
vmscan.c:		list_del(&page->lru);
vmscan.c:			spin_unlock_irq(&pgdat->lru_lock);
vmscan.c:			spin_lock_irq(&pgdat->lru_lock);
vmscan.c:			reclaim_stat->recent_rotated[file] += numpages;
vmscan.c:				spin_unlock_irq(&pgdat->lru_lock);
vmscan.c:				spin_lock_irq(&pgdat->lru_lock);
vmscan.c:				list_add(&page->lru, &pages_to_free);
vmscan.c: * If a kernel thread (such as nfsd for loop-back mounts) services
vmscan.c:	return !(current->flags & PF_LESS_THROTTLE) ||
vmscan.c:		current->backing_dev_info == NULL ||
vmscan.c:		bdi_write_congested(current->backing_dev_info);
vmscan.c:	struct zone_reclaim_stat *reclaim_stat = &lruvec->reclaim_stat;
vmscan.c:	if (!sc->may_unmap)
vmscan.c:	spin_lock_irq(&pgdat->lru_lock);
vmscan.c:	reclaim_stat->recent_scanned[file] += nr_taken;
vmscan.c:	spin_unlock_irq(&pgdat->lru_lock);
vmscan.c:	spin_lock_irq(&pgdat->lru_lock);
vmscan.c:	__mod_node_page_state(pgdat, NR_ISOLATED_ANON + file, -nr_taken);
vmscan.c:	spin_unlock_irq(&pgdat->lru_lock);
vmscan.c:	 * that the long-lived page allocation rate is exceeding the page
vmscan.c:		set_bit(PGDAT_WRITEBACK, &pgdat->flags);
vmscan.c:			set_bit(PGDAT_CONGESTED, &pgdat->flags);
vmscan.c:			set_bit(PGDAT_DIRTY, &pgdat->flags);
vmscan.c:	if (!sc->hibernation_mode && !current_is_kswapd() &&
vmscan.c:	trace_mm_vmscan_lru_shrink_inactive(pgdat->node_id,
vmscan.c:			sc->priority, file);
vmscan.c: * It is safe to rely on PG_active against the non-LRU pages in here because
vmscan.c: * nobody will play with that bit on a non-LRU page.
vmscan.c: * The downside is that we have to touch page->_refcount against each page.
vmscan.c: * But we had to alter page->flags anyway.
vmscan.c:		list_move(&page->lru, &lruvec->lists[lru]);
vmscan.c:				spin_unlock_irq(&pgdat->lru_lock);
vmscan.c:				spin_lock_irq(&pgdat->lru_lock);
vmscan.c:				list_add(&page->lru, pages_to_free);
vmscan.c:	struct zone_reclaim_stat *reclaim_stat = &lruvec->reclaim_stat;
vmscan.c:	if (!sc->may_unmap)
vmscan.c:	spin_lock_irq(&pgdat->lru_lock);
vmscan.c:	reclaim_stat->recent_scanned[file] += nr_taken;
vmscan.c:	spin_unlock_irq(&pgdat->lru_lock);
vmscan.c:		list_del(&page->lru);
vmscan.c:		if (page_referenced(page, 0, sc->target_mem_cgroup,
vmscan.c:			 * Identify referenced, file-backed active pages and
vmscan.c:			 * are not likely to be evicted by use-once streaming
vmscan.c:				list_add(&page->lru, &l_active);
vmscan.c:		ClearPageActive(page);	/* we are de-activating */
vmscan.c:		list_add(&page->lru, &l_inactive);
vmscan.c:	spin_lock_irq(&pgdat->lru_lock);
vmscan.c:	 * even though only some of them are actually re-activated.  This
vmscan.c:	reclaim_stat->recent_rotated[file] += nr_rotated;
vmscan.c:	nr_deactivate = move_active_pages_to_lru(lruvec, &l_inactive, &l_hold, lru - LRU_ACTIVE);
vmscan.c:	__mod_node_page_state(pgdat, NR_ISOLATED_ANON + file, -nr_taken);
vmscan.c:	spin_unlock_irq(&pgdat->lru_lock);
vmscan.c:	trace_mm_vmscan_lru_shrink_active(pgdat->node_id, nr_taken, nr_activate,
vmscan.c:			nr_deactivate, nr_rotated, sc->priority, file);
vmscan.c: * to the established workingset on the scan-resistant active list,
vmscan.c: * on this LRU, maintained by the pageout code. A zone->inactive_ratio
vmscan.c: * -------------------------------------
vmscan.c:	inactive = lruvec_lru_size(lruvec, inactive_lru, sc->reclaim_idx);
vmscan.c:	active = lruvec_lru_size(lruvec, active_lru, sc->reclaim_idx);
vmscan.c:	if (file && actual_reclaim && lruvec->refaults != refaults) {
vmscan.c:		gb = (inactive + active) >> (30 - PAGE_SHIFT);
vmscan.c:		trace_mm_vmscan_inactive_list_is_low(pgdat->node_id, sc->reclaim_idx,
vmscan.c:	struct zone_reclaim_stat *reclaim_stat = &lruvec->reclaim_stat;
vmscan.c:	if (!sc->may_swap || mem_cgroup_get_nr_swap_pages(memcg) <= 0) {
vmscan.c:	if (!sc->priority && swappiness) {
vmscan.c:		pgdatfree = sum_zone_node_page_state(pgdat->node_id, NR_FREE_PAGES);
vmscan.c:			struct zone *zone = &pgdat->node_zones[z];
vmscan.c:			    lruvec_lru_size(lruvec, LRU_INACTIVE_ANON, sc->reclaim_idx)
vmscan.c:					>> sc->priority) {
vmscan.c:	    lruvec_lru_size(lruvec, LRU_INACTIVE_FILE, sc->reclaim_idx) >> sc->priority) {
vmscan.c:	file_prio = 200 - anon_prio;
vmscan.c:	spin_lock_irq(&pgdat->lru_lock);
vmscan.c:	if (unlikely(reclaim_stat->recent_scanned[0] > anon / 4)) {
vmscan.c:		reclaim_stat->recent_scanned[0] /= 2;
vmscan.c:		reclaim_stat->recent_rotated[0] /= 2;
vmscan.c:	if (unlikely(reclaim_stat->recent_scanned[1] > file / 4)) {
vmscan.c:		reclaim_stat->recent_scanned[1] /= 2;
vmscan.c:		reclaim_stat->recent_rotated[1] /= 2;
vmscan.c:	ap = anon_prio * (reclaim_stat->recent_scanned[0] + 1);
vmscan.c:	ap /= reclaim_stat->recent_rotated[0] + 1;
vmscan.c:	fp = file_prio * (reclaim_stat->recent_scanned[1] + 1);
vmscan.c:	fp /= reclaim_stat->recent_rotated[1] + 1;
vmscan.c:	spin_unlock_irq(&pgdat->lru_lock);
vmscan.c:		size = lruvec_lru_size(lruvec, lru, sc->reclaim_idx);
vmscan.c:		scan = size >> sc->priority;
vmscan.c: * This is a basic per-node page freer.  Used by both kswapd and direct reclaim.
vmscan.c:	unsigned long nr_to_reclaim = sc->nr_to_reclaim;
vmscan.c:			 sc->priority == DEF_PRIORITY);
vmscan.c:				nr[lru] -= nr_to_scan;
vmscan.c:		nr_scanned = targets[lru] - nr[lru];
vmscan.c:		nr[lru] = targets[lru] * (100 - percentage) / 100;
vmscan.c:		nr[lru] -= min(nr[lru], nr_scanned);
vmscan.c:		nr_scanned = targets[lru] - nr[lru];
vmscan.c:		nr[lru] = targets[lru] * (100 - percentage) / 100;
vmscan.c:		nr[lru] -= min(nr[lru], nr_scanned);
vmscan.c:	sc->nr_reclaimed += nr_reclaimed;
vmscan.c:	if (IS_ENABLED(CONFIG_COMPACTION) && sc->order &&
vmscan.c:			(sc->order > PAGE_ALLOC_COSTLY_ORDER ||
vmscan.c:			 sc->priority < DEF_PRIORITY - 2))
vmscan.c: * Reclaim/compaction is used for high-order allocation requests. It reclaims
vmscan.c: * order-0 pages before compacting the zone. should_continue_reclaim() returns
vmscan.c:	if (sc->gfp_mask & __GFP_RETRY_MAYFAIL) {
vmscan.c:		 * For non-__GFP_RETRY_MAYFAIL allocations which can presumably
vmscan.c:	pages_for_compaction = compact_gap(sc->order);
vmscan.c:	if (sc->nr_reclaimed < pages_for_compaction &&
vmscan.c:	for (z = 0; z <= sc->reclaim_idx; z++) {
vmscan.c:		struct zone *zone = &pgdat->node_zones[z];
vmscan.c:		switch (compaction_suitable(zone, sc->order, 0, sc->reclaim_idx)) {
vmscan.c:	struct reclaim_state *reclaim_state = current->reclaim_state;
vmscan.c:		struct mem_cgroup *root = sc->target_mem_cgroup;
vmscan.c:			.priority = sc->priority,
vmscan.c:		nr_reclaimed = sc->nr_reclaimed;
vmscan.c:		nr_scanned = sc->nr_scanned;
vmscan.c:				if (!sc->memcg_low_reclaim) {
vmscan.c:					sc->memcg_low_skipped = 1;
vmscan.c:			reclaimed = sc->nr_reclaimed;
vmscan.c:			scanned = sc->nr_scanned;
vmscan.c:				shrink_slab(sc->gfp_mask, pgdat->node_id,
vmscan.c:					    memcg, sc->nr_scanned - scanned,
vmscan.c:			vmpressure(sc->gfp_mask, memcg, false,
vmscan.c:				   sc->nr_scanned - scanned,
vmscan.c:				   sc->nr_reclaimed - reclaimed);
vmscan.c:					sc->nr_reclaimed >= sc->nr_to_reclaim) {
vmscan.c:			shrink_slab(sc->gfp_mask, pgdat->node_id, NULL,
vmscan.c:				    sc->nr_scanned - nr_scanned,
vmscan.c:			sc->nr_reclaimed += reclaim_state->reclaimed_slab;
vmscan.c:			reclaim_state->reclaimed_slab = 0;
vmscan.c:		vmpressure(sc->gfp_mask, sc->target_mem_cgroup, true,
vmscan.c:			   sc->nr_scanned - nr_scanned,
vmscan.c:			   sc->nr_reclaimed - nr_reclaimed);
vmscan.c:		if (sc->nr_reclaimed - nr_reclaimed)
vmscan.c:	} while (should_continue_reclaim(pgdat, sc->nr_reclaimed - nr_reclaimed,
vmscan.c:					 sc->nr_scanned - nr_scanned, sc));
vmscan.c:		pgdat->kswapd_failures = 0;
vmscan.c: * Returns true if compaction should go ahead for a costly-order request, or
vmscan.c:	suitable = compaction_suitable(zone, sc->order, 0, sc->reclaim_idx);
vmscan.c:	watermark = high_wmark_pages(zone) + compact_gap(sc->order);
vmscan.c:	return zone_watermark_ok_safe(zone, 0, watermark, sc->reclaim_idx);
vmscan.c: * This is the direct reclaim path, for page-allocating processes.  We only
vmscan.c:	orig_mask = sc->gfp_mask;
vmscan.c:		sc->gfp_mask |= __GFP_HIGHMEM;
vmscan.c:		sc->reclaim_idx = gfp_zone(sc->gfp_mask);
vmscan.c:					sc->reclaim_idx, sc->nodemask) {
vmscan.c:			 * non-zero order, only frequent costly order
vmscan.c:			    sc->order > PAGE_ALLOC_COSTLY_ORDER &&
vmscan.c:				sc->compaction_ready = true;
vmscan.c:			if (zone->zone_pgdat == last_pgdat)
vmscan.c:			nr_soft_reclaimed = mem_cgroup_soft_limit_reclaim(zone->zone_pgdat,
vmscan.c:						sc->order, sc->gfp_mask,
vmscan.c:			sc->nr_reclaimed += nr_soft_reclaimed;
vmscan.c:			sc->nr_scanned += nr_soft_scanned;
vmscan.c:		if (zone->zone_pgdat == last_pgdat)
vmscan.c:		last_pgdat = zone->zone_pgdat;
vmscan.c:		shrink_node(zone->zone_pgdat, sc);
vmscan.c:	sc->gfp_mask = orig_mask;
vmscan.c:		lruvec->refaults = refaults;
vmscan.c: * high - the zone may be full of dirty or under-writeback pages, which this
vmscan.c:	int initial_priority = sc->priority;
vmscan.c:		__count_zid_vm_events(ALLOCSTALL, sc->reclaim_idx, 1);
vmscan.c:		vmpressure_prio(sc->gfp_mask, sc->target_mem_cgroup,
vmscan.c:				sc->priority);
vmscan.c:		sc->nr_scanned = 0;
vmscan.c:		if (sc->nr_reclaimed >= sc->nr_to_reclaim)
vmscan.c:		if (sc->compaction_ready)
vmscan.c:		if (sc->priority < DEF_PRIORITY - 2)
vmscan.c:			sc->may_writepage = 1;
vmscan.c:	} while (--sc->priority >= 0);
vmscan.c:	for_each_zone_zonelist_nodemask(zone, z, zonelist, sc->reclaim_idx,
vmscan.c:					sc->nodemask) {
vmscan.c:		if (zone->zone_pgdat == last_pgdat)
vmscan.c:		last_pgdat = zone->zone_pgdat;
vmscan.c:		snapshot_refaults(sc->target_mem_cgroup, zone->zone_pgdat);
vmscan.c:	if (sc->nr_reclaimed)
vmscan.c:		return sc->nr_reclaimed;
vmscan.c:	if (sc->compaction_ready)
vmscan.c:	if (sc->memcg_low_skipped) {
vmscan.c:		sc->priority = initial_priority;
vmscan.c:		sc->memcg_low_reclaim = 1;
vmscan.c:		sc->memcg_low_skipped = 0;
vmscan.c:	if (pgdat->kswapd_failures >= MAX_RECLAIM_RETRIES)
vmscan.c:		zone = &pgdat->node_zones[i];
vmscan.c:	if (!wmark_ok && waitqueue_active(&pgdat->kswapd_wait)) {
vmscan.c:		pgdat->kswapd_classzone_idx = min(pgdat->kswapd_classzone_idx,
vmscan.c:		wake_up_interruptible(&pgdat->kswapd_wait);
vmscan.c:	if (current->flags & PF_KTHREAD)
vmscan.c:		pgdat = zone->zone_pgdat;
vmscan.c:		wait_event_interruptible_timeout(pgdat->pfmemalloc_wait,
vmscan.c:	wait_event_killable(zone->zone_pgdat->pfmemalloc_wait,
vmscan.c:		.reclaim_idx = MAX_NR_ZONES - 1,
vmscan.c:		.reclaim_idx = MAX_NR_ZONES - 1,
vmscan.c:	zonelist = &NODE_DATA(nid)->node_zonelists[ZONELIST_FALLBACK];
vmscan.c:	unsigned long mark = -1;
vmscan.c:		zone = pgdat->node_zones + i;
vmscan.c:	 * need balancing by definition. This can happen if a zone-restricted
vmscan.c:	if (mark == -1)
vmscan.c:	clear_bit(PGDAT_CONGESTED, &pgdat->flags);
vmscan.c:	clear_bit(PGDAT_DIRTY, &pgdat->flags);
vmscan.c:	clear_bit(PGDAT_WRITEBACK, &pgdat->flags);
vmscan.c:	if (waitqueue_active(&pgdat->pfmemalloc_wait))
vmscan.c:		wake_up_all(&pgdat->pfmemalloc_wait);
vmscan.c:	if (pgdat->kswapd_failures >= MAX_RECLAIM_RETRIES)
vmscan.c:	sc->nr_to_reclaim = 0;
vmscan.c:	for (z = 0; z <= sc->reclaim_idx; z++) {
vmscan.c:		zone = pgdat->node_zones + z;
vmscan.c:		sc->nr_to_reclaim += max(high_wmark_pages(zone), SWAP_CLUSTER_MAX);
vmscan.c:	 * high-order allocations. If twice the allocation size has been
vmscan.c:	 * reclaimed then recheck watermarks only at order-0 to prevent
vmscan.c:	 * excessive reclaim. Assume that a process requested a high-order
vmscan.c:	if (sc->order && sc->nr_reclaimed >= compact_gap(sc->order))
vmscan.c:		sc->order = 0;
vmscan.c:	return sc->nr_scanned >= sc->nr_to_reclaim;
vmscan.c: * kswapd scans the zones in the highmem->normal->dma direction.  It skips
vmscan.c:		 * purpose -- on 64-bit systems it is expected that
vmscan.c:		 * buffer_heads are stripped during active rotation. On 32-bit
vmscan.c:			for (i = MAX_NR_ZONES - 1; i >= 0; i--) {
vmscan.c:				zone = pgdat->node_zones + i;
vmscan.c:		if (sc.priority < DEF_PRIORITY - 2)
vmscan.c:		if (waitqueue_active(&pgdat->pfmemalloc_wait) &&
vmscan.c:			wake_up_all(&pgdat->pfmemalloc_wait);
vmscan.c:		nr_reclaimed = sc.nr_reclaimed - nr_reclaimed;
vmscan.c:			sc.priority--;
vmscan.c:		pgdat->kswapd_failures++;
vmscan.c: * pgdat->kswapd_classzone_idx is the highest zone index that a recent
vmscan.c:	if (pgdat->kswapd_classzone_idx == MAX_NR_ZONES)
vmscan.c:	return max(pgdat->kswapd_classzone_idx, classzone_idx);
vmscan.c:	prepare_to_wait(&pgdat->kswapd_wait, &wait, TASK_INTERRUPTIBLE);
vmscan.c:			pgdat->kswapd_classzone_idx = kswapd_classzone_idx(pgdat, classzone_idx);
vmscan.c:			pgdat->kswapd_order = max(pgdat->kswapd_order, reclaim_order);
vmscan.c:		finish_wait(&pgdat->kswapd_wait, &wait);
vmscan.c:		prepare_to_wait(&pgdat->kswapd_wait, &wait, TASK_INTERRUPTIBLE);
vmscan.c:		trace_mm_vmscan_kswapd_sleep(pgdat->node_id);
vmscan.c:		 * per-cpu vmstat threshold while kswapd is awake and restore
vmscan.c:	finish_wait(&pgdat->kswapd_wait, &wait);
vmscan.c: * If there are applications that are active memory-allocators
vmscan.c:	unsigned int classzone_idx = MAX_NR_ZONES - 1;
vmscan.c:	const struct cpumask *cpumask = cpumask_of_node(pgdat->node_id);
vmscan.c:	current->reclaim_state = &reclaim_state;
vmscan.c:	tsk->flags |= PF_MEMALLOC | PF_SWAPWRITE | PF_KSWAPD;
vmscan.c:	pgdat->kswapd_order = 0;
vmscan.c:	pgdat->kswapd_classzone_idx = MAX_NR_ZONES;
vmscan.c:		alloc_order = reclaim_order = pgdat->kswapd_order;
vmscan.c:		alloc_order = reclaim_order = pgdat->kswapd_order;
vmscan.c:		pgdat->kswapd_order = 0;
vmscan.c:		pgdat->kswapd_classzone_idx = MAX_NR_ZONES;
vmscan.c:		 * Reclaim begins at the requested order but if a high-order
vmscan.c:		 * order-0. If that happens, kswapd will consider sleeping
vmscan.c:		trace_mm_vmscan_kswapd_wake(pgdat->node_id, classzone_idx,
vmscan.c:	tsk->flags &= ~(PF_MEMALLOC | PF_SWAPWRITE | PF_KSWAPD);
vmscan.c:	current->reclaim_state = NULL;
vmscan.c:	pgdat = zone->zone_pgdat;
vmscan.c:	pgdat->kswapd_classzone_idx = kswapd_classzone_idx(pgdat,
vmscan.c:	pgdat->kswapd_order = max(pgdat->kswapd_order, order);
vmscan.c:	if (!waitqueue_active(&pgdat->kswapd_wait))
vmscan.c:	if (pgdat->kswapd_failures >= MAX_RECLAIM_RETRIES)
vmscan.c:	trace_mm_vmscan_wakeup_kswapd(pgdat->node_id, classzone_idx, order);
vmscan.c:	wake_up_interruptible(&pgdat->kswapd_wait);
vmscan.c: * Try to free `nr_to_reclaim' of memory, system-wide, and return the number of
vmscan.c:		.reclaim_idx = MAX_NR_ZONES - 1,
vmscan.c:	p->reclaim_state = &reclaim_state;
vmscan.c:	p->reclaim_state = NULL;
vmscan.c:		mask = cpumask_of_node(pgdat->node_id);
vmscan.c:			set_cpus_allowed_ptr(pgdat->kswapd, mask);
vmscan.c: * This kswapd start function will be called by init and node-hot-add.
vmscan.c: * On node-hot-add, kswapd will moved to proper cpus if cpus are hot-added.
vmscan.c:	if (pgdat->kswapd)
vmscan.c:	pgdat->kswapd = kthread_run(kswapd, pgdat, "kswapd%d", nid);
vmscan.c:	if (IS_ERR(pgdat->kswapd)) {
vmscan.c:		ret = PTR_ERR(pgdat->kswapd);
vmscan.c:		pgdat->kswapd = NULL;
vmscan.c:	struct task_struct *kswapd = NODE_DATA(nid)->kswapd;
vmscan.c:		NODE_DATA(nid)->kswapd = NULL;
vmscan.c: * If non-zero call node_reclaim when the number of free pages falls below
vmscan.c:	return (file_lru > file_mapped) ? (file_lru - file_mapped) : 0;
vmscan.c:	return nr_pagecache_reclaimable - delta;
vmscan.c:	p->flags |= PF_SWAPWRITE;
vmscan.c:	p->reclaim_state = &reclaim_state;
vmscan.c:	if (node_pagecache_reclaimable(pgdat) > pgdat->min_unmapped_pages) {
vmscan.c:		} while (sc.nr_reclaimed < nr_pages && --sc.priority >= 0);
vmscan.c:	p->reclaim_state = NULL;
vmscan.c:	current->flags &= ~PF_SWAPWRITE;
vmscan.c:	if (node_pagecache_reclaimable(pgdat) <= pgdat->min_unmapped_pages &&
vmscan.c:	    node_page_state(pgdat, NR_SLAB_RECLAIMABLE) <= pgdat->min_slab_pages)
vmscan.c:	if (!gfpflags_allow_blocking(gfp_mask) || (current->flags & PF_MEMALLOC))
vmscan.c:	if (node_state(pgdat->node_id, N_CPU) && pgdat->node_id != numa_node_id())
vmscan.c:	if (test_and_set_bit(PGDAT_RECLAIM_LOCKED, &pgdat->flags))
vmscan.c:	clear_bit(PGDAT_RECLAIM_LOCKED, &pgdat->flags);
vmscan.c: * page_evictable - test whether a page is evictable
vmscan.c: * Test whether page is evictable--i.e., should be placed on active/inactive
vmscan.c: * check_move_unevictable_pages - check pages for evictability and move to appropriate zone lru list
vmscan.c:				spin_unlock_irq(&pgdat->lru_lock);
vmscan.c:			spin_lock_irq(&pgdat->lru_lock);
vmscan.c:		spin_unlock_irq(&pgdat->lru_lock);
Kconfig:	bool "Allow for memory hot-add"
Kconfig:	  See Documentation/memory-hotplug.txt for more information.
Kconfig:	  Say Y here if you want all hot-plugged memory blocks to appear in
Kconfig:	  Say N here if you want the default policy to keep all hot-plugged
Kconfig:# Heavily threaded applications may benefit from splitting the mm-wide
Kconfig:# ARM's adjust_pte (unused if VIPT) depends on mm-wide page_table_lock.
Kconfig:# PA-RISC 7xxx's spinlock_t would enlarge struct page from 32 to 44 bytes.
Kconfig:          linux-mm@kvack.org.
Kconfig:# a 32-bit address to OHCI.  So we need to use a bounce pool instead.
Kconfig:	  allocator for chunks in 2^N*PAGE_SIZE amounts - which is frequently
Kconfig:	  long-term mappings means that the space is wasted.
Kconfig:	  See Documentation/nommu-mmap.txt for more information.
Kconfig:	  Cleancache can be thought of as a page-granularity victim cache
Kconfig:	  time-varying size.  And when a cleancache-enabled
Kconfig:	  are reduced to a single pointer-compare-against-NULL resulting
Kconfig:	  time-varying size.  When space in transcendent memory is available,
Kconfig:	  available, all frontswap calls are reduced to a single pointer-
Kconfig:	  compare-against-NULL resulting in a negligible performance hit
Kconfig:	  subsystems to allocate big physically-contiguous blocks of memory.
Kconfig:	  soft-dirty bit on pte-s. This bit it set when someone writes
Kconfig:	  See Documentation/vm/soft-dirty.txt for more details.
Kconfig:	  compress them into a dynamically allocated RAM-based memory pool.
Kconfig:	  zsmalloc is a slab-based memory allocator designed to store
Kconfig:	  non-standard allocator interface where a handle, not a pointer, is
Kconfig:	  By default, zsmalloc uses a copy-based object mapping method to
Kconfig:	int "Maximum user stack size for 32-bit processes (MB)"
Kconfig:	  This is the maximum stack size in Megabytes in the VM layout of 32-bit
Kconfig:	  by starting one-off "pgdatinitX" kernel thread for each node X. This
Kconfig:	  "device-physical" addresses which is needed for using a DAX
Kconfig:	  Prerequisites: the device must provide the ability to write-protect its
page_alloc.c:#include <linux/backing-dev.h>
page_alloc.c:#include <linux/fault-inject.h>
page_alloc.c:#include <linux/page-isolation.h>
page_alloc.c:/* prevent >1 _updater_ of zone percpu pageset ->high and ->batch fields */
page_alloc.c:/* work_structs for global per-cpu drains */
page_alloc.c:/* Protect totalram_pages and zone->managed_pages */
page_alloc.c: * other index - this ensures that it will be put on the correct CMA freelist.
page_alloc.c:	return page->index;
page_alloc.c:	page->index = migratetype;
page_alloc.c: *	1G machine -> (16M dma, 800M-16M normal, 1G-800M high)
page_alloc.c: *	1G machine -> (16M dma, 784M normal, 224M high)
page_alloc.c: * TBD: should special case ZONE_DMA32 machines here - in those we normally
page_alloc.c:int sysctl_lowmem_reserve_ratio[MAX_NR_ZONES-1] = {
page_alloc.c:int user_min_free_kbytes = -1;
page_alloc.c: * (non-deferred initialization).
page_alloc.c: * The value of first_deferred_pfn will be set later, once non-deferred pages
page_alloc.c:	max_pgcnt = max(2UL << (30 - PAGE_SHIFT),
page_alloc.c:			(pgdat->node_spanned_pages >> 8));
page_alloc.c:	start_addr = PFN_PHYS(pgdat->node_start_pfn);
page_alloc.c:	end_addr = PFN_PHYS(pgdat->node_start_pfn + max_pgcnt);
page_alloc.c:	pgdat->static_init_pgcnt = min(max_pgcnt, pgdat->node_spanned_pages);
page_alloc.c:	pgdat->first_deferred_pfn = ULONG_MAX;
page_alloc.c:	if (node_online(nid) && pfn >= NODE_DATA(nid)->first_deferred_pfn)
page_alloc.c:	/* Always populate low zones for address-contrained allocations */
page_alloc.c:	if ((*nr_initialised > pgdat->static_init_pgcnt) &&
page_alloc.c:	    (pfn & (PAGES_PER_SECTION - 1)) == 0) {
page_alloc.c:		pgdat->first_deferred_pfn = pfn;
page_alloc.c:	return __pfn_to_section(pfn)->pageblock_flags;
page_alloc.c:	return page_zone(page)->pageblock_flags;
page_alloc.c:	pfn &= (PAGES_PER_SECTION-1);
page_alloc.c:	pfn = pfn - round_down(page_zone(page)->zone_start_pfn, pageblock_nr_pages);
page_alloc.c: * get_pfnblock_flags_mask - Return the requested group of flags for the pageblock_nr_pages block of pages
page_alloc.c:	bitidx &= (BITS_PER_LONG-1);
page_alloc.c:	return (word >> (BITS_PER_LONG - bitidx - 1)) & mask;
page_alloc.c: * set_pfnblock_flags_mask - Set the requested group of flags for a pageblock_nr_pages block of pages
page_alloc.c:	bitidx &= (BITS_PER_LONG-1);
page_alloc.c:	mask <<= (BITS_PER_LONG - bitidx - 1);
page_alloc.c:	flags <<= (BITS_PER_LONG - bitidx - 1);
page_alloc.c:		start_pfn = zone->zone_start_pfn;
page_alloc.c:		sp = zone->spanned_pages;
page_alloc.c:		pr_err("page 0x%lx outside node %d zone %s [ 0x%lx - 0x%lx ]\n",
page_alloc.c:			pfn, zone_to_nid(zone), zone->name,
page_alloc.c:		current->comm, page_to_pfn(page));
page_alloc.c:	bad_flags &= page->flags;
page_alloc.c: * Higher-order pages are called "compound pages".  They are structured thusly:
page_alloc.c: * in bit 0 of page->compound_head. The rest of bits is pointer to head page.
page_alloc.c: * The first tail page's ->compound_dtor holds the offset in array of compound
page_alloc.c: * The first tail page's ->compound_order holds the order of allocation.
page_alloc.c: * This usage means that zero-order pages may not be compound.
page_alloc.c:		p->mapping = TAIL_MAPPING;
page_alloc.c:	atomic_set(compound_mapcount_ptr(page), -1);
page_alloc.c:		return -EINVAL;
page_alloc.c:	__set_bit(PAGE_EXT_DEBUG_GUARD, &page_ext->flags);
page_alloc.c:	INIT_LIST_HEAD(&page->lru);
page_alloc.c:	__mod_zone_freepage_state(zone, -(1 << order), migratetype);
page_alloc.c:	__clear_bit(PAGE_EXT_DEBUG_GUARD, &page_ext->flags);
page_alloc.c: * For recording whether a page is in the buddy system, we set ->_mapcount
page_alloc.c: * serialized by zone->lock.
page_alloc.c: * The concept of a buddy system is to maintain direct-mapped table
page_alloc.c: * -- nyc
page_alloc.c:	VM_BUG_ON_PAGE(page->flags & PAGE_FLAGS_CHECK_AT_PREP, page);
page_alloc.c:	VM_BUG_ON(migratetype == -1);
page_alloc.c:	VM_BUG_ON_PAGE(pfn & ((1 << order) - 1), page);
page_alloc.c:	while (order < max_order - 1) {
page_alloc.c:		buddy = page + (buddy_pfn - pfn);
page_alloc.c:			list_del(&buddy->lru);
page_alloc.c:			zone->free_area[order].nr_free--;
page_alloc.c:		page = page + (combined_pfn - pfn);
page_alloc.c:		 * low-order merging.
page_alloc.c:			buddy = page + (buddy_pfn - pfn);
page_alloc.c:	 * of the next-highest order is free. If it is, it's possible
page_alloc.c:	if ((order < MAX_ORDER-2) && pfn_valid_within(buddy_pfn)) {
page_alloc.c:		higher_page = page + (combined_pfn - pfn);
page_alloc.c:		higher_buddy = higher_page + (buddy_pfn - combined_pfn);
page_alloc.c:			list_add_tail(&page->lru,
page_alloc.c:				&zone->free_area[order].free_list[migratetype]);
page_alloc.c:	list_add(&page->lru, &zone->free_area[order].free_list[migratetype]);
page_alloc.c:	zone->free_area[order].nr_free++;
page_alloc.c:	if (unlikely(atomic_read(&page->_mapcount) != -1))
page_alloc.c:	if (unlikely((unsigned long)page->mapping |
page_alloc.c:			(unsigned long)page->mem_cgroup |
page_alloc.c:			(page->flags & check_flags)))
page_alloc.c:	if (unlikely(atomic_read(&page->_mapcount) != -1))
page_alloc.c:	if (unlikely(page->mapping != NULL))
page_alloc.c:		bad_reason = "non-NULL mapping";
page_alloc.c:	if (unlikely(page->flags & PAGE_FLAGS_CHECK_AT_FREE)) {
page_alloc.c:	if (unlikely(page->mem_cgroup))
page_alloc.c:	 * We rely page->lru.next never has bit 0 set, unless the page
page_alloc.c:	 * is PageTail(). Let's make sure that's true even for poisoned ->lru.
page_alloc.c:	switch (page - head_page) {
page_alloc.c:		/* the first tail page: ->mapping is compound_mapcount() */
page_alloc.c:		 * the second tail page: ->mapping is
page_alloc.c:		 * page_deferred_list().next -- ignore value.
page_alloc.c:		if (page->mapping != TAIL_MAPPING) {
page_alloc.c:	page->mapping = NULL;
page_alloc.c:	 * avoid checking PageCompound for order-0 pages.
page_alloc.c:			(page + i)->flags &= ~PAGE_FLAGS_CHECK_AT_PREP;
page_alloc.c:		page->mapping = NULL;
page_alloc.c:	page->flags &= ~PAGE_FLAGS_CHECK_AT_PREP;
page_alloc.c:	spin_lock(&zone->lock);
page_alloc.c:		 * Remove pages from lists in a round-robin fashion. A
page_alloc.c:			list = &pcp->lists[migratetype];
page_alloc.c:		/* This is the only non-empty list. Free them all. */
page_alloc.c:			int mt;	/* migratetype of the to-be-freed page */
page_alloc.c:			list_del(&page->lru);
page_alloc.c:		} while (--count && --batch_free && !list_empty(list));
page_alloc.c:	spin_unlock(&zone->lock);
page_alloc.c:	spin_lock(&zone->lock);
page_alloc.c:	spin_unlock(&zone->lock);
page_alloc.c:	INIT_LIST_HEAD(&page->lru);
page_alloc.c:		struct zone *zone = &pgdat->node_zones[zid];
page_alloc.c:		if (pfn >= zone->zone_start_pfn && pfn < zone_end_pfn(zone))
page_alloc.c:			/* Avoid false-positive PageTail() */
page_alloc.c:			INIT_LIST_HEAD(&page->lru);
page_alloc.c:	for (loop = 0; loop < (nr_pages - 1); loop++, p++) {
page_alloc.c:	page_zone(page)->managed_pages += nr_pages;
page_alloc.c:/* Only safe to use early in boot when initialisation is single-threaded */
page_alloc.c:	end_pfn--;
page_alloc.c:	unsigned long block_start_pfn = zone->zone_start_pfn;
page_alloc.c:	zone->contiguous = true;
page_alloc.c:	zone->contiguous = false;
page_alloc.c:	/* Free a large naturally-aligned chunk if possible */
page_alloc.c:	    (pfn & (pageblock_nr_pages - 1)) == 0) {
page_alloc.c:		if ((pfn & (pageblock_nr_pages - 1)) == 0)
page_alloc.c:	int nid = pgdat->node_id;
page_alloc.c:	unsigned long first_init_pfn = pgdat->first_deferred_pfn;
page_alloc.c:	const struct cpumask *cpumask = cpumask_of_node(pgdat->node_id);
page_alloc.c:	BUG_ON(pgdat->first_deferred_pfn < pgdat->node_start_pfn);
page_alloc.c:	BUG_ON(pgdat->first_deferred_pfn > pgdat_end_pfn(pgdat));
page_alloc.c:	pgdat->first_deferred_pfn = ULONG_MAX;
page_alloc.c:		zone = pgdat->node_zones + zid;
page_alloc.c:		if (pfn < zone->zone_start_pfn)
page_alloc.c:			pfn = zone->zone_start_pfn;
page_alloc.c:			if ((pfn & (pageblock_nr_pages - 1)) == 0) {
page_alloc.c:			if (page && (pfn & (pageblock_nr_pages - 1)) != 0) {
page_alloc.c:			if (page->flags) {
page_alloc.c:					jiffies_to_msecs(jiffies - start));
page_alloc.c:	} while (++p, --i);
page_alloc.c:			__free_pages(p, MAX_ORDER - 1);
page_alloc.c:		} while (i -= MAX_ORDER_NR_PAGES);
page_alloc.c: * -- nyc
page_alloc.c:		area--;
page_alloc.c:		high--;
page_alloc.c:		list_add(&page[size].lru, &area->free_list[migratetype]);
page_alloc.c:		area->nr_free++;
page_alloc.c:	if (unlikely(atomic_read(&page->_mapcount) != -1))
page_alloc.c:	if (unlikely(page->mapping != NULL))
page_alloc.c:		bad_reason = "non-NULL mapping";
page_alloc.c:	if (unlikely(page->flags & __PG_HWPOISON)) {
page_alloc.c:		bad_reason = "HWPoisoned (hardware-corrupted)";
page_alloc.c:	if (unlikely(page->flags & PAGE_FLAGS_CHECK_AT_PREP)) {
page_alloc.c:	if (unlikely(page->mem_cgroup))
page_alloc.c:		area = &(zone->free_area[current_order]);
page_alloc.c:		page = list_first_entry_or_null(&area->free_list[migratetype],
page_alloc.c:		list_del(&page->lru);
page_alloc.c:		area->nr_free--;
page_alloc.c:		list_move(&page->lru,
page_alloc.c:			  &zone->free_area[order].free_list[migratetype]);
page_alloc.c:	start_pfn = start_pfn & ~(pageblock_nr_pages-1);
page_alloc.c:	end_page = start_page + pageblock_nr_pages - 1;
page_alloc.c:	end_pfn = start_pfn + pageblock_nr_pages - 1;
page_alloc.c:	int nr_pageblocks = 1 << (start_order - pageblock_order);
page_alloc.c:	while (nr_pageblocks--) {
page_alloc.c: * pageblock to our migratetype and determine how many already-allocated pages
page_alloc.c:		 * to MOVABLE pageblock, consider all non-movable pages as
page_alloc.c:		 * exact migratetype of non-movable pages.
page_alloc.c:						- (free_pages + movable_pages);
page_alloc.c:	if (free_pages + alike_pages >= (1 << (pageblock_order-1)) ||
page_alloc.c:	area = &zone->free_area[current_order];
page_alloc.c:	list_move(&page->lru, &area->free_list[start_type]);
page_alloc.c:	if (area->nr_free == 0)
page_alloc.c:		return -1;
page_alloc.c:		if (list_empty(&area->free_list[fallback_mt]))
page_alloc.c:	return -1;
page_alloc.c: * Reserve a pageblock for exclusive use of high-order atomic allocations if
page_alloc.c:	 * Check is race-prone but harmless.
page_alloc.c:	max_managed = (zone->managed_pages / 100) + pageblock_nr_pages;
page_alloc.c:	if (zone->nr_reserved_highatomic >= max_managed)
page_alloc.c:	spin_lock_irqsave(&zone->lock, flags);
page_alloc.c:	if (zone->nr_reserved_highatomic >= max_managed)
page_alloc.c:		zone->nr_reserved_highatomic += pageblock_nr_pages;
page_alloc.c:	spin_unlock_irqrestore(&zone->lock, flags);
page_alloc.c: * potentially hurts the reliability of high-order allocations when under
page_alloc.c:	struct zonelist *zonelist = ac->zonelist;
page_alloc.c:	for_each_zone_zonelist_nodemask(zone, z, zonelist, ac->high_zoneidx,
page_alloc.c:								ac->nodemask) {
page_alloc.c:		if (!force && zone->nr_reserved_highatomic <=
page_alloc.c:		spin_lock_irqsave(&zone->lock, flags);
page_alloc.c:			struct free_area *area = &(zone->free_area[order]);
page_alloc.c:					&area->free_list[MIGRATE_HIGHATOMIC],
page_alloc.c:			 * from highatomic to ac->migratetype. So we should
page_alloc.c:				 * locking could inadvertently allow a per-cpu
page_alloc.c:				zone->nr_reserved_highatomic -= min(
page_alloc.c:						zone->nr_reserved_highatomic);
page_alloc.c:			 * Convert to ac->migratetype and avoid the normal
page_alloc.c:			set_pageblock_migratetype(page, ac->migratetype);
page_alloc.c:			ret = move_freepages_block(zone, page, ac->migratetype,
page_alloc.c:				spin_unlock_irqrestore(&zone->lock, flags);
page_alloc.c:		spin_unlock_irqrestore(&zone->lock, flags);
page_alloc.c:	for (current_order = MAX_ORDER - 1; current_order >= order;
page_alloc.c:				--current_order) {
page_alloc.c:		area = &(zone->free_area[current_order]);
page_alloc.c:		if (fallback_mt == -1)
page_alloc.c:		area = &(zone->free_area[current_order]);
page_alloc.c:		if (fallback_mt != -1)
page_alloc.c:	 * This should not happen - we already found a suitable fallback
page_alloc.c:	page = list_first_entry(&area->free_list[fallback_mt],
page_alloc.c: * Call me with the zone->lock already held.
page_alloc.c:	spin_lock(&zone->lock);
page_alloc.c:			list_add(&page->lru, list);
page_alloc.c:			list_add_tail(&page->lru, list);
page_alloc.c:		list = &page->lru;
page_alloc.c:					      -(1 << order));
page_alloc.c:	__mod_zone_page_state(zone, NR_FREE_PAGES, -(i << order));
page_alloc.c:	spin_unlock(&zone->lock);
page_alloc.c:	batch = READ_ONCE(pcp->batch);
page_alloc.c:	to_drain = min(pcp->count, batch);
page_alloc.c:		pcp->count -= to_drain;
page_alloc.c:	pset = per_cpu_ptr(zone->pageset, cpu);
page_alloc.c:	pcp = &pset->pcp;
page_alloc.c:	if (pcp->count) {
page_alloc.c:		free_pcppages_bulk(zone, pcp->count, pcp);
page_alloc.c:		pcp->count = 0;
page_alloc.c: * Spill all of this CPU's per-cpu pages back into the buddy allocator.
page_alloc.c: * The CPU has to be pinned. When zone parameter is non-NULL, spill just
page_alloc.c: * Spill all the per-cpu pages from all CPUs back into the buddy allocator.
page_alloc.c: * When zone parameter is non-NULL, spill just the single zone's pages.
page_alloc.c:			pcp = per_cpu_ptr(zone->pageset, cpu);
page_alloc.c:			if (pcp->pcp.count)
page_alloc.c:				pcp = per_cpu_ptr(z->pageset, cpu);
page_alloc.c:				if (pcp->pcp.count) {
page_alloc.c:	spin_lock_irqsave(&zone->lock, flags);
page_alloc.c:	for (pfn = zone->zone_start_pfn; pfn < max_zone_pfn; pfn++)
page_alloc.c:			if (!--page_count) {
page_alloc.c:				&zone->free_area[order].free_list[t], lru) {
page_alloc.c:				if (!--page_count) {
page_alloc.c:	spin_unlock_irqrestore(&zone->lock, flags);
page_alloc.c: * Free a 0-order page
page_alloc.c:	pcp = &this_cpu_ptr(zone->pageset)->pcp;
page_alloc.c:		list_add(&page->lru, &pcp->lists[migratetype]);
page_alloc.c:		list_add_tail(&page->lru, &pcp->lists[migratetype]);
page_alloc.c:	pcp->count++;
page_alloc.c:	if (pcp->count >= pcp->high) {
page_alloc.c:		unsigned long batch = READ_ONCE(pcp->batch);
page_alloc.c:		pcp->count -= batch;
page_alloc.c: * Free a list of 0-order pages
page_alloc.c: * split_page takes a non-compound higher-order page, and splits it into
page_alloc.c: * n (1<<order) sub-pages: page[0..n]
page_alloc.c: * Each sub-page must be freed individually.
page_alloc.c:		 * emulate a high-order watermark check with a raised order-0
page_alloc.c:		 * watermark, because we already know our high-order page
page_alloc.c:		__mod_zone_freepage_state(zone, -(1UL << order), mt);
page_alloc.c:	list_del(&page->lru);
page_alloc.c:	zone->free_area[order].nr_free--;
page_alloc.c:	if (order >= pageblock_order - 1) {
page_alloc.c:		struct page *endpage = page + (1 << order) - 1;
page_alloc.c:	if (z->node != numa_node_id())
page_alloc.c:	if (z->node == preferred_zone->node)
page_alloc.c:/* Remove page from the per-cpu list, caller must protect the list */
page_alloc.c:			pcp->count += rmqueue_bulk(zone, 0,
page_alloc.c:					pcp->batch, list,
page_alloc.c:		list_del(&page->lru);
page_alloc.c:		pcp->count--;
page_alloc.c:/* Lock and remove page from the per-cpu list */
page_alloc.c:	pcp = &this_cpu_ptr(zone->pageset)->pcp;
page_alloc.c:	list = &pcp->lists[migratetype];
page_alloc.c: * Allocate a page from the given zone. Use pcplists for order-0 allocations.
page_alloc.c:	 * allocate greater than order-1 page units with __GFP_NOFAIL.
page_alloc.c:	spin_lock_irqsave(&zone->lock, flags);
page_alloc.c:	spin_unlock(&zone->lock);
page_alloc.c:	__mod_zone_freepage_state(zone, -(1 << order),
page_alloc.c:	if (!debugfs_create_bool("ignore-gfp-wait", mode, dir,
page_alloc.c:	if (!debugfs_create_bool("ignore-gfp-highmem", mode, dir,
page_alloc.c:	if (!debugfs_create_u32("min-order", mode, dir,
page_alloc.c:	return -ENOMEM;
page_alloc.c: * Return true if free base pages are above 'mark'. For high-order checks it
page_alloc.c: * will return true of the order-0 watermark is reached and there is at least
page_alloc.c:	/* free_pages may go negative - that's OK */
page_alloc.c:	free_pages -= (1 << order) - 1;
page_alloc.c:		min -= min / 2;
page_alloc.c:	 * the high-atomic reserves. This will over-estimate the size of the
page_alloc.c:		free_pages -= z->nr_reserved_highatomic;
page_alloc.c:		 * makes during the free path will be small and short-lived.
page_alloc.c:			min -= min / 2;
page_alloc.c:			min -= min / 4;
page_alloc.c:		free_pages -= zone_page_state(z, NR_FREE_CMA_PAGES);
page_alloc.c:	 * Check watermarks for an order-0 allocation request. If these
page_alloc.c:	 * are not met, then a high-order request also cannot go ahead
page_alloc.c:	if (free_pages <= min + z->lowmem_reserve[classzone_idx])
page_alloc.c:	/* If this is an order-0 request then the watermark is fine */
page_alloc.c:	/* For a high-order request, check at least one suitable page is free */
page_alloc.c:		struct free_area *area = &z->free_area[o];
page_alloc.c:		if (!area->nr_free)
page_alloc.c:			if (!list_empty(&area->free_list[mt]))
page_alloc.c:		    !list_empty(&area->free_list[MIGRATE_CMA])) {
page_alloc.c:			!list_empty(&area->free_list[MIGRATE_HIGHATOMIC]))
page_alloc.c:	 * Fast check for order-0 only. If this fails then the reserves
page_alloc.c:	 * passes but only the high-order atomic reserve are free. If
page_alloc.c:	if (!order && (free_pages - cma_pages) > mark + z->lowmem_reserve[classzone_idx])
page_alloc.c:	if (z->percpu_drift_mark && free_pages < z->percpu_drift_mark)
page_alloc.c:	struct zoneref *z = ac->preferred_zoneref;
page_alloc.c:	for_next_zone_zonelist_nodemask(zone, z, ac->zonelist, ac->high_zoneidx,
page_alloc.c:								ac->nodemask) {
page_alloc.c:		 * exceed the per-node dirty limit in the slowpath
page_alloc.c:		 * dirty-throttling and the flusher threads.
page_alloc.c:		if (ac->spread_dirty_pages) {
page_alloc.c:			if (last_pgdat_dirty_limit == zone->zone_pgdat)
page_alloc.c:			if (!node_dirty_ok(zone->zone_pgdat)) {
page_alloc.c:				last_pgdat_dirty_limit = zone->zone_pgdat;
page_alloc.c:		mark = zone->watermark[alloc_flags & ALLOC_WMARK_MASK];
page_alloc.c:			    !zone_allows_reclaim(ac->preferred_zoneref->zone, zone))
page_alloc.c:			ret = node_reclaim(zone->zone_pgdat, gfp_mask, order);
page_alloc.c:		page = rmqueue(ac->preferred_zoneref->zone, zone, order,
page_alloc.c:				gfp_mask, alloc_flags, ac->migratetype);
page_alloc.c:			 * If this is a high-order atomic allocation then check
page_alloc.c: * Large machines with many possible nodes should not always dump per-node
page_alloc.c:		    (current->flags & (PF_MEMALLOC | PF_EXITING)))
page_alloc.c:	pr_warn("%s: ", current->comm);
page_alloc.c:		.zonelist = ac->zonelist,
page_alloc.c:		.nodemask = ac->nodemask,
page_alloc.c:	if (current->flags & PF_DUMPCORE)
page_alloc.c:	if (ac->high_zoneidx < ZONE_NORMAL)
page_alloc.c:		 * Help non-failing allocations by giving them access to memory
page_alloc.c:/* Try memory compaction for high-order allocations before reclaim */
page_alloc.c:		zone->compact_blockskip_flush = false;
page_alloc.c:		(*compact_priority)--;
page_alloc.c:	 * Let's give them a good hope and keep retrying while the order-0
page_alloc.c:	for_each_zone_zonelist_nodemask(zone, z, ac->zonelist, ac->high_zoneidx,
page_alloc.c:					ac->nodemask) {
page_alloc.c:	if (current->flags & PF_MEMALLOC)
page_alloc.c:	current->reclaim_state = &reclaim_state;
page_alloc.c:	progress = try_to_free_pages(ac->zonelist, order, gfp_mask,
page_alloc.c:								ac->nodemask);
page_alloc.c:	current->reclaim_state = NULL;
page_alloc.c:	 * pages are pinned on the per-cpu lists or in high alloc reserves.
page_alloc.c:	for_each_zone_zonelist_nodemask(zone, z, ac->zonelist,
page_alloc.c:					ac->high_zoneidx, ac->nodemask) {
page_alloc.c:		if (last_pgdat != zone->zone_pgdat)
page_alloc.c:			wakeup_kswapd(zone, order, ac->high_zoneidx);
page_alloc.c:		last_pgdat = zone->zone_pgdat;
page_alloc.c:	if (in_serving_softirq() && (current->flags & PF_MEMALLOC))
page_alloc.c:		if (current->flags & PF_MEMALLOC)
page_alloc.c:	for_each_zone_zonelist_nodemask(zone, z, ac->zonelist, ac->high_zoneidx,
page_alloc.c:					ac->nodemask) {
page_alloc.c:			if (current->flags & PF_WQ_WORKER)
page_alloc.c:	 * This assumes that for all allocations, ac->nodemask can come only
page_alloc.c:	if (cpusets_enabled() && ac->nodemask &&
page_alloc.c:			!cpuset_nodemask_valid_mems_allowed(ac->nodemask)) {
page_alloc.c:		ac->nodemask = NULL;
page_alloc.c:	 * there was a cpuset modification and we are retrying - otherwise we
page_alloc.c:	 * could end up iterating over non-eligible zones endlessly.
page_alloc.c:	ac->preferred_zoneref = first_zones_zonelist(ac->zonelist,
page_alloc.c:					ac->high_zoneidx, ac->nodemask);
page_alloc.c:	if (!ac->preferred_zoneref->zone)
page_alloc.c:	 * that we have enough base pages and don't need to reclaim. For non-
page_alloc.c:	 * movable high-order allocations, do that as well, as compaction will
page_alloc.c:			   (order > 0 && ac->migratetype != MIGRATE_MOVABLE))
page_alloc.c:			 * If compaction is deferred for high-order allocations,
page_alloc.c:		ac->preferred_zoneref = first_zones_zonelist(ac->zonelist,
page_alloc.c:					ac->high_zoneidx, ac->nodemask);
page_alloc.c:		warn_alloc(gfp_mask & ~__GFP_NOWARN, ac->nodemask,
page_alloc.c:			jiffies_to_msecs(jiffies-alloc_start), order);
page_alloc.c:	if (current->flags & PF_MEMALLOC)
page_alloc.c:	 * It doesn't make any sense to retry for the compaction if the order-0
page_alloc.c:		WARN_ON_ONCE(current->flags & PF_MEMALLOC);
page_alloc.c:		 * Help non-failing allocations by giving them access to memory
page_alloc.c:	warn_alloc(gfp_mask, ac->nodemask,
page_alloc.c:	ac->high_zoneidx = gfp_zone(gfp_mask);
page_alloc.c:	ac->zonelist = node_zonelist(preferred_nid, gfp_mask);
page_alloc.c:	ac->nodemask = nodemask;
page_alloc.c:	ac->migratetype = gfpflags_to_migratetype(gfp_mask);
page_alloc.c:		if (!ac->nodemask)
page_alloc.c:			ac->nodemask = &cpuset_current_mems_allowed;
page_alloc.c:	if (IS_ENABLED(CONFIG_CMA) && ac->migratetype == MIGRATE_MOVABLE)
page_alloc.c:	ac->spread_dirty_pages = (gfp_mask & __GFP_WRITE);
page_alloc.c:	ac->preferred_zoneref = first_zones_zonelist(ac->zonelist,
page_alloc.c:					ac->high_zoneidx, ac->nodemask);
page_alloc.c:	 * &cpuset_current_mems_allowed to optimize the fast-path attempt.
page_alloc.c:	 * __get_free_pages() returns a 32-bit address, which cannot represent
page_alloc.c: *  An arbitrary-length arbitrary-offset area of memory which resides
page_alloc.c: * sk_buff->head, or to be used in the "frags" portion of skb_shared_info.
page_alloc.c:	nc->size = page ? PAGE_FRAG_CACHE_MAX_SIZE : PAGE_SIZE;
page_alloc.c:	nc->va = page ? page_address(page) : NULL;
page_alloc.c:	if (unlikely(!nc->va)) {
page_alloc.c:		size = nc->size;
page_alloc.c:		page_ref_add(page, size - 1);
page_alloc.c:		nc->pfmemalloc = page_is_pfmemalloc(page);
page_alloc.c:		nc->pagecnt_bias = size;
page_alloc.c:		nc->offset = size;
page_alloc.c:	offset = nc->offset - fragsz;
page_alloc.c:		page = virt_to_page(nc->va);
page_alloc.c:		if (!page_ref_sub_and_test(page, nc->pagecnt_bias))
page_alloc.c:		size = nc->size;
page_alloc.c:		nc->pagecnt_bias = size;
page_alloc.c:		offset = size - fragsz;
page_alloc.c:	nc->pagecnt_bias--;
page_alloc.c:	nc->offset = offset;
page_alloc.c:	return nc->va + offset;
page_alloc.c: * alloc_pages_exact - allocate an exact number physically-contiguous pages.
page_alloc.c: * allocate memory in power-of-two pages.
page_alloc.c: * alloc_pages_exact_nid - allocate an exact number of physically-contiguous
page_alloc.c: * free_pages_exact - release memory allocated via alloc_pages_exact()
page_alloc.c: * nr_free_zone_pages - count number of pages beyond high watermark
page_alloc.c: *     nr_free_zone_pages = managed_pages - high_pages
page_alloc.c:		unsigned long size = zone->managed_pages;
page_alloc.c:			sum += size - high;
page_alloc.c: * nr_free_buffer_pages - count number of pages beyond high watermark
page_alloc.c: * nr_free_pagecache_pages - count number of pages beyond high watermark
page_alloc.c:		wmark_low += zone->watermark[WMARK_LOW];
page_alloc.c:	available = global_zone_page_state(NR_FREE_PAGES) - totalreserve_pages;
page_alloc.c:	pagecache -= min(pagecache / 2, wmark_low);
page_alloc.c:	available += global_node_page_state(NR_SLAB_RECLAIMABLE) -
page_alloc.c:	val->totalram = totalram_pages;
page_alloc.c:	val->sharedram = global_node_page_state(NR_SHMEM);
page_alloc.c:	val->freeram = global_zone_page_state(NR_FREE_PAGES);
page_alloc.c:	val->bufferram = nr_blockdev_pages();
page_alloc.c:	val->totalhigh = totalhigh_pages;
page_alloc.c:	val->freehigh = nr_free_highpages();
page_alloc.c:	val->mem_unit = PAGE_SIZE;
page_alloc.c:		managed_pages += pgdat->node_zones[zone_type].managed_pages;
page_alloc.c:	val->totalram = managed_pages;
page_alloc.c:	val->sharedram = node_page_state(pgdat, NR_SHMEM);
page_alloc.c:	val->freeram = sum_zone_node_page_state(nid, NR_FREE_PAGES);
page_alloc.c:		struct zone *zone = &pgdat->node_zones[zone_type];
page_alloc.c:			managed_highpages += zone->managed_pages;
page_alloc.c:	val->totalhigh = managed_highpages;
page_alloc.c:	val->freehigh = free_highpages;
page_alloc.c:	val->totalhigh = managed_highpages;
page_alloc.c:	val->freehigh = free_highpages;
page_alloc.c:	val->mem_unit = PAGE_SIZE;
page_alloc.c:	 * no node mask - aka implicit memory numa policy. Do not bother with
page_alloc.c:	 * the synchronization - read_mems_allowed_begin - because we do not
page_alloc.c:#define K(x) ((x) << (PAGE_SHIFT-10))
page_alloc.c: * Show free area list (used inside shift_scroll-lock stuff)
page_alloc.c:			free_pcp += per_cpu_ptr(zone->pageset, cpu)->pcp.count;
page_alloc.c:		if (show_mem_node_skip(filter, pgdat->node_id, nodemask))
page_alloc.c:			pgdat->node_id,
page_alloc.c:			pgdat->kswapd_failures >= MAX_RECLAIM_RETRIES ?
page_alloc.c:			free_pcp += per_cpu_ptr(zone->pageset, cpu)->pcp.count;
page_alloc.c:			zone->name,
page_alloc.c:			K(zone->present_pages),
page_alloc.c:			K(zone->managed_pages),
page_alloc.c:			K(this_cpu_read(zone->pageset->pcp.count)),
page_alloc.c:			printk(KERN_CONT " %ld", zone->lowmem_reserve[i]);
page_alloc.c:		printk(KERN_CONT "%s: ", zone->name);
page_alloc.c:		spin_lock_irqsave(&zone->lock, flags);
page_alloc.c:			struct free_area *area = &zone->free_area[order];
page_alloc.c:			nr[order] = area->nr_free;
page_alloc.c:				if (!list_empty(&area->free_list[type]))
page_alloc.c:		spin_unlock_irqrestore(&zone->lock, flags);
page_alloc.c:	zoneref->zone = zone;
page_alloc.c:	zoneref->zone_idx = zone_idx(zone);
page_alloc.c:		zone_type--;
page_alloc.c:		zone = pgdat->node_zones + zone_type;
page_alloc.c:		return -EINVAL;
page_alloc.c: * find_next_best_node - find the next node that should appear in a given node's fallback list
page_alloc.c: * It returns -1 if no node is found.
page_alloc.c: * This results in maximum locality--normal zone overflows into local
page_alloc.c: * DMA zone, if any--but risks exhausting DMA zone.
page_alloc.c:	zonerefs = pgdat->node_zonelists[ZONELIST_FALLBACK]._zonerefs;
page_alloc.c:	zonerefs->zone = NULL;
page_alloc.c:	zonerefs->zone_idx = 0;
page_alloc.c:	zonerefs = pgdat->node_zonelists[ZONELIST_NOFALLBACK]._zonerefs;
page_alloc.c:	zonerefs->zone = NULL;
page_alloc.c:	zonerefs->zone_idx = 0;
page_alloc.c:	/* NUMA-aware ordering of nodes */
page_alloc.c:	local_node = pgdat->node_id;
page_alloc.c:		 * distance group to make it round-robin.
page_alloc.c:		load--;
page_alloc.c:	return z->zone->node;
page_alloc.c:	local_node = pgdat->node_id;
page_alloc.c:	zonerefs = pgdat->node_zonelists[ZONELIST_FALLBACK]._zonerefs;
page_alloc.c:	zonerefs->zone = NULL;
page_alloc.c:	zonerefs->zone_idx = 0;
page_alloc.c:	 * building zonelists is fine - no need to touch other nodes.
page_alloc.c:	if (self && !node_online(self->node_id)) {
page_alloc.c:		 * We now know the "local memory node" for each node--
page_alloc.c:		 * Set up numa_mem percpu variable for on-line cpus.  During
page_alloc.c:		 * boot, only the boot cpu should be on-line;  we'll init the
page_alloc.c:		 * secondary cpus' numa_mem as they come on-line.  During
page_alloc.c:		 * node/memory hotplug, we'll fixup all on-line cpus.
page_alloc.c:	 * (a chicken-egg dilemma).
page_alloc.c:	 * more accurate, but expensive to check per-zone. This check is
page_alloc.c:	 * made on memory-hotadd so a system can start with mobility
page_alloc.c: * Initially all pages are reserved - free ones are freed
page_alloc.c: * done. Non-atomic initialization, single-pass.
page_alloc.c:	if (highest_memmap_pfn < end_pfn - 1)
page_alloc.c:		highest_memmap_pfn = end_pfn - 1;
page_alloc.c:	if (altmap && start_pfn == altmap->base_pfn)
page_alloc.c:		start_pfn += altmap->reserve;
page_alloc.c:		 * There can be holes in boot-time mem_map[]s handed to this
page_alloc.c:		 * the address space during boot when many long-lived
page_alloc.c:		if (!(pfn & (pageblock_nr_pages - 1))) {
page_alloc.c:		INIT_LIST_HEAD(&zone->free_area[order].free_list[t]);
page_alloc.c:		zone->free_area[order].nr_free = 0;
page_alloc.c:	 * The per-cpu-pages pools are set to around 1000th of the
page_alloc.c:	batch = zone->managed_pages / 1024;
page_alloc.c:	 * Clamp the batch to a 2^n - 1 value. Having a power
page_alloc.c:	batch = rounddown_pow_of_two(batch + batch/2) - 1;
page_alloc.c:	 * fragmented and becoming unavailable for high-order allocations.
page_alloc.c: * pcp->high and pcp->batch values are related and dependent on one another:
page_alloc.c: * ->batch must never be higher then ->high.
page_alloc.c: * Any new users of pcp->batch and pcp->high should ensure they can cope with
page_alloc.c:	pcp->batch = 1;
page_alloc.c:	pcp->high = high;
page_alloc.c:	pcp->batch = batch;
page_alloc.c:	pageset_update(&p->pcp, 6 * batch, max(1UL, 1 * batch));
page_alloc.c:	pcp = &p->pcp;
page_alloc.c:	pcp->count = 0;
page_alloc.c:		INIT_LIST_HEAD(&pcp->lists[migratetype]);
page_alloc.c:	pageset_update(&p->pcp, high, batch);
page_alloc.c:			(zone->managed_pages /
page_alloc.c:	struct per_cpu_pageset *pcp = per_cpu_ptr(zone->pageset, cpu);
page_alloc.c:	zone->pageset = alloc_percpu(struct per_cpu_pageset);
page_alloc.c:		pgdat->per_cpu_nodestats =
page_alloc.c:	zone->pageset = &boot_pageset;
page_alloc.c:			zone->name, zone->present_pages,
page_alloc.c:	struct pglist_data *pgdat = zone->zone_pgdat;
page_alloc.c:	pgdat->nr_zones = zone_idx(zone) + 1;
page_alloc.c:	zone->zone_start_pfn = zone_start_pfn;
page_alloc.c:			"Initialising map node %d zone %lu pfns %lu -> %lu\n",
page_alloc.c:			pgdat->node_id,
page_alloc.c:	zone->initialized = 1;
page_alloc.c:	if (state->last_start <= pfn && pfn < state->last_end)
page_alloc.c:		return state->last_nid;
page_alloc.c:	if (nid != -1) {
page_alloc.c:		state->last_start = start_pfn;
page_alloc.c:		state->last_end = end_pfn;
page_alloc.c:		state->last_nid = nid;
page_alloc.c: * free_bootmem_with_active_regions - Call memblock_free_early_nid for each active range
page_alloc.c:					(end_pfn - start_pfn) << PAGE_SHIFT,
page_alloc.c: * sparse_memory_present_with_active_regions - Call memory_present for each active range
page_alloc.c: * get_pfn_range_for_nid - Return the start and end page frames for a node
page_alloc.c:	*start_pfn = -1UL;
page_alloc.c:	if (*start_pfn == -1UL)
page_alloc.c:	for (zone_index = MAX_NR_ZONES - 1; zone_index >= 0; zone_index--) {
page_alloc.c:	VM_BUG_ON(zone_index == -1);
page_alloc.c: * present_pages = zone_spanned_pages_in_node() - zone_absent_pages_in_node()
page_alloc.c:	return *zone_end_pfn - *zone_start_pfn;
page_alloc.c:	unsigned long nr_absent = range_end_pfn - range_start_pfn;
page_alloc.c:		nr_absent -= end_pfn - start_pfn;
page_alloc.c: * absent_pages_in_range - Return number of page frames in holes within a range
page_alloc.c:				nr_absent += end_pfn - start_pfn;
page_alloc.c:				nr_absent += end_pfn - start_pfn;
page_alloc.c:		struct zone *zone = pgdat->node_zones + i;
page_alloc.c:		size = zone_spanned_pages_in_node(pgdat->node_id, i,
page_alloc.c:		real_size = size - zone_absent_pages_in_node(pgdat->node_id, i,
page_alloc.c:			zone->zone_start_pfn = zone_start_pfn;
page_alloc.c:			zone->zone_start_pfn = 0;
page_alloc.c:		zone->spanned_pages = size;
page_alloc.c:		zone->present_pages = real_size;
page_alloc.c:	pgdat->node_spanned_pages = totalpages;
page_alloc.c:	pgdat->node_present_pages = realtotalpages;
page_alloc.c:	printk(KERN_DEBUG "On node %d totalpages: %lu\n", pgdat->node_id,
page_alloc.c: * Calculate the size of the zone->blockflags rounded to an unsigned long
page_alloc.c:	zonesize += zone_start_pfn & (pageblock_nr_pages-1);
page_alloc.c:	zone->pageblock_flags = NULL;
page_alloc.c:		zone->pageblock_flags =
page_alloc.c:							 pgdat->node_id);
page_alloc.c:		order = MAX_ORDER - 1;
page_alloc.c: * is unused as pageblock_order is set at compile-time. See
page_alloc.c: * include/linux/pageblock-flags.h for the values of pageblock_order based on
page_alloc.c: *   - mark all pages reserved
page_alloc.c: *   - mark all memory queues empty
page_alloc.c: *   - clear the memory bitmaps
page_alloc.c:	int nid = pgdat->node_id;
page_alloc.c:	spin_lock_init(&pgdat->numabalancing_migrate_lock);
page_alloc.c:	pgdat->numabalancing_migrate_nr_pages = 0;
page_alloc.c:	pgdat->numabalancing_migrate_next_window = jiffies;
page_alloc.c:	spin_lock_init(&pgdat->split_queue_lock);
page_alloc.c:	INIT_LIST_HEAD(&pgdat->split_queue);
page_alloc.c:	pgdat->split_queue_len = 0;
page_alloc.c:	init_waitqueue_head(&pgdat->kswapd_wait);
page_alloc.c:	init_waitqueue_head(&pgdat->pfmemalloc_wait);
page_alloc.c:	init_waitqueue_head(&pgdat->kcompactd_wait);
page_alloc.c:	spin_lock_init(&pgdat->lru_lock);
page_alloc.c:	pgdat->per_cpu_nodestats = &boot_nodestats;
page_alloc.c:		struct zone *zone = pgdat->node_zones + j;
page_alloc.c:		unsigned long zone_start_pfn = zone->zone_start_pfn;
page_alloc.c:		size = zone->spanned_pages;
page_alloc.c:		realsize = freesize = zone->present_pages;
page_alloc.c:		 * and per-cpu initialisations
page_alloc.c:				freesize -= memmap_pages;
page_alloc.c:			freesize -= dma_reserve;
page_alloc.c:			nr_kernel_pages -= memmap_pages;
page_alloc.c:		zone->managed_pages = is_highmem_idx(j) ? realsize : freesize;
page_alloc.c:		zone->node = nid;
page_alloc.c:		zone->name = zone_names[j];
page_alloc.c:		zone->zone_pgdat = pgdat;
page_alloc.c:		spin_lock_init(&zone->lock);
page_alloc.c:	if (!pgdat->node_spanned_pages)
page_alloc.c:	start = pgdat->node_start_pfn & ~(MAX_ORDER_NR_PAGES - 1);
page_alloc.c:	offset = pgdat->node_start_pfn - start;
page_alloc.c:	if (!pgdat->node_mem_map) {
page_alloc.c:		size =  (end - start) * sizeof(struct page);
page_alloc.c:		map = alloc_remap(pgdat->node_id, size);
page_alloc.c:							       pgdat->node_id);
page_alloc.c:		pgdat->node_mem_map = map + offset;
page_alloc.c:		mem_map = NODE_DATA(0)->node_mem_map;
page_alloc.c:		if (page_to_pfn(mem_map) != pgdat->node_start_pfn)
page_alloc.c:			mem_map -= offset;
page_alloc.c:	WARN_ON(pgdat->nr_zones || pgdat->kswapd_classzone_idx);
page_alloc.c:	pgdat->node_id = nid;
page_alloc.c:	pgdat->node_start_pfn = node_start_pfn;
page_alloc.c:	pgdat->per_cpu_nodestats = NULL;
page_alloc.c:	pr_info("Initmem setup node %d [mem %#018Lx-%#018Lx]\n", nid,
page_alloc.c:		end_pfn ? ((u64)end_pfn << PAGE_SHIFT) - 1 : 0);
page_alloc.c:		(unsigned long)pgdat->node_mem_map);
page_alloc.c: * node_map_pfn_alignment - determine the maximum internode alignment
page_alloc.c: * would indicate 1GiB alignment with (1 << (30 - PAGE_SHIFT)).  If the
page_alloc.c: * This is used to test whether pfn -> nid mapping of the chosen memory
page_alloc.c:	int last_nid = -1;
page_alloc.c:		 * Start with a mask granular enough to pin-point to the
page_alloc.c:		 * start pfn and tick off bits one-by-one until it becomes
page_alloc.c:		mask = ~((1 << __ffs(start)) - 1);
page_alloc.c: * find_min_pfn_with_active_regions - Find the minimum PFN registered
page_alloc.c:		unsigned long pages = end_pfn - start_pfn;
page_alloc.c:			nid = r->nid;
page_alloc.c:			usable_startpfn = PFN_DOWN(r->base);
page_alloc.c:			nid = r->nid;
page_alloc.c:		 * Round-up so that ZONE_MOVABLE is at least as large as what
page_alloc.c:		corepages = totalpages - required_movablecore;
page_alloc.c:								- start_pfn;
page_alloc.c:				kernelcore_remaining -= min(kernel_pages,
page_alloc.c:				required_kernelcore -= min(kernel_pages,
page_alloc.c:			 * start_pfn->end_pfn. Calculate size_pages as the
page_alloc.c:			size_pages = end_pfn - start_pfn;
page_alloc.c:			required_kernelcore -= min(required_kernelcore,
page_alloc.c:			kernelcore_remaining -= size_pages;
page_alloc.c:	usable_nodes--;
page_alloc.c:	for (zone_type = 0; zone_type <= ZONE_MOVABLE - 1; zone_type++) {
page_alloc.c:		struct zone *zone = &pgdat->node_zones[zone_type];
page_alloc.c: * free_area_init_nodes - Initialise all pg_data_t and zone data
page_alloc.c:		pr_info("  %-8s ", zone_names[i]);
page_alloc.c:			pr_cont("[mem %#018Lx-%#018Lx]\n",
page_alloc.c:					<< PAGE_SHIFT) - 1);
page_alloc.c:		pr_info("  node %3d: [mem %#018Lx-%#018Lx]\n", nid,
page_alloc.c:			((u64)end_pfn << PAGE_SHIFT) - 1);
page_alloc.c:		if (pgdat->node_present_pages)
page_alloc.c:		return -EINVAL;
page_alloc.c:	page_zone(page)->managed_pages += count;
page_alloc.c:			s, pages << (PAGE_SHIFT - 10));
page_alloc.c:	page_zone(page)->managed_pages++;
page_alloc.c:	codesize = _etext - _stext;
page_alloc.c:	datasize = _edata - _sdata;
page_alloc.c:	rosize = __end_rodata - __start_rodata;
page_alloc.c:	bss_size = __bss_stop - __bss_start;
page_alloc.c:	init_data_size = __init_end - __init_begin;
page_alloc.c:	init_code_size = _einittext - _sinittext;
page_alloc.c:			size -= adj; \
page_alloc.c:	pr_info("Memory: %luK/%luK available (%luK kernel code, %luK rwdata, %luK rodata, %luK init, %luK bss, %luK reserved, %luK cma-reserved"
page_alloc.c:		nr_free_pages() << (PAGE_SHIFT - 10),
page_alloc.c:		physpages << (PAGE_SHIFT - 10),
page_alloc.c:		(physpages - totalram_pages - totalcma_pages) << (PAGE_SHIFT - 10),
page_alloc.c:		totalcma_pages << (PAGE_SHIFT - 10),
page_alloc.c:		totalhigh_pages << (PAGE_SHIFT - 10),
page_alloc.c: * set_dma_reserve - set the specified number of pages reserved in the first zone
page_alloc.c: * The per-cpu batchsize and zone watermarks are determined by managed_pages.
page_alloc.c: * smaller per-cpu batchsize.
page_alloc.c: * calculate_totalreserve_pages - called when sysctl_lowmem_reserve_ratio
page_alloc.c:		pgdat->totalreserve_pages = 0;
page_alloc.c:			struct zone *zone = pgdat->node_zones + i;
page_alloc.c:				if (zone->lowmem_reserve[j] > max)
page_alloc.c:					max = zone->lowmem_reserve[j];
page_alloc.c:			if (max > zone->managed_pages)
page_alloc.c:				max = zone->managed_pages;
page_alloc.c:			pgdat->totalreserve_pages += max;
page_alloc.c: * setup_per_zone_lowmem_reserve - called whenever
page_alloc.c:			struct zone *zone = pgdat->node_zones + j;
page_alloc.c:			unsigned long managed_pages = zone->managed_pages;
page_alloc.c:			zone->lowmem_reserve[j] = 0;
page_alloc.c:				idx--;
page_alloc.c:				lower_zone = pgdat->node_zones + idx;
page_alloc.c:				lower_zone->lowmem_reserve[j] = managed_pages /
page_alloc.c:				managed_pages += lower_zone->managed_pages;
page_alloc.c:	unsigned long pages_min = min_free_kbytes >> (PAGE_SHIFT - 10);
page_alloc.c:			lowmem_pages += zone->managed_pages;
page_alloc.c:		spin_lock_irqsave(&zone->lock, flags);
page_alloc.c:		tmp = (u64)pages_min * zone->managed_pages;
page_alloc.c:			 * The WMARK_HIGH-WMARK_LOW and (WMARK_LOW-WMARK_MIN)
page_alloc.c:			min_pages = zone->managed_pages / 1024;
page_alloc.c:			zone->watermark[WMARK_MIN] = min_pages;
page_alloc.c:			zone->watermark[WMARK_MIN] = tmp;
page_alloc.c:			    mult_frac(zone->managed_pages,
page_alloc.c:		zone->watermark[WMARK_LOW]  = min_wmark_pages(zone) + tmp;
page_alloc.c:		zone->watermark[WMARK_HIGH] = min_wmark_pages(zone) + tmp * 2;
page_alloc.c:		spin_unlock_irqrestore(&zone->lock, flags);
page_alloc.c: * setup_per_zone_wmarks - called when min_free_kbytes changes
page_alloc.c: * or when memory is hot-{added|removed}
page_alloc.c: * min_free_kbytes_sysctl_handler - just a wrapper around proc_dointvec() so
page_alloc.c:		pgdat->min_unmapped_pages = 0;
page_alloc.c:		zone->zone_pgdat->min_unmapped_pages += (zone->managed_pages *
page_alloc.c:		pgdat->min_slab_pages = 0;
page_alloc.c:		zone->zone_pgdat->min_slab_pages += (zone->managed_pages *
page_alloc.c: * lowmem_reserve_ratio_sysctl_handler - just a wrapper around
page_alloc.c: * percpu_pagelist_fraction - changes the pcp->high for each zone on each
page_alloc.c:		ret = -EINVAL;
page_alloc.c:					per_cpu_ptr(zone->pageset, cpu));
page_alloc.c: * Because 32-bit systems cannot have large physical memory, where this scaling
page_alloc.c: * - it is assumed that the hash table must contain an exact power-of-2
page_alloc.c: * - limit is the number of hash buckets, not the total allocation size
page_alloc.c:		numentries -= arch_reserved_kernel_pages();
page_alloc.c:			numentries >>= (scale - PAGE_SHIFT);
page_alloc.c:			numentries <<= (PAGE_SHIFT - scale);
page_alloc.c:		/* Make sure we've got at least a 0-order allocation.. */
page_alloc.c:			 * If bucketsize is not a power-of-two, we may free
page_alloc.c:	} while (!table && size > PAGE_SIZE && --log2qty);
page_alloc.c:		tablename, 1UL << log2qty, ilog2(size) - PAGE_SHIFT, size);
page_alloc.c:		*_hash_mask = (1 << log2qty) - 1;
page_alloc.c: * check without lock_page also may miss some movable non-lru pages at
page_alloc.c:			iter = round_up(iter + 1, 1<<compound_order(page)) - 1;
page_alloc.c:		 * because their page->_refcount is zero at all time.
page_alloc.c:				iter += (1 << page_order(page)) - 1;
page_alloc.c:		 * we don't need more check. This is an _used_ not-movable page.
page_alloc.c:	 * its NODE_DATA will be NULL - see page_zone.
page_alloc.c:			     pageblock_nr_pages) - 1);
page_alloc.c:	while (pfn < end || !list_empty(&cc->migratepages)) {
page_alloc.c:			ret = -EINTR;
page_alloc.c:		if (list_empty(&cc->migratepages)) {
page_alloc.c:			cc->nr_migratepages = 0;
page_alloc.c:				ret = -EINTR;
page_alloc.c:			ret = ret < 0 ? ret : -EBUSY;
page_alloc.c:		nr_reclaimed = reclaim_clean_pages_from_list(cc->zone,
page_alloc.c:							&cc->migratepages);
page_alloc.c:		cc->nr_migratepages -= nr_reclaimed;
page_alloc.c:		ret = migrate_pages(&cc->migratepages, alloc_migrate_target,
page_alloc.c:				    NULL, 0, cc->mode, MR_CMA);
page_alloc.c:		putback_movable_pages(&cc->migratepages);
page_alloc.c: * alloc_contig_range() -- tries to allocate given range of pages
page_alloc.c: * @end:	one-past-the-last PFN to allocate
page_alloc.c:		.order = -1,
page_alloc.c:	 * In case of -EBUSY, we'd like to know which page causes problem.
page_alloc.c:	 * -EBUSY is not accidentally used or returned to caller.
page_alloc.c:	if (ret && ret != -EBUSY)
page_alloc.c:	 * We don't have to hold zone->lock here because the pages are
page_alloc.c:		ret = -EBUSY;
page_alloc.c:		ret = -EBUSY;
page_alloc.c:		free_contig_range(outer_start, start - outer_start);
page_alloc.c:		free_contig_range(end, outer_end - end);
page_alloc.c:	for (; nr_pages--; pfn++) {
page_alloc.c:				per_cpu_ptr(zone->pageset, cpu));
page_alloc.c:	if (zone->pageset != &boot_pageset) {
page_alloc.c:			pset = per_cpu_ptr(zone->pageset, cpu);
page_alloc.c:		free_percpu(zone->pageset);
page_alloc.c:		zone->pageset = &boot_pageset;
page_alloc.c:	spin_lock_irqsave(&zone->lock, flags);
page_alloc.c:		list_del(&page->lru);
page_alloc.c:		zone->free_area[order].nr_free--;
page_alloc.c:	spin_unlock_irqrestore(&zone->lock, flags);
page_alloc.c:	spin_lock_irqsave(&zone->lock, flags);
page_alloc.c:		struct page *page_head = page - (pfn & ((1 << order) - 1));
page_alloc.c:	spin_unlock_irqrestore(&zone->lock, flags);
slub.c:// SPDX-License-Identifier: GPL-2.0
slub.c:#include <linux/fault-inject.h>
slub.c: *   2. node->list_lock
slub.c: *	A. page->freelist	-> List of object free in a page
slub.c: *	B. page->counters	-> Counters of objects
slub.c: *	C. page->frozen		-> frozen state
slub.c:	return unlikely(s->flags & SLAB_DEBUG_FLAGS);
slub.c:	if (kmem_cache_debug(s) && s->flags & SLAB_RED_ZONE)
slub.c:		p += s->red_left_pad;
slub.c: * - Support PAGE_ALLOC_DEBUG. Should be easy to do.
slub.c: * - Variable sizing of the per node arrays
slub.c:#define OO_MASK		((1 << OO_SHIFT) - 1)
slub.c:	 * avoid this_cpu_add()'s irq-disable overhead.
slub.c:	raw_cpu_inc(s->cpu_slab->stat[si]);
slub.c: * with an XOR of the address where the pointer is held and a per-cache
slub.c:	return (void *)((unsigned long)ptr ^ s->random ^ ptr_addr);
slub.c:	return freelist_dereference(s, object + s->offset);
slub.c:		prefetch(freelist_dereference(s, object + s->offset));
slub.c:	freepointer_addr = (unsigned long)object + s->offset;
slub.c:	unsigned long freeptr_addr = (unsigned long)object + s->offset;
slub.c:		__p < (__addr) + (__objects) * (__s)->size; \
slub.c:		__p += (__s)->size)
slub.c:		__p += (__s)->size, __idx++)
slub.c:	return (p - addr) / s->size;
slub.c:	return ((PAGE_SIZE << order) - reserved) / size;
slub.c:	bit_spin_lock(PG_locked, &page->flags);
slub.c:	__bit_spin_unlock(PG_locked, &page->flags);
slub.c:	 * page->counters can cover frozen/inuse/objects as well
slub.c:	 * as page->_refcount.  If we assign to ->counters directly
slub.c:	 * we run the risk of losing updates to page->_refcount, so
slub.c:	page->frozen  = tmp.frozen;
slub.c:	page->inuse   = tmp.inuse;
slub.c:	page->objects = tmp.objects;
slub.c:	if (s->flags & __CMPXCHG_DOUBLE) {
slub.c:		if (cmpxchg_double(&page->freelist, &page->counters,
slub.c:		if (page->freelist == freelist_old &&
slub.c:					page->counters == counters_old) {
slub.c:			page->freelist = freelist_new;
slub.c:	pr_info("%s %s: cmpxchg double redo ", n, s->name);
slub.c:	if (s->flags & __CMPXCHG_DOUBLE) {
slub.c:		if (cmpxchg_double(&page->freelist, &page->counters,
slub.c:		if (page->freelist == freelist_old &&
slub.c:					page->counters == counters_old) {
slub.c:			page->freelist = freelist_new;
slub.c:	pr_info("%s %s: cmpxchg double redo ", n, s->name);
slub.c:	for (p = page->freelist; p; p = get_freepointer(s, p))
slub.c:	if (s->flags & SLAB_RED_ZONE)
slub.c:		return s->size - s->red_left_pad;
slub.c:	return s->size;
slub.c:	if (s->flags & SLAB_RED_ZONE)
slub.c:		p -= s->red_left_pad;
slub.c:	if (object < base || object >= base + page->objects * s->size ||
slub.c:		(object - base) % s->size) {
slub.c:	if (s->offset)
slub.c:		p = object + s->offset + sizeof(void *);
slub.c:		p = object + s->inuse;
slub.c:		trace.entries = p->addrs;
slub.c:		    trace.entries[trace.nr_entries - 1] == ULONG_MAX)
slub.c:			trace.nr_entries--;
slub.c:			p->addrs[i] = 0;
slub.c:		p->addr = addr;
slub.c:		p->cpu = smp_processor_id();
slub.c:		p->pid = current->pid;
slub.c:		p->when = jiffies;
slub.c:	if (!(s->flags & SLAB_STORE_USER))
slub.c:	if (!t->addr)
slub.c:	       s, (void *)t->addr, jiffies - t->when, t->cpu, t->pid);
slub.c:			if (t->addrs[i])
slub.c:				pr_err("\t%pS\n", (void *)t->addrs[i]);
slub.c:	if (!(s->flags & SLAB_STORE_USER))
slub.c:	       page, page->objects, page->inuse, page->freelist, page->flags);
slub.c:	pr_err("BUG %s (%s): %pV\n", s->name, print_tainted(), &vaf);
slub.c:	pr_err("-----------------------------------------------------------------------------\n\n");
slub.c:	pr_err("FIX %s: %pV\n", s->name, &vaf);
slub.c:	       p, p - addr, get_freepointer(s, p));
slub.c:	if (s->flags & SLAB_RED_ZONE)
slub.c:		print_section(KERN_ERR, "Redzone ", p - s->red_left_pad,
slub.c:			      s->red_left_pad);
slub.c:		print_section(KERN_ERR, "Bytes b4 ", p - 16, 16);
slub.c:		      min_t(unsigned long, s->object_size, PAGE_SIZE));
slub.c:	if (s->flags & SLAB_RED_ZONE)
slub.c:		print_section(KERN_ERR, "Redzone ", p + s->object_size,
slub.c:			s->inuse - s->object_size);
slub.c:	if (s->offset)
slub.c:		off = s->offset + sizeof(void *);
slub.c:		off = s->inuse;
slub.c:	if (s->flags & SLAB_STORE_USER)
slub.c:			      size_from_object(s) - off);
slub.c:	if (s->flags & SLAB_RED_ZONE)
slub.c:		memset(p - s->red_left_pad, val, s->red_left_pad);
slub.c:	if (s->flags & __OBJECT_POISON) {
slub.c:		memset(p, POISON_FREE, s->object_size - 1);
slub.c:		p[s->object_size - 1] = POISON_END;
slub.c:	if (s->flags & SLAB_RED_ZONE)
slub.c:		memset(p + s->object_size, val, s->inuse - s->object_size);
slub.c:	slab_fix(s, "Restoring 0x%p-0x%p=0x%x\n", from, to - 1, data);
slub.c:	memset(from, data, to - from);
slub.c:	while (end > fault && end[-1] == value)
slub.c:		end--;
slub.c:	pr_err("INFO: 0x%p-0x%p. First byte 0x%x instead of 0x%x\n",
slub.c:					fault, end - 1, fault[0], value);
slub.c: * object + s->object_size
slub.c: * object + s->inuse
slub.c: * object + s->size
slub.c: * 	Nothing is used beyond s->size.
slub.c:	unsigned long off = s->inuse;	/* The end of info */
slub.c:	if (s->offset)
slub.c:	if (s->flags & SLAB_STORE_USER)
slub.c:			p + off, POISON_INUSE, size_from_object(s) - off);
slub.c:	if (!(s->flags & SLAB_POISON))
slub.c:	length = (PAGE_SIZE << compound_order(page)) - s->reserved;
slub.c:	remainder = length % s->size;
slub.c:	fault = memchr_inv(end - remainder, POISON_INUSE, remainder);
slub.c:	while (end > fault && end[-1] == POISON_INUSE)
slub.c:		end--;
slub.c:	slab_err(s, page, "Padding overwritten. 0x%p-0x%p", fault, end - 1);
slub.c:	print_section(KERN_ERR, "Padding ", end - remainder, remainder);
slub.c:	restore_bytes(s, "slab padding", POISON_INUSE, end - remainder, end);
slub.c:	u8 *endobject = object + s->object_size;
slub.c:	if (s->flags & SLAB_RED_ZONE) {
slub.c:			object - s->red_left_pad, val, s->red_left_pad))
slub.c:			endobject, val, s->inuse - s->object_size))
slub.c:		if ((s->flags & SLAB_POISON) && s->object_size < s->inuse) {
slub.c:				s->inuse - s->object_size);
slub.c:	if (s->flags & SLAB_POISON) {
slub.c:		if (val != SLUB_RED_ACTIVE && (s->flags & __OBJECT_POISON) &&
slub.c:					POISON_FREE, s->object_size - 1) ||
slub.c:				p + s->object_size - 1, POISON_END, 1)))
slub.c:	if (!s->offset && val == SLUB_RED_ACTIVE)
slub.c:	maxobj = order_objects(compound_order(page), s->size, s->reserved);
slub.c:	if (page->objects > maxobj) {
slub.c:			page->objects, maxobj);
slub.c:	if (page->inuse > page->objects) {
slub.c:			page->inuse, page->objects);
slub.c:	fp = page->freelist;
slub.c:	while (fp && nr <= page->objects) {
slub.c:				page->freelist = NULL;
slub.c:				page->inuse = page->objects;
slub.c:	max_objects = order_objects(compound_order(page), s->size, s->reserved);
slub.c:	if (page->objects != max_objects) {
slub.c:			 page->objects, max_objects);
slub.c:		page->objects = max_objects;
slub.c:	if (page->inuse != page->objects - nr) {
slub.c:			 page->inuse, page->objects - nr);
slub.c:		page->inuse = page->objects - nr;
slub.c:	if (s->flags & SLAB_TRACE) {
slub.c:			s->name,
slub.c:			object, page->inuse,
slub.c:			page->freelist);
slub.c:					s->object_size);
slub.c:	if (!(s->flags & SLAB_STORE_USER))
slub.c:	lockdep_assert_held(&n->list_lock);
slub.c:	list_add(&page->lru, &n->full);
slub.c:	if (!(s->flags & SLAB_STORE_USER))
slub.c:	lockdep_assert_held(&n->list_lock);
slub.c:	list_del(&page->lru);
slub.c:	return atomic_long_read(&n->nr_slabs);
slub.c:	return atomic_long_read(&n->nr_slabs);
slub.c:	 * kmem_cache_node structure. Solve the chicken-egg
slub.c:		atomic_long_inc(&n->nr_slabs);
slub.c:		atomic_long_add(objects, &n->total_objects);
slub.c:	atomic_long_dec(&n->nr_slabs);
slub.c:	atomic_long_sub(objects, &n->total_objects);
slub.c:	if (!(s->flags & (SLAB_STORE_USER|SLAB_RED_ZONE|__OBJECT_POISON)))
slub.c:	if (s->flags & SLAB_CONSISTENCY_CHECKS) {
slub.c:	if (s->flags & SLAB_STORE_USER)
slub.c:		page->inuse = page->objects;
slub.c:		page->freelist = NULL;
slub.c:	if (unlikely(s != page->slab_cache)) {
slub.c:		} else if (!page->slab_cache) {
slub.c:	spin_lock_irqsave(&n->list_lock, flags);
slub.c:	if (s->flags & SLAB_CONSISTENCY_CHECKS) {
slub.c:	if (s->flags & SLAB_CONSISTENCY_CHECKS) {
slub.c:	if (s->flags & SLAB_STORE_USER)
slub.c:	spin_unlock_irqrestore(&n->list_lock, flags);
slub.c:	if (*str == '-')
slub.c:	kmemleak_free_recursive(x, s->flags);
slub.c:		debug_check_no_locks_freed(x, s->object_size);
slub.c:	if (!(s->flags & SLAB_DEBUG_OBJECTS))
slub.c:		debug_check_no_obj_freed(x, s->object_size);
slub.c:	if (unlikely(s->ctor)) {
slub.c:		s->ctor(object);
slub.c:/* Pre-initialize the random sequence cache */
slub.c:	unsigned long i, count = oo_objects(s->oo);
slub.c:	if (s->random_seq)
slub.c:			s->name);
slub.c:	if (s->random_seq) {
slub.c:			s->random_seq[i] *= s->size;
slub.c:/* Get the next entry on the pre-computed freelist randomized */
slub.c:		idx = s->random_seq[*pos];
slub.c:/* Shuffle the single linked freelist based on a random pre-computed sequence */
slub.c:	if (page->objects < 2 || !s->random_seq)
slub.c:	freelist_count = oo_objects(s->oo);
slub.c:	page_limit = page->objects * s->size;
slub.c:	page->freelist = cur;
slub.c:	for (idx = 1; idx < page->objects; idx++) {
slub.c:	struct kmem_cache_order_objects oo = s->oo;
slub.c:	flags |= s->allocflags;
slub.c:	 * Let the initial higher-order allocation fail under memory pressure
slub.c:	 * so we fall-back to the minimum order allocation.
slub.c:	if ((alloc_gfp & __GFP_DIRECT_RECLAIM) && oo_order(oo) > oo_order(s->min))
slub.c:		oo = s->min;
slub.c:	page->objects = oo_objects(oo);
slub.c:	page->slab_cache = s;
slub.c:	if (unlikely(s->flags & SLAB_POISON))
slub.c:		for_each_object_idx(p, idx, s, start, page->objects) {
slub.c:			if (likely(idx < page->objects))
slub.c:				set_freepointer(s, p, p + s->size);
slub.c:		page->freelist = fixup_red_left(s, start);
slub.c:	page->inuse = page->objects;
slub.c:	page->frozen = 1;
slub.c:		(s->flags & SLAB_RECLAIM_ACCOUNT) ?
slub.c:	inc_slabs_node(s, page_to_nid(page), page->objects);
slub.c:	if (s->flags & SLAB_CONSISTENCY_CHECKS) {
slub.c:						page->objects)
slub.c:		(s->flags & SLAB_RECLAIM_ACCOUNT) ?
slub.c:		-pages);
slub.c:	if (current->reclaim_state)
slub.c:		current->reclaim_state->reclaimed_slab += pages;
slub.c:	(sizeof(((struct page *)NULL)->lru) < sizeof(struct rcu_head))
slub.c:	__free_slab(page->slab_cache, page);
slub.c:	if (unlikely(s->flags & SLAB_TYPESAFE_BY_RCU)) {
slub.c:			int offset = (PAGE_SIZE << order) - s->reserved;
slub.c:			VM_BUG_ON(s->reserved != sizeof(*head));
slub.c:			head = &page->rcu_head;
slub.c:	dec_slabs_node(s, page_to_nid(page), page->objects);
slub.c:	n->nr_partial++;
slub.c:		list_add_tail(&page->lru, &n->partial);
slub.c:		list_add(&page->lru, &n->partial);
slub.c:	lockdep_assert_held(&n->list_lock);
slub.c:	lockdep_assert_held(&n->list_lock);
slub.c:	list_del(&page->lru);
slub.c:	n->nr_partial--;
slub.c:	lockdep_assert_held(&n->list_lock);
slub.c:	freelist = page->freelist;
slub.c:	counters = page->counters;
slub.c:	*objects = new.objects - new.inuse;
slub.c:		new.inuse = page->objects;
slub.c:	if (!n || !n->nr_partial)
slub.c:	spin_lock(&n->list_lock);
slub.c:	list_for_each_entry_safe(page, page2, &n->partial, lru) {
slub.c:			c->page = page;
slub.c:	spin_unlock(&n->list_lock);
slub.c:	if (!s->remote_node_defrag_ratio ||
slub.c:			get_cycles() % 1024 > s->remote_node_defrag_ratio)
slub.c:					n->nr_partial > s->min_partial) {
slub.c:					 * here - if mems_allowed was updated in
slub.c:	unsigned long actual_tid = __this_cpu_read(s->cpu_slab->tid);
slub.c:	pr_info("%s %s: cmpxchg redo ", n, s->name);
slub.c:		pr_warn("due to cpu change %d -> %d\n",
slub.c:		pr_warn("due to cpu running other code. Event %ld->%ld\n",
slub.c:		per_cpu_ptr(s->cpu_slab, cpu)->tid = init_tid(cpu);
slub.c:	if (page->freelist) {
slub.c:	 * There is no need to take the list->lock because the page
slub.c:			prior = page->freelist;
slub.c:			counters = page->counters;
slub.c:			new.inuse--;
slub.c:	old.freelist = page->freelist;
slub.c:	old.counters = page->counters;
slub.c:		new.inuse--;
slub.c:	if (!new.inuse && n->nr_partial >= s->min_partial)
slub.c:			spin_lock(&n->list_lock);
slub.c:			spin_lock(&n->list_lock);
slub.c:		spin_unlock(&n->list_lock);
slub.c:	c->page = NULL;
slub.c:	c->freelist = NULL;
slub.c:	while ((page = c->partial)) {
slub.c:		c->partial = page->next;
slub.c:				spin_unlock(&n->list_lock);
slub.c:			spin_lock(&n->list_lock);
slub.c:			old.freelist = page->freelist;
slub.c:			old.counters = page->counters;
slub.c:		if (unlikely(!new.inuse && n->nr_partial >= s->min_partial)) {
slub.c:			page->next = discard_page;
slub.c:		spin_unlock(&n->list_lock);
slub.c:		discard_page = discard_page->next;
slub.c:		oldpage = this_cpu_read(s->cpu_slab->partial);
slub.c:			pobjects = oldpage->pobjects;
slub.c:			pages = oldpage->pages;
slub.c:			if (drain && pobjects > s->cpu_partial) {
slub.c:				unfreeze_partials(s, this_cpu_ptr(s->cpu_slab));
slub.c:		pobjects += page->objects - page->inuse;
slub.c:		page->pages = pages;
slub.c:		page->pobjects = pobjects;
slub.c:		page->next = oldpage;
slub.c:	} while (this_cpu_cmpxchg(s->cpu_slab->partial, oldpage, page)
slub.c:	if (unlikely(!s->cpu_partial)) {
slub.c:		unfreeze_partials(s, this_cpu_ptr(s->cpu_slab));
slub.c:	deactivate_slab(s, c->page, c->freelist, c);
slub.c:	c->tid = next_tid(c->tid);
slub.c:	struct kmem_cache_cpu *c = per_cpu_ptr(s->cpu_slab, cpu);
slub.c:		if (c->page)
slub.c:	struct kmem_cache_cpu *c = per_cpu_ptr(s->cpu_slab, cpu);
slub.c:	return c->page || slub_percpu_partial(c);
slub.c:	return page->objects - page->inuse;
slub.c:	return atomic_long_read(&n->total_objects);
slub.c:	spin_lock_irqsave(&n->list_lock, flags);
slub.c:	list_for_each_entry(page, &n->partial, lru)
slub.c:	spin_unlock_irqrestore(&n->list_lock, flags);
slub.c:		s->name, s->object_size, s->size, oo_order(s->oo),
slub.c:		oo_order(s->min));
slub.c:	if (oo_order(s->min) > get_order(s->object_size))
slub.c:			s->name);
slub.c:		c = raw_cpu_ptr(s->cpu_slab);
slub.c:		if (c->page)
slub.c:		freelist = page->freelist;
slub.c:		page->freelist = NULL;
slub.c:		c->page = page;
slub.c: * Check the page->freelist of a page and either transfer the freelist to the
slub.c:		freelist = page->freelist;
slub.c:		counters = page->counters;
slub.c:		new.inuse = page->objects;
slub.c:	page = c->page;
slub.c:			deactivate_slab(s, page, c->freelist, c);
slub.c:	 * information when the page leaves the per-cpu allocator
slub.c:		deactivate_slab(s, page, c->freelist, c);
slub.c:	/* must check again c->freelist in case of cpu migration or IRQ */
slub.c:	freelist = c->freelist;
slub.c:		c->page = NULL;
slub.c:	VM_BUG_ON(!c->page->frozen);
slub.c:	c->freelist = get_freepointer(s, freelist);
slub.c:	c->tid = next_tid(c->tid);
slub.c:		page = c->page = slub_percpu_partial(c);
slub.c:	page = c->page;
slub.c:	c = this_cpu_ptr(s->cpu_slab);
slub.c:		tid = this_cpu_read(s->cpu_slab->tid);
slub.c:		c = raw_cpu_ptr(s->cpu_slab);
slub.c:		 unlikely(tid != READ_ONCE(c->tid)));
slub.c:	object = c->freelist;
slub.c:	page = c->page;
slub.c:				s->cpu_slab->freelist, s->cpu_slab->tid,
slub.c:		memset(object, 0, s->object_size);
slub.c:	trace_kmem_cache_alloc(_RET_IP_, ret, s->object_size,
slub.c:				s->size, gfpflags);
slub.c:	trace_kmalloc(_RET_IP_, ret, size, s->size, gfpflags);
slub.c:				    s->object_size, s->size, gfpflags, node);
slub.c:			   size, s->size, gfpflags, node);
slub.c:			spin_unlock_irqrestore(&n->list_lock, flags);
slub.c:		prior = page->freelist;
slub.c:		counters = page->counters;
slub.c:		new.inuse -= cnt;
slub.c:				spin_lock_irqsave(&n->list_lock, flags);
slub.c:	if (unlikely(!new.inuse && n->nr_partial >= s->min_partial))
slub.c:	spin_unlock_irqrestore(&n->list_lock, flags);
slub.c:	spin_unlock_irqrestore(&n->list_lock, flags);
slub.c:		tid = this_cpu_read(s->cpu_slab->tid);
slub.c:		c = raw_cpu_ptr(s->cpu_slab);
slub.c:		 unlikely(tid != READ_ONCE(c->tid)));
slub.c:	if (likely(page == c->page)) {
slub.c:		set_freepointer(s, tail_obj, c->freelist);
slub.c:				s->cpu_slab->freelist, s->cpu_slab->tid,
slub.c:				c->freelist, tid,
slub.c:	if (s->flags & SLAB_KASAN && !(s->flags & SLAB_TYPESAFE_BY_RCU))
slub.c:	/* Always re-init detached_freelist */
slub.c:	df->page = NULL;
slub.c:		object = p[--size];
slub.c:		df->s = page->slab_cache;
slub.c:		df->s = cache_from_obj(s, object); /* Support for memcg */
slub.c:	df->page = page;
slub.c:	set_freepointer(df->s, object, NULL);
slub.c:	df->tail = object;
slub.c:	df->freelist = object;
slub.c:	df->cnt = 1;
slub.c:		object = p[--size];
slub.c:		/* df->page is always set at this point */
slub.c:		if (df->page == virt_to_head_page(object)) {
slub.c:			set_freepointer(df->s, object, df->freelist);
slub.c:			df->freelist = object;
slub.c:			df->cnt++;
slub.c:		if (!--lookahead)
slub.c:	c = this_cpu_ptr(s->cpu_slab);
slub.c:		void *object = c->freelist;
slub.c:			 * Invoking slow path likely have side-effect
slub.c:			 * of re-populating per CPU c->freelist
slub.c:			c = this_cpu_ptr(s->cpu_slab);
slub.c:			continue; /* goto for-loop */
slub.c:		c->freelist = get_freepointer(s, object);
slub.c:	c->tid = next_tid(c->tid);
slub.c:			memset(p[j], 0, s->object_size);
slub.c:		return get_order(size * MAX_OBJS_PER_PAGE) - 1;
slub.c:		rem = (slab_size - reserved) % size;
slub.c:		min_objects--;
slub.c:	return -ENOSYS;
slub.c:	n->nr_partial = 0;
slub.c:	spin_lock_init(&n->list_lock);
slub.c:	INIT_LIST_HEAD(&n->partial);
slub.c:	atomic_long_set(&n->nr_slabs, 0);
slub.c:	atomic_long_set(&n->total_objects, 0);
slub.c:	INIT_LIST_HEAD(&n->full);
slub.c:	s->cpu_slab = __alloc_percpu(sizeof(struct kmem_cache_cpu),
slub.c:	if (!s->cpu_slab)
slub.c:	BUG_ON(kmem_cache_node->size < sizeof(struct kmem_cache_node));
slub.c:	n = page->freelist;
slub.c:	page->freelist = get_freepointer(kmem_cache_node, n);
slub.c:	page->inuse = 1;
slub.c:	page->frozen = 0;
slub.c:	kmem_cache_node->node[node] = n;
slub.c:	inc_slabs_node(kmem_cache_node, node, page->objects);
slub.c:		s->node[node] = NULL;
slub.c:	free_percpu(s->cpu_slab);
slub.c:		s->node[node] = n;
slub.c:	s->min_partial = min;
slub.c:		s->cpu_partial = 0;
slub.c:	else if (s->size >= PAGE_SIZE)
slub.c:		s->cpu_partial = 2;
slub.c:	else if (s->size >= 1024)
slub.c:		s->cpu_partial = 6;
slub.c:	else if (s->size >= 256)
slub.c:		s->cpu_partial = 13;
slub.c:		s->cpu_partial = 30;
slub.c:	unsigned long flags = s->flags;
slub.c:	size_t size = s->object_size;
slub.c:			!s->ctor)
slub.c:		s->flags |= __OBJECT_POISON;
slub.c:		s->flags &= ~__OBJECT_POISON;
slub.c:	if ((flags & SLAB_RED_ZONE) && size == s->object_size)
slub.c:	s->inuse = size;
slub.c:		s->ctor)) {
slub.c:		s->offset = size;
slub.c:	kasan_cache_create(s, &size, &s->flags);
slub.c:		s->red_left_pad = sizeof(void *);
slub.c:		s->red_left_pad = ALIGN(s->red_left_pad, s->align);
slub.c:		size += s->red_left_pad;
slub.c:	size = ALIGN(size, s->align);
slub.c:	s->size = size;
slub.c:		order = calculate_order(size, s->reserved);
slub.c:	s->allocflags = 0;
slub.c:		s->allocflags |= __GFP_COMP;
slub.c:	if (s->flags & SLAB_CACHE_DMA)
slub.c:		s->allocflags |= GFP_DMA;
slub.c:	if (s->flags & SLAB_RECLAIM_ACCOUNT)
slub.c:		s->allocflags |= __GFP_RECLAIMABLE;
slub.c:	s->oo = oo_make(order, size, s->reserved);
slub.c:	s->min = oo_make(get_order(size), size, s->reserved);
slub.c:	if (oo_objects(s->oo) > oo_objects(s->max))
slub.c:		s->max = s->oo;
slub.c:	return !!oo_objects(s->oo);
slub.c:	s->flags = kmem_cache_flags(s->size, flags, s->name, s->ctor);
slub.c:	s->reserved = 0;
slub.c:	s->random = get_random_long();
slub.c:	if (need_reserve_slab_rcu && (s->flags & SLAB_TYPESAFE_BY_RCU))
slub.c:		s->reserved = sizeof(struct rcu_head);
slub.c:	if (!calculate_sizes(s, -1))
slub.c:		if (get_order(s->size) > get_order(s->object_size)) {
slub.c:			s->flags &= ~DEBUG_METADATA_FLAGS;
slub.c:			s->offset = 0;
slub.c:			if (!calculate_sizes(s, -1))
slub.c:	if (system_has_cmpxchg_double() && (s->flags & SLAB_NO_CMPXCHG) == 0)
slub.c:		s->flags |= __CMPXCHG_DOUBLE;
slub.c:	set_min_partial(s, ilog2(s->size) / 2);
slub.c:	s->remote_node_defrag_ratio = 1000;
slub.c:	/* Initialize the pre-computed randomized freelist if slab is up */
slub.c:		      s->name, (unsigned long)s->size, s->size,
slub.c:		      oo_order(s->oo), s->offset, flags);
slub.c:	return -EINVAL;
slub.c:	unsigned long *map = kzalloc(BITS_TO_LONGS(page->objects) *
slub.c:	slab_err(s, page, text, s->name);
slub.c:	for_each_object(p, s, addr, page->objects) {
slub.c:			pr_err("INFO: Object 0x%p @offset=%tu\n", p, p - addr);
slub.c:	spin_lock_irq(&n->list_lock);
slub.c:	list_for_each_entry_safe(page, h, &n->partial, lru) {
slub.c:		if (!page->inuse) {
slub.c:			list_add(&page->lru, &discard);
slub.c:	spin_unlock_irq(&n->list_lock);
slub.c:		if (n->nr_partial || slabs_node(s, node))
slub.c:	slub_max_order = min(slub_max_order, MAX_ORDER - 1);
slub.c:	trace_kmalloc(_RET_IP_, ret, size, s->size, flags);
slub.c:	trace_kmalloc_node(_RET_IP_, ret, size, s->size, flags, node);
slub.c:	s = page->slab_cache;
slub.c:		return s->name;
slub.c:	offset = (ptr - page_address(page)) % s->size;
slub.c:	if (kmem_cache_debug(s) && s->flags & SLAB_RED_ZONE) {
slub.c:		if (offset < s->red_left_pad)
slub.c:			return s->name;
slub.c:		offset -= s->red_left_pad;
slub.c:	if (offset <= object_size && n <= object_size - offset)
slub.c:	return s->name;
slub.c:	return slab_ksize(page->slab_cache);
slub.c:	slab_free(page->slab_cache, page, object, NULL, 1, _RET_IP_);
slub.c:		spin_lock_irqsave(&n->list_lock, flags);
slub.c:		 * list_lock. page->inuse here is the upper limit.
slub.c:		list_for_each_entry_safe(page, t, &n->partial, lru) {
slub.c:			int free = page->objects - page->inuse;
slub.c:			/* Do not reread page->inuse */
slub.c:			if (free == page->objects) {
slub.c:				list_move(&page->lru, &discard);
slub.c:				n->nr_partial--;
slub.c:				list_move(&page->lru, promote + free - 1);
slub.c:		for (i = SHRINK_PROMOTE_MAX - 1; i >= 0; i--)
slub.c:			list_splice(promote + i, &n->partial);
slub.c:		spin_unlock_irqrestore(&n->list_lock, flags);
slub.c:	 * doesn't have allocations already in-flight and thus can't
slub.c:	s->min_partial = 0;
slub.c:	 * s->cpu_partial is checked locklessly (see put_cpu_partial), so
slub.c:	offline_node = marg->status_change_nid_normal;
slub.c:			 * if n->nr_slabs > 0, slabs still exist on the node
slub.c:			s->node[offline_node] = NULL;
slub.c:	int nid = marg->status_change_nid_normal;
slub.c:			ret = -ENOMEM;
slub.c:		s->node[nid] = n;
slub.c:	memcpy(s, static_cache, kmem_cache->object_size);
slub.c:		list_for_each_entry(p, &n->partial, lru)
slub.c:			p->slab_cache = s;
slub.c:		list_for_each_entry(p, &n->full, lru)
slub.c:			p->slab_cache = s;
slub.c:	list_add(&s->list, &slab_caches);
slub.c:	pr_info("SLUB: HWalign=%d, Order=%d-%d, MinObjects=%d, CPUs=%u, Nodes=%d\n",
slub.c:		s->refcount++;
slub.c:		s->object_size = max(s->object_size, (int)size);
slub.c:		s->inuse = max_t(int, s->inuse, ALIGN(size, sizeof(void *)));
slub.c:			c->object_size = s->object_size;
slub.c:			c->inuse = max_t(int, c->inuse,
slub.c:			s->refcount--;
slub.c:	trace_kmalloc(caller, ret, size, s->size, gfpflags);
slub.c:	trace_kmalloc_node(caller, ret, size, s->size, gfpflags, node);
slub.c:	return page->inuse;
slub.c:	return page->objects;
slub.c:	bitmap_zero(map, page->objects);
slub.c:	for_each_object(p, s, addr, page->objects) {
slub.c:	for_each_object(p, s, addr, page->objects)
slub.c:	spin_lock_irqsave(&n->list_lock, flags);
slub.c:	list_for_each_entry(page, &n->partial, lru) {
slub.c:	if (count != n->nr_partial)
slub.c:		       s->name, count, n->nr_partial);
slub.c:	if (!(s->flags & SLAB_STORE_USER))
slub.c:	list_for_each_entry(page, &n->full, lru) {
slub.c:	if (count != atomic_long_read(&n->nr_slabs))
slub.c:		       s->name, count, atomic_long_read(&n->nr_slabs));
slub.c:	spin_unlock_irqrestore(&n->list_lock, flags);
slub.c:	unsigned long *map = kmalloc(BITS_TO_LONGS(oo_objects(s->max)) *
slub.c:		return -ENOMEM;
slub.c:	if (t->max)
slub.c:		free_pages((unsigned long)t->loc,
slub.c:			get_order(sizeof(struct location) * t->max));
slub.c:	if (t->count) {
slub.c:		memcpy(l, t->loc, sizeof(struct location) * t->count);
slub.c:	t->max = max;
slub.c:	t->loc = l;
slub.c:	unsigned long age = jiffies - track->when;
slub.c:	start = -1;
slub.c:	end = t->count;
slub.c:		pos = start + (end - start + 1) / 2;
slub.c:		caddr = t->loc[pos].addr;
slub.c:		if (track->addr == caddr) {
slub.c:			l = &t->loc[pos];
slub.c:			l->count++;
slub.c:			if (track->when) {
slub.c:				l->sum_time += age;
slub.c:				if (age < l->min_time)
slub.c:					l->min_time = age;
slub.c:				if (age > l->max_time)
slub.c:					l->max_time = age;
slub.c:				if (track->pid < l->min_pid)
slub.c:					l->min_pid = track->pid;
slub.c:				if (track->pid > l->max_pid)
slub.c:					l->max_pid = track->pid;
slub.c:				cpumask_set_cpu(track->cpu,
slub.c:						to_cpumask(l->cpus));
slub.c:			node_set(page_to_nid(virt_to_page(track)), l->nodes);
slub.c:		if (track->addr < caddr)
slub.c:	if (t->count >= t->max && !alloc_loc_track(t, 2 * t->max, GFP_ATOMIC))
slub.c:	l = t->loc + pos;
slub.c:	if (pos < t->count)
slub.c:			(t->count - pos) * sizeof(struct location));
slub.c:	t->count++;
slub.c:	l->count = 1;
slub.c:	l->addr = track->addr;
slub.c:	l->sum_time = age;
slub.c:	l->min_time = age;
slub.c:	l->max_time = age;
slub.c:	l->min_pid = track->pid;
slub.c:	l->max_pid = track->pid;
slub.c:	cpumask_clear(to_cpumask(l->cpus));
slub.c:	cpumask_set_cpu(track->cpu, to_cpumask(l->cpus));
slub.c:	nodes_clear(l->nodes);
slub.c:	node_set(page_to_nid(virt_to_page(track)), l->nodes);
slub.c:	bitmap_zero(map, page->objects);
slub.c:	for_each_object(p, s, addr, page->objects)
slub.c:	unsigned long *map = kmalloc(BITS_TO_LONGS(oo_objects(s->max)) *
slub.c:		if (!atomic_long_read(&n->nr_slabs))
slub.c:		spin_lock_irqsave(&n->list_lock, flags);
slub.c:		list_for_each_entry(page, &n->partial, lru)
slub.c:		list_for_each_entry(page, &n->full, lru)
slub.c:		spin_unlock_irqrestore(&n->list_lock, flags);
slub.c:		if (len > PAGE_SIZE - KSYM_SYMBOL_LEN - 100)
slub.c:		len += sprintf(buf + len, "%7ld ", l->count);
slub.c:		if (l->addr)
slub.c:			len += sprintf(buf + len, "%pS", (void *)l->addr);
slub.c:			len += sprintf(buf + len, "<not-available>");
slub.c:		if (l->sum_time != l->min_time) {
slub.c:				l->min_time,
slub.c:				(long)div_u64(l->sum_time, l->count),
slub.c:				l->max_time);
slub.c:				l->min_time);
slub.c:		if (l->min_pid != l->max_pid)
slub.c:			len += sprintf(buf + len, " pid=%ld-%ld",
slub.c:				l->min_pid, l->max_pid);
slub.c:				l->min_pid);
slub.c:				!cpumask_empty(to_cpumask(l->cpus)) &&
slub.c:				len < PAGE_SIZE - 60)
slub.c:			len += scnprintf(buf + len, PAGE_SIZE - len - 50,
slub.c:					 cpumask_pr_args(to_cpumask(l->cpus)));
slub.c:		if (nr_online_nodes > 1 && !nodes_empty(l->nodes) &&
slub.c:				len < PAGE_SIZE - 60)
slub.c:			len += scnprintf(buf + len, PAGE_SIZE - len - 50,
slub.c:					 nodemask_pr_args(&l->nodes));
slub.c:	pr_err("-----------------------\n");
slub.c:	pr_err("\n1. kmalloc-16: Clobber Redzone/next pointer 0x12->0x%p\n\n",
slub.c:	pr_err("\n2. kmalloc-32: Clobber next pointer/next slab 0x34 -> -0x%p\n",
slub.c:	pr_err("\n3. kmalloc-64: corrupting random byte 0x56->0x%p\n",
slub.c:	pr_err("1. kmalloc-128: Clobber first word 0x78->0x%p\n\n", p);
slub.c:	pr_err("\n2. kmalloc-256: Clobber 50th byte 0x9a->0x%p\n\n", p);
slub.c:	pr_err("\n3. kmalloc-512: Clobber redzone 0xab->0x%p\n\n", p);
slub.c:		return -ENOMEM;
slub.c:			struct kmem_cache_cpu *c = per_cpu_ptr(s->cpu_slab,
slub.c:			page = READ_ONCE(c->page);
slub.c:				x = page->objects;
slub.c:				x = page->inuse;
slub.c:					x = page->pages;
slub.c:				x = atomic_long_read(&n->total_objects);
slub.c:				x = atomic_long_read(&n->total_objects) -
slub.c:				x = atomic_long_read(&n->nr_slabs);
slub.c:				x = n->nr_partial;
slub.c:		if (atomic_long_read(&n->total_objects))
slub.c:	return sprintf(buf, "%d\n", s->size);
slub.c:	return sprintf(buf, "%d\n", s->align);
slub.c:	return sprintf(buf, "%d\n", s->object_size);
slub.c:	return sprintf(buf, "%d\n", oo_objects(s->oo));
slub.c:		return -EINVAL;
slub.c:	return sprintf(buf, "%d\n", oo_order(s->oo));
slub.c:	return sprintf(buf, "%lu\n", s->min_partial);
slub.c:		return -EINVAL;
slub.c:	if (!s->ctor)
slub.c:	return sprintf(buf, "%pS\n", s->ctor);
slub.c:	return sprintf(buf, "%d\n", s->refcount < 0 ? 0 : s->refcount - 1);
slub.c:		page = slub_percpu_partial(per_cpu_ptr(s->cpu_slab, cpu));
slub.c:			pages += page->pages;
slub.c:			objects += page->pobjects;
slub.c:		page = slub_percpu_partial(per_cpu_ptr(s->cpu_slab, cpu));
slub.c:		if (page && len < PAGE_SIZE - 20)
slub.c:				page->pobjects, page->pages);
slub.c:	return sprintf(buf, "%d\n", !!(s->flags & SLAB_RECLAIM_ACCOUNT));
slub.c:	s->flags &= ~SLAB_RECLAIM_ACCOUNT;
slub.c:		s->flags |= SLAB_RECLAIM_ACCOUNT;
slub.c:	return sprintf(buf, "%d\n", !!(s->flags & SLAB_HWCACHE_ALIGN));
slub.c:	return sprintf(buf, "%d\n", !!(s->flags & SLAB_CACHE_DMA));
slub.c:	return sprintf(buf, "%d\n", !!(s->flags & SLAB_TYPESAFE_BY_RCU));
slub.c:	return sprintf(buf, "%d\n", s->reserved);
slub.c:	return sprintf(buf, "%d\n", !!(s->flags & SLAB_CONSISTENCY_CHECKS));
slub.c:	s->flags &= ~SLAB_CONSISTENCY_CHECKS;
slub.c:		s->flags &= ~__CMPXCHG_DOUBLE;
slub.c:		s->flags |= SLAB_CONSISTENCY_CHECKS;
slub.c:	return sprintf(buf, "%d\n", !!(s->flags & SLAB_TRACE));
slub.c:	if (s->refcount > 1)
slub.c:		return -EINVAL;
slub.c:	s->flags &= ~SLAB_TRACE;
slub.c:		s->flags &= ~__CMPXCHG_DOUBLE;
slub.c:		s->flags |= SLAB_TRACE;
slub.c:	return sprintf(buf, "%d\n", !!(s->flags & SLAB_RED_ZONE));
slub.c:		return -EBUSY;
slub.c:	s->flags &= ~SLAB_RED_ZONE;
slub.c:		s->flags |= SLAB_RED_ZONE;
slub.c:	calculate_sizes(s, -1);
slub.c:	return sprintf(buf, "%d\n", !!(s->flags & SLAB_POISON));
slub.c:		return -EBUSY;
slub.c:	s->flags &= ~SLAB_POISON;
slub.c:		s->flags |= SLAB_POISON;
slub.c:	calculate_sizes(s, -1);
slub.c:	return sprintf(buf, "%d\n", !!(s->flags & SLAB_STORE_USER));
slub.c:		return -EBUSY;
slub.c:	s->flags &= ~SLAB_STORE_USER;
slub.c:		s->flags &= ~__CMPXCHG_DOUBLE;
slub.c:		s->flags |= SLAB_STORE_USER;
slub.c:	calculate_sizes(s, -1);
slub.c:	int ret = -EINVAL;
slub.c:	if (!(s->flags & SLAB_STORE_USER))
slub.c:		return -ENOSYS;
slub.c:	if (!(s->flags & SLAB_STORE_USER))
slub.c:		return -ENOSYS;
slub.c:	return sprintf(buf, "%d\n", !!(s->flags & SLAB_FAILSLAB));
slub.c:	if (s->refcount > 1)
slub.c:		return -EINVAL;
slub.c:	s->flags &= ~SLAB_FAILSLAB;
slub.c:		s->flags |= SLAB_FAILSLAB;
slub.c:		return -EINVAL;
slub.c:	return sprintf(buf, "%d\n", s->remote_node_defrag_ratio / 10);
slub.c:		s->remote_node_defrag_ratio = ratio * 10;
slub.c:		return -ENOMEM;
slub.c:		unsigned x = per_cpu_ptr(s->cpu_slab, cpu)->stat[si];
slub.c:		if (data[cpu] && len < PAGE_SIZE - 20)
slub.c:		per_cpu_ptr(s->cpu_slab, cpu)->stat[si] = 0;
slub.c:		return -EINVAL;					\
slub.c:	if (!attribute->show)
slub.c:		return -EIO;
slub.c:	err = attribute->show(s, buf);
slub.c:	if (!attribute->store)
slub.c:		return -EIO;
slub.c:	err = attribute->store(s, buf, len);
slub.c:		if (s->max_attr_size < len)
slub.c:			s->max_attr_size = len;
slub.c:		 * defined semantics for rollbacks - most of the actions will
slub.c:		 * through the descendants with best-effort propagation.
slub.c:			attribute->store(c, buf, len);
slub.c:	root_cache = s->memcg_params.root_cache;
slub.c:	if (!root_cache->max_attr_size)
slub.c:		if (!attr || !attr->store || !attr->show)
slub.c:		else if (root_cache->max_attr_size < ARRAY_SIZE(mbuf))
slub.c:		len = attr->show(root_cache, buf);
slub.c:			attr->store(s, buf, len);
slub.c:		return s->memcg_params.root_cache->memcg_kset;
slub.c: * Format	:[flags-]size
slub.c:	if (s->flags & SLAB_CACHE_DMA)
slub.c:	if (s->flags & SLAB_RECLAIM_ACCOUNT)
slub.c:	if (s->flags & SLAB_CONSISTENCY_CHECKS)
slub.c:	if (s->flags & SLAB_ACCOUNT)
slub.c:		*p++ = '-';
slub.c:	p += sprintf(p, "%07d", s->size);
slub.c:	BUG_ON(p > name + ID_STR_LENGTH - 1);
slub.c:	if (!s->kobj.state_in_sysfs)
slub.c:	kset_unregister(s->memcg_kset);
slub.c:	kobject_uevent(&s->kobj, KOBJ_REMOVE);
slub.c:	kobject_put(&s->kobj);
slub.c:	INIT_WORK(&s->kobj_remove_work, sysfs_slab_remove_workfn);
slub.c:		kobject_init(&s->kobj, &slab_ktype);
slub.c:		sysfs_remove_link(&slab_kset->kobj, s->name);
slub.c:		name = s->name;
slub.c:	s->kobj.kset = kset;
slub.c:	err = kobject_init_and_add(&s->kobj, &slab_ktype, NULL, "%s", name);
slub.c:	err = sysfs_create_group(&s->kobj, &slab_attr_group);
slub.c:		s->memcg_kset = kset_create_and_add("cgroup", NULL, &s->kobj);
slub.c:		if (!s->memcg_kset) {
slub.c:			err = -ENOMEM;
slub.c:	kobject_uevent(&s->kobj, KOBJ_ADD);
slub.c:		sysfs_slab_alias(s, s->name);
slub.c:	kobject_del(&s->kobj);
slub.c:	kobject_get(&s->kobj);
slub.c:	schedule_work(&s->kobj_remove_work);
slub.c:		kobject_del(&s->kobj);
slub.c:		kobject_put(&s->kobj);
slub.c:		sysfs_remove_link(&slab_kset->kobj, name);
slub.c:		return sysfs_create_link(&slab_kset->kobj, &s->kobj, name);
slub.c:		return -ENOMEM;
slub.c:	al->s = s;
slub.c:	al->name = name;
slub.c:	al->next = alias_list;
slub.c:		return -ENOSYS;
slub.c:			       s->name);
slub.c:		alias_list = alias_list->next;
slub.c:		err = sysfs_slab_alias(al->s, al->name);
slub.c:			       al->name);
slub.c:	sinfo->active_objs = nr_objs - nr_free;
slub.c:	sinfo->num_objs = nr_objs;
slub.c:	sinfo->active_slabs = nr_slabs;
slub.c:	sinfo->num_slabs = nr_slabs;
slub.c:	sinfo->objects_per_slab = oo_objects(s->oo);
slub.c:	sinfo->cache_order = oo_order(s->oo);
slub.c:	return -EIO;
zsmalloc.c: * Released under the terms of 3-clause BSD License
zsmalloc.c: *	page->private: points to zspage
zsmalloc.c: *	page->freelist(index): links together all component pages of a zspage
zsmalloc.c: *	page->units: first object offset in a subpage of zspage
zsmalloc.c: * A single 'zspage' is composed of up to 2^N discontiguous 0-order (single)
zsmalloc.c:#define _PFN_BITS		(MAX_PHYSMEM_BITS - PAGE_SHIFT)
zsmalloc.c: * header keeps handle which is 4byte-aligned address so we
zsmalloc.c:#define OBJ_INDEX_BITS	(BITS_PER_LONG - _PFN_BITS - OBJ_TAG_BITS)
zsmalloc.c:#define OBJ_INDEX_MASK	((_AC(1, UL) << OBJ_INDEX_BITS) - 1)
zsmalloc.c: * trader-off here:
zsmalloc.c: *  - Large number of size classes is potentially wasteful as free page are
zsmalloc.c: *  - Small number of size classes causes large internal fragmentation
zsmalloc.c: *  - Probably its better to use specific size classes (empirically
zsmalloc.c:#define ZS_SIZE_CLASSES	(DIV_ROUND_UP(ZS_MAX_ALLOC_SIZE - ZS_MIN_ALLOC_SIZE, \
zsmalloc.c: * For every zspage, zspage->freeobj gives head of this list.
zsmalloc.c:		 * It's valid for non-allocated object
zsmalloc.c:	pool->handle_cachep = kmem_cache_create("zs_handle", ZS_HANDLE_SIZE,
zsmalloc.c:	if (!pool->handle_cachep)
zsmalloc.c:	pool->zspage_cachep = kmem_cache_create("zspage", sizeof(struct zspage),
zsmalloc.c:	if (!pool->zspage_cachep) {
zsmalloc.c:		kmem_cache_destroy(pool->handle_cachep);
zsmalloc.c:		pool->handle_cachep = NULL;
zsmalloc.c:	kmem_cache_destroy(pool->handle_cachep);
zsmalloc.c:	kmem_cache_destroy(pool->zspage_cachep);
zsmalloc.c:	return (unsigned long)kmem_cache_alloc(pool->handle_cachep,
zsmalloc.c:	kmem_cache_free(pool->handle_cachep, (void *)handle);
zsmalloc.c:	return kmem_cache_alloc(pool->zspage_cachep,
zsmalloc.c:	kmem_cache_free(pool->zspage_cachep, zspage);
zsmalloc.c:	return *handle ? 0 : -1;
zsmalloc.c:	return -EINVAL;
zsmalloc.c:MODULE_ALIAS("zpool-zsmalloc");
zsmalloc.c:/* per-cpu VM mapping areas for zspage accesses that cross page boundaries */
zsmalloc.c:	return zspage->isolated;
zsmalloc.c:/* Protected by class->lock */
zsmalloc.c:	return zspage->inuse;
zsmalloc.c:	zspage->inuse = val;
zsmalloc.c:	zspage->inuse += val;
zsmalloc.c:	struct page *first_page = zspage->first_page;
zsmalloc.c:	return page->units;
zsmalloc.c:	page->units = offset;
zsmalloc.c:	return zspage->freeobj;
zsmalloc.c:	zspage->freeobj = obj;
zsmalloc.c:	BUG_ON(zspage->magic != ZSPAGE_MAGIC);
zsmalloc.c:	*fullness = zspage->fullness;
zsmalloc.c:	*class_idx = zspage->class;
zsmalloc.c:	zspage->class = class_idx;
zsmalloc.c:	zspage->fullness = fullness;
zsmalloc.c:		idx = DIV_ROUND_UP(size - ZS_MIN_ALLOC_SIZE,
zsmalloc.c:	return min_t(int, ZS_SIZE_CLASSES - 1, idx);
zsmalloc.c:	class->stats.objs[type] += cnt;
zsmalloc.c:	class->stats.objs[type] -= cnt;
zsmalloc.c:	return class->stats.objs[type];
zsmalloc.c:	struct zs_pool *pool = s->private;
zsmalloc.c:		class = pool->size_class[i];
zsmalloc.c:		if (class->index != i)
zsmalloc.c:		spin_lock(&class->lock);
zsmalloc.c:		spin_unlock(&class->lock);
zsmalloc.c:		objs_per_zspage = class->objs_per_zspage;
zsmalloc.c:				class->pages_per_zspage;
zsmalloc.c:			i, class->size, class_almost_full, class_almost_empty,
zsmalloc.c:			class->pages_per_zspage, freeable);
zsmalloc.c:	return single_open(file, zs_stats_size_show, inode->i_private);
zsmalloc.c:	pool->stat_dentry = entry;
zsmalloc.c:			pool->stat_dentry, pool, &zs_stat_size_ops);
zsmalloc.c:		debugfs_remove_recursive(pool->stat_dentry);
zsmalloc.c:		pool->stat_dentry = NULL;
zsmalloc.c:	debugfs_remove_recursive(pool->stat_dentry);
zsmalloc.c:	objs_per_zspage = class->objs_per_zspage;
zsmalloc.c:	head = list_first_entry_or_null(&class->fullness_list[fullness],
zsmalloc.c:	 * Put pages with higher ->inuse first.
zsmalloc.c:			list_add(&zspage->list, &head->list);
zsmalloc.c:	list_add(&zspage->list, &class->fullness_list[fullness]);
zsmalloc.c:	VM_BUG_ON(list_empty(&class->fullness_list[fullness]));
zsmalloc.c:	list_del_init(&zspage->list);
zsmalloc.c: *     usage = Zp - wastage
zsmalloc.c:		usedpc = (zspage_size - waste) * 100 / zspage_size;
zsmalloc.c:	struct zspage *zspage = (struct zspage *)page->private;
zsmalloc.c:	BUG_ON(zspage->magic != ZSPAGE_MAGIC);
zsmalloc.c:	return page->freelist;
zsmalloc.c: * obj_to_location - get (<page>, <obj_idx>) from encoded object value
zsmalloc.c: * location_to_obj - get obj value encoded from (<page>, <obj_idx>)
zsmalloc.c:		return page->index;
zsmalloc.c:	page->freelist = NULL;
zsmalloc.c:	assert_spin_locked(&class->lock);
zsmalloc.c:	zs_stat_dec(class, OBJ_ALLOCATED, class->objs_per_zspage);
zsmalloc.c:	atomic_long_sub(class->pages_per_zspage,
zsmalloc.c:					&pool->pages_allocated);
zsmalloc.c:	VM_BUG_ON(list_empty(&zspage->list));
zsmalloc.c:		while ((off += class->size) < PAGE_SIZE) {
zsmalloc.c:			link->next = freeobj++ << OBJ_TAG_BITS;
zsmalloc.c:			link += class->size / sizeof(*link);
zsmalloc.c:			link->next = freeobj++ << OBJ_TAG_BITS;
zsmalloc.c:			link->next = -1 << OBJ_TAG_BITS;
zsmalloc.c:	int nr_pages = class->pages_per_zspage;
zsmalloc.c:	 * 1. all pages are linked together using page->freelist
zsmalloc.c:	 * 2. each sub-page point to zspage using page->private
zsmalloc.c:	 * we set PG_private to identify the first page (i.e. no other sub-page
zsmalloc.c:		page->freelist = NULL;
zsmalloc.c:			zspage->first_page = page;
zsmalloc.c:			if (unlikely(class->objs_per_zspage == 1 &&
zsmalloc.c:					class->pages_per_zspage == 1))
zsmalloc.c:			prev_page->freelist = page;
zsmalloc.c:	zspage->magic = ZSPAGE_MAGIC;
zsmalloc.c:	for (i = 0; i < class->pages_per_zspage; i++) {
zsmalloc.c:			while (--i >= 0) {
zsmalloc.c:	for (i = ZS_ALMOST_FULL; i >= ZS_EMPTY; i--) {
zsmalloc.c:		zspage = list_first_entry_or_null(&class->fullness_list[i],
zsmalloc.c:	if (area->vm)
zsmalloc.c:	area->vm = alloc_vm_area(PAGE_SIZE * 2, NULL);
zsmalloc.c:	if (!area->vm)
zsmalloc.c:		return -ENOMEM;
zsmalloc.c:	if (area->vm)
zsmalloc.c:		free_vm_area(area->vm);
zsmalloc.c:	area->vm = NULL;
zsmalloc.c:	BUG_ON(map_vm_area(area->vm, PAGE_KERNEL, pages));
zsmalloc.c:	area->vm_addr = area->vm->addr;
zsmalloc.c:	return area->vm_addr + off;
zsmalloc.c:	unsigned long addr = (unsigned long)area->vm_addr;
zsmalloc.c:	if (area->vm_buf)
zsmalloc.c:	area->vm_buf = kmalloc(ZS_MAX_ALLOC_SIZE, GFP_KERNEL);
zsmalloc.c:	if (!area->vm_buf)
zsmalloc.c:		return -ENOMEM;
zsmalloc.c:	kfree(area->vm_buf);
zsmalloc.c:	area->vm_buf = NULL;
zsmalloc.c:	char *buf = area->vm_buf;
zsmalloc.c:	if (area->vm_mm == ZS_MM_WO)
zsmalloc.c:	sizes[0] = PAGE_SIZE - off;
zsmalloc.c:	sizes[1] = size - sizes[0];
zsmalloc.c:	/* copy object to per-cpu buffer */
zsmalloc.c:	return area->vm_buf;
zsmalloc.c:	if (area->vm_mm == ZS_MM_RO)
zsmalloc.c:	buf = area->vm_buf;
zsmalloc.c:	size -= ZS_HANDLE_SIZE;
zsmalloc.c:	sizes[0] = PAGE_SIZE - off;
zsmalloc.c:	sizes[1] = size - sizes[0];
zsmalloc.c:	/* copy per-cpu buffer to object */
zsmalloc.c:	if (prev->pages_per_zspage == pages_per_zspage &&
zsmalloc.c:		prev->objs_per_zspage == objs_per_zspage)
zsmalloc.c:	return get_zspage_inuse(zspage) == class->objs_per_zspage;
zsmalloc.c:	return atomic_long_read(&pool->pages_allocated);
zsmalloc.c: * zs_map_object - get address of allocated object from handle.
zsmalloc.c:	 * Because we use per-cpu mapping areas shared among the
zsmalloc.c:	class = pool->size_class[class_idx];
zsmalloc.c:	off = (class->size * obj_idx) & ~PAGE_MASK;
zsmalloc.c:	area->vm_mm = mm;
zsmalloc.c:	if (off + class->size <= PAGE_SIZE) {
zsmalloc.c:		area->vm_addr = kmap_atomic(page);
zsmalloc.c:		ret = area->vm_addr + off;
zsmalloc.c:	ret = __zs_map_object(area, pages, off, class->size);
zsmalloc.c:	class = pool->size_class[class_idx];
zsmalloc.c:	off = (class->size * obj_idx) & ~PAGE_MASK;
zsmalloc.c:	if (off + class->size <= PAGE_SIZE)
zsmalloc.c:		kunmap_atomic(area->vm_addr);
zsmalloc.c:		__zs_unmap_object(area, pages, off, class->size);
zsmalloc.c:	offset = obj * class->size;
zsmalloc.c:	set_freeobj(zspage, link->next >> OBJ_TAG_BITS);
zsmalloc.c:		link->handle = handle;
zsmalloc.c:		/* record handle to page->index */
zsmalloc.c:		zspage->first_page->index = handle;
zsmalloc.c: * zs_malloc - Allocate block of given size from pool.
zsmalloc.c:	class = pool->size_class[get_size_class_index(size)];
zsmalloc.c:	spin_lock(&class->lock);
zsmalloc.c:		spin_unlock(&class->lock);
zsmalloc.c:	spin_unlock(&class->lock);
zsmalloc.c:	spin_lock(&class->lock);
zsmalloc.c:	set_zspage_mapping(zspage, class->index, newfg);
zsmalloc.c:	atomic_long_add(class->pages_per_zspage,
zsmalloc.c:				&pool->pages_allocated);
zsmalloc.c:	zs_stat_inc(class, OBJ_ALLOCATED, class->objs_per_zspage);
zsmalloc.c:	spin_unlock(&class->lock);
zsmalloc.c:	f_offset = (class->size * f_objidx) & ~PAGE_MASK;
zsmalloc.c:	link->next = get_freeobj(zspage) << OBJ_TAG_BITS;
zsmalloc.c:	mod_zspage_inuse(zspage, -1);
zsmalloc.c:	class = pool->size_class[class_idx];
zsmalloc.c:	spin_lock(&class->lock);
zsmalloc.c:	spin_unlock(&class->lock);
zsmalloc.c:	s_size = d_size = class->size;
zsmalloc.c:	s_off = (class->size * s_objidx) & ~PAGE_MASK;
zsmalloc.c:	d_off = (class->size * d_objidx) & ~PAGE_MASK;
zsmalloc.c:	if (s_off + class->size > PAGE_SIZE)
zsmalloc.c:		s_size = PAGE_SIZE - s_off;
zsmalloc.c:	if (d_off + class->size > PAGE_SIZE)
zsmalloc.c:		d_size = PAGE_SIZE - d_off;
zsmalloc.c:		if (written == class->size)
zsmalloc.c:		s_size -= size;
zsmalloc.c:		d_size -= size;
zsmalloc.c:			s_size = class->size - written;
zsmalloc.c:			d_size = class->size - written;
zsmalloc.c:	offset += class->size * index;
zsmalloc.c:		offset += class->size;
zsmalloc.c:	struct page *s_page = cc->s_page;
zsmalloc.c:	struct page *d_page = cc->d_page;
zsmalloc.c:	int obj_idx = cc->obj_idx;
zsmalloc.c:			ret = -ENOMEM;
zsmalloc.c:	cc->s_page = s_page;
zsmalloc.c:	cc->obj_idx = obj_idx;
zsmalloc.c:		zspage = list_first_entry_or_null(&class->fullness_list[fg[i]],
zsmalloc.c: * putback_zspage - add @zspage into right class's fullness list
zsmalloc.c:	set_zspage_mapping(zspage, class->index, fullness);
zsmalloc.c:	rwlock_init(&zspage->lock);
zsmalloc.c:	read_lock(&zspage->lock);
zsmalloc.c:	read_unlock(&zspage->lock);
zsmalloc.c:	write_lock(&zspage->lock);
zsmalloc.c:	write_unlock(&zspage->lock);
zsmalloc.c:	zspage->isolated++;
zsmalloc.c:	zspage->isolated--;
zsmalloc.c:		newpage->index = oldpage->index;
zsmalloc.c:	pool = mapping->private_data;
zsmalloc.c:	class = pool->size_class[class_idx];
zsmalloc.c:	spin_lock(&class->lock);
zsmalloc.c:		spin_unlock(&class->lock);
zsmalloc.c:	if (list_empty(&zspage->list) && !is_zspage_isolated(zspage)) {
zsmalloc.c:		spin_unlock(&class->lock);
zsmalloc.c:	if (!list_empty(&zspage->list) && !is_zspage_isolated(zspage)) {
zsmalloc.c:	spin_unlock(&class->lock);
zsmalloc.c:	int ret = -EAGAIN;
zsmalloc.c:		return -EINVAL;
zsmalloc.c:	pool = mapping->private_data;
zsmalloc.c:	class = pool->size_class[class_idx];
zsmalloc.c:	spin_lock(&class->lock);
zsmalloc.c:		pos += class->size;
zsmalloc.c:					addr += class->size) {
zsmalloc.c:						addr += class->size) {
zsmalloc.c:	spin_unlock(&class->lock);
zsmalloc.c:	pool = mapping->private_data;
zsmalloc.c:	class = pool->size_class[class_idx];
zsmalloc.c:	spin_lock(&class->lock);
zsmalloc.c:			schedule_work(&pool->free_work);
zsmalloc.c:	spin_unlock(&class->lock);
zsmalloc.c:	pool->inode = alloc_anon_inode(zsmalloc_mnt->mnt_sb);
zsmalloc.c:	if (IS_ERR(pool->inode)) {
zsmalloc.c:		pool->inode = NULL;
zsmalloc.c:	pool->inode->i_mapping->private_data = pool;
zsmalloc.c:	pool->inode->i_mapping->a_ops = &zsmalloc_aops;
zsmalloc.c:	flush_work(&pool->free_work);
zsmalloc.c:	iput(pool->inode);
zsmalloc.c:		class = pool->size_class[i];
zsmalloc.c:		if (class->index != i)
zsmalloc.c:		spin_lock(&class->lock);
zsmalloc.c:		list_splice_init(&class->fullness_list[ZS_EMPTY], &free_pages);
zsmalloc.c:		spin_unlock(&class->lock);
zsmalloc.c:		list_del(&zspage->list);
zsmalloc.c:		class = pool->size_class[class_idx];
zsmalloc.c:		spin_lock(&class->lock);
zsmalloc.c:		__free_zspage(pool, pool->size_class[class_idx], zspage);
zsmalloc.c:		spin_unlock(&class->lock);
zsmalloc.c:	schedule_work(&pool->free_work);
zsmalloc.c:	INIT_WORK(&pool->free_work, async_free_zspage);
zsmalloc.c:		__SetPageMovable(page, pool->inode->i_mapping);
zsmalloc.c:	obj_wasted = obj_allocated - obj_used;
zsmalloc.c:	obj_wasted /= class->objs_per_zspage;
zsmalloc.c:	return obj_wasted * class->pages_per_zspage;
zsmalloc.c:	spin_lock(&class->lock);
zsmalloc.c:			pool->stats.pages_compacted += class->pages_per_zspage;
zsmalloc.c:		spin_unlock(&class->lock);
zsmalloc.c:		spin_lock(&class->lock);
zsmalloc.c:	spin_unlock(&class->lock);
zsmalloc.c:	for (i = ZS_SIZE_CLASSES - 1; i >= 0; i--) {
zsmalloc.c:		class = pool->size_class[i];
zsmalloc.c:		if (class->index != i)
zsmalloc.c:	return pool->stats.pages_compacted;
zsmalloc.c:	memcpy(stats, &pool->stats, sizeof(struct zs_pool_stats));
zsmalloc.c:	pages_freed = pool->stats.pages_compacted;
zsmalloc.c:	pages_freed = zs_compact(pool) - pages_freed;
zsmalloc.c:	for (i = ZS_SIZE_CLASSES - 1; i >= 0; i--) {
zsmalloc.c:		class = pool->size_class[i];
zsmalloc.c:		if (class->index != i)
zsmalloc.c:	if (pool->shrinker_enabled) {
zsmalloc.c:		unregister_shrinker(&pool->shrinker);
zsmalloc.c:		pool->shrinker_enabled = false;
zsmalloc.c:	pool->shrinker.scan_objects = zs_shrinker_scan;
zsmalloc.c:	pool->shrinker.count_objects = zs_shrinker_count;
zsmalloc.c:	pool->shrinker.batch = 0;
zsmalloc.c:	pool->shrinker.seeks = DEFAULT_SEEKS;
zsmalloc.c:	return register_shrinker(&pool->shrinker);
zsmalloc.c: * zs_create_pool - Creates an allocation pool to work from.
zsmalloc.c:	pool->name = kstrdup(name, GFP_KERNEL);
zsmalloc.c:	if (!pool->name)
zsmalloc.c:	for (i = ZS_SIZE_CLASSES - 1; i >= 0; i--) {
zsmalloc.c:				pool->size_class[i] = prev_class;
zsmalloc.c:		class->size = size;
zsmalloc.c:		class->index = i;
zsmalloc.c:		class->pages_per_zspage = pages_per_zspage;
zsmalloc.c:		class->objs_per_zspage = objs_per_zspage;
zsmalloc.c:		spin_lock_init(&class->lock);
zsmalloc.c:		pool->size_class[i] = class;
zsmalloc.c:			INIT_LIST_HEAD(&class->fullness_list[fullness]);
zsmalloc.c:		pool->shrinker_enabled = true;
zsmalloc.c:		struct size_class *class = pool->size_class[i];
zsmalloc.c:		if (class->index != i)
zsmalloc.c:			if (!list_empty(&class->fullness_list[fg])) {
zsmalloc.c:				pr_info("Freeing non-empty class with size %db, fullness group %d\n",
zsmalloc.c:					class->size, fg);
zsmalloc.c:	kfree(pool->name);
page-writeback.c: * mm/page-writeback.c
page-writeback.c:#include <linux/backing-dev.h>
page-writeback.c:#define DIRTY_POLL_THRESH	(128 >> (PAGE_SHIFT - 10))
page-writeback.c: * The interval between `kupdate'-style writebacks
page-writeback.c:/* End of sysctl-exported parameters */
page-writeback.c:	unsigned long		wb_dirty;	/* per-wb counterparts */
page-writeback.c:				.wb_completions = &(__wb)->completions
page-writeback.c:				.wb_completions = &(__wb)->memcg_completions, \
page-writeback.c:	return dtc->dom;
page-writeback.c:	return dtc->dom;
page-writeback.c:	return mdtc->gdtc;
page-writeback.c:	return &wb->memcg_completions;
page-writeback.c:	unsigned long this_bw = wb->avg_write_bandwidth;
page-writeback.c:	unsigned long tot_bw = atomic_long_read(&wb->bdi->tot_write_bandwidth);
page-writeback.c:	unsigned long long min = wb->bdi->min_ratio;
page-writeback.c:	unsigned long long max = wb->bdi->max_ratio;
page-writeback.c:				.wb_completions = &(__wb)->completions
page-writeback.c:	*minp = wb->bdi->min_ratio;
page-writeback.c:	*maxp = wb->bdi->max_ratio;
page-writeback.c: * user-configurable dirty ratio is the effictive number of pages that
page-writeback.c: * absolute number of bytes, calculating the per-zone dirty limit can
page-writeback.c: * node_dirtyable_memory - number of dirtyable pages in a node
page-writeback.c: * page cache.  This is the base value for the per-node dirty limits.
page-writeback.c:		struct zone *zone = pgdat->node_zones + z;
page-writeback.c:	nr_pages -= min(nr_pages, pgdat->totalreserve_pages);
page-writeback.c:			z = &NODE_DATA(node)->node_zones[i];
page-writeback.c:			nr_pages -= min(nr_pages, high_wmark_pages(z));
page-writeback.c: * global_dirtyable_memory - number of globally dirtyable pages
page-writeback.c:	x -= min(x, totalreserve_pages);
page-writeback.c:		x -= highmem_dirtyable_memory(x);
page-writeback.c: * domain_dirty_limits - calculate thresh and bg_thresh for a wb_domain
page-writeback.c: * Calculate @dtc->thresh and ->bg_thresh considering
page-writeback.c: * must ensure that @dtc->avail is set before calling this function.  The
page-writeback.c: * real-time tasks.
page-writeback.c:	const unsigned long available_memory = dtc->avail;
page-writeback.c:	/* convert ratios to per-PAGE_SIZE for higher precision */
page-writeback.c:		unsigned long global_avail = gdtc->avail;
page-writeback.c:		 * per-PAGE_SIZE, they can be obtained by dividing bytes by
page-writeback.c:	if (tsk->flags & PF_LESS_THROTTLE || rt_task(tsk)) {
page-writeback.c:	dtc->thresh = thresh;
page-writeback.c:	dtc->bg_thresh = bg_thresh;
page-writeback.c: * global_dirty_limits - background-writeback and dirty-throttling thresholds
page-writeback.c: * node_dirty_limit - maximum number of dirty pages allowed in a node
page-writeback.c:	if (tsk->flags & PF_LESS_THROTTLE || rt_task(tsk))
page-writeback.c: * node_dirty_ok - tells whether a node is within its dirty limits
page-writeback.c:	__fprop_inc_percpu_max(&dom->completions, completions,
page-writeback.c:	if (unlikely(!dom->period_time)) {
page-writeback.c:		dom->period_time = wp_next_time(jiffies);
page-writeback.c:		mod_timer(&dom->period_timer, dom->period_time);
page-writeback.c:	wb_domain_writeout_inc(&global_wb_domain, &wb->completions,
page-writeback.c:			       wb->bdi->max_prop_frac);
page-writeback.c:				       wb->bdi->max_prop_frac);
page-writeback.c:	int miss_periods = (jiffies - dom->period_time) /
page-writeback.c:	if (fprop_new_period(&dom->completions, miss_periods + 1)) {
page-writeback.c:		dom->period_time = wp_next_time(dom->period_time +
page-writeback.c:		mod_timer(&dom->period_timer, dom->period_time);
page-writeback.c:		dom->period_time = 0;
page-writeback.c:	spin_lock_init(&dom->lock);
page-writeback.c:	setup_deferrable_timer(&dom->period_timer, writeout_period,
page-writeback.c:	dom->dirty_limit_tstamp = jiffies;
page-writeback.c:	return fprop_global_init(&dom->completions, gfp);
page-writeback.c:	del_timer_sync(&dom->period_timer);
page-writeback.c:	fprop_global_destroy(&dom->completions);
page-writeback.c:	if (min_ratio > bdi->max_ratio) {
page-writeback.c:		ret = -EINVAL;
page-writeback.c:		min_ratio -= bdi->min_ratio;
page-writeback.c:			bdi->min_ratio += min_ratio;
page-writeback.c:			ret = -EINVAL;
page-writeback.c:		return -EINVAL;
page-writeback.c:	if (bdi->min_ratio > max_ratio) {
page-writeback.c:		ret = -EINVAL;
page-writeback.c:		bdi->max_ratio = max_ratio;
page-writeback.c:		bdi->max_prop_frac = (FPROP_FRAC_BASE * max_ratio) / 100;
page-writeback.c:	return max(thresh, dom->dirty_limit);
page-writeback.c: * system-wide clean memory excluding the amount being used in the domain.
page-writeback.c:	unsigned long clean = filepages - min(filepages, mdtc->dirty);
page-writeback.c:	unsigned long global_clean = gdtc->avail - min(gdtc->avail, gdtc->dirty);
page-writeback.c:	unsigned long other_clean = global_clean - min(global_clean, clean);
page-writeback.c:	mdtc->avail = filepages + min(headroom, other_clean);
page-writeback.c: * __wb_calc_thresh - @wb's share of dirty throttling threshold
page-writeback.c: * - starving fast devices
page-writeback.c: * - piling up dirty pages (that will take long time to sync) on slow devices
page-writeback.c: * bounded by the bdi->min_ratio and/or bdi->max_ratio parameters, if set.
page-writeback.c:	unsigned long thresh = dtc->thresh;
page-writeback.c:	fprop_fraction_percpu(&dom->completions, dtc->wb_completions,
page-writeback.c:	wb_thresh = (thresh * (100 - bdi_min_ratio)) / 100;
page-writeback.c:	wb_min_max_ratio(dtc->wb, &wb_min_ratio, &wb_max_ratio);
page-writeback.c: *                           setpoint - dirty 3
page-writeback.c: *        f(dirty) := 1.0 + (----------------)
page-writeback.c: *                           limit - setpoint
page-writeback.c:	x = div64_s64(((s64)setpoint - (s64)dirty) << RATELIMIT_CALC_SHIFT,
page-writeback.c:		      (limit - setpoint) | 1);
page-writeback.c: *   0 +------------.------------------.----------------------*------------->
page-writeback.c: *   0 +----------------------.-------------------------------.------------->
page-writeback.c: * - start writing to a slow SD card and a fast disk at the same time. The SD
page-writeback.c: * - the wb dirty thresh drops quickly due to change of JBOD workload
page-writeback.c:	struct bdi_writeback *wb = dtc->wb;
page-writeback.c:	unsigned long write_bw = wb->avg_write_bandwidth;
page-writeback.c:	unsigned long freerun = dirty_freerun_ceiling(dtc->thresh, dtc->bg_thresh);
page-writeback.c:	unsigned long limit = hard_dirty_limit(dtc_dom(dtc), dtc->thresh);
page-writeback.c:	unsigned long wb_thresh = dtc->wb_thresh;
page-writeback.c:	dtc->pos_ratio = 0;
page-writeback.c:	if (unlikely(dtc->dirty >= limit))
page-writeback.c:	pos_ratio = pos_ratio_polynom(setpoint, dtc->dirty, limit);
page-writeback.c:	 * This is especially important for fuse which sets bdi->max_ratio to
page-writeback.c:	 * total amount of RAM is 16GB, bdi->max_ratio is equal to 1%, global
page-writeback.c:	if (unlikely(wb->bdi->capabilities & BDI_CAP_STRICTLIMIT)) {
page-writeback.c:		if (dtc->wb_dirty < 8) {
page-writeback.c:			dtc->pos_ratio = min_t(long long, pos_ratio * 2,
page-writeback.c:		if (dtc->wb_dirty >= wb_thresh)
page-writeback.c:						    dtc->wb_bg_thresh);
page-writeback.c:		wb_pos_ratio = pos_ratio_polynom(wb_setpoint, dtc->wb_dirty,
page-writeback.c:		 * but it would look too non-natural for the case of all
page-writeback.c:		 * with bdi->max_ratio == 100%.
page-writeback.c:		dtc->pos_ratio = min(pos_ratio, wb_pos_ratio);
page-writeback.c:	 *        f(wb_dirty) := 1.0 + k * (wb_dirty - wb_setpoint)
page-writeback.c:	 *                        x_intercept - wb_dirty
page-writeback.c:	 *                     := --------------------------
page-writeback.c:	 *                        x_intercept - wb_setpoint
page-writeback.c:	 * (2) k = - 1 / (8 * write_bw)  (in single wb case)
page-writeback.c:	 *        [wb_setpoint - write_bw/2, wb_setpoint + write_bw/2]
page-writeback.c:	if (unlikely(wb_thresh > dtc->thresh))
page-writeback.c:		wb_thresh = dtc->thresh;
page-writeback.c:	wb_thresh = max(wb_thresh, (limit - dtc->dirty) / 8);
page-writeback.c:	x = div_u64((u64)wb_thresh << 16, dtc->thresh | 1);
page-writeback.c:	 * (thresh - wb_thresh ~= 0) and transit to wb_thresh in JBOD case.
page-writeback.c:	 *        wb_thresh                    thresh - wb_thresh
page-writeback.c:	 * span = --------- * (8 * write_bw) + ------------------ * wb_thresh
page-writeback.c:	span = (dtc->thresh - wb_thresh + 8 * write_bw) * (u64)x >> 16;
page-writeback.c:	if (dtc->wb_dirty < x_intercept - span / 4) {
page-writeback.c:		pos_ratio = div64_u64(pos_ratio * (x_intercept - dtc->wb_dirty),
page-writeback.c:				      (x_intercept - wb_setpoint) | 1);
page-writeback.c:	if (dtc->wb_dirty < x_intercept) {
page-writeback.c:		if (dtc->wb_dirty > x_intercept / 8)
page-writeback.c:					    dtc->wb_dirty);
page-writeback.c:	dtc->pos_ratio = pos_ratio;
page-writeback.c:	unsigned long avg = wb->avg_write_bandwidth;
page-writeback.c:	unsigned long old = wb->write_bandwidth;
page-writeback.c:	 *                   bw * elapsed + write_bandwidth * (period - elapsed)
page-writeback.c:	 * write_bandwidth = ---------------------------------------------------
page-writeback.c:	bw = written - min(written, wb->written_stamp);
page-writeback.c:	bw += (u64)wb->write_bandwidth * (period - elapsed);
page-writeback.c:		avg -= (avg - old) >> 3;
page-writeback.c:		avg += (old - avg) >> 3;
page-writeback.c:		long delta = avg - wb->avg_write_bandwidth;
page-writeback.c:					&wb->bdi->tot_write_bandwidth) <= 0);
page-writeback.c:	wb->write_bandwidth = bw;
page-writeback.c:	wb->avg_write_bandwidth = avg;
page-writeback.c:	unsigned long thresh = dtc->thresh;
page-writeback.c:	unsigned long limit = dom->dirty_limit;
page-writeback.c:	 * dom->dirty_limit which is guaranteed to lie above the dirty pages.
page-writeback.c:	thresh = max(thresh, dtc->dirty);
page-writeback.c:		limit -= (limit - thresh) >> 5;
page-writeback.c:	dom->dirty_limit = limit;
page-writeback.c:	if (time_before(now, dom->dirty_limit_tstamp + BANDWIDTH_INTERVAL))
page-writeback.c:	spin_lock(&dom->lock);
page-writeback.c:	if (time_after_eq(now, dom->dirty_limit_tstamp + BANDWIDTH_INTERVAL)) {
page-writeback.c:		dom->dirty_limit_tstamp = now;
page-writeback.c:	spin_unlock(&dom->lock);
page-writeback.c: * Maintain wb->dirty_ratelimit, the base dirty throttle rate.
page-writeback.c:	struct bdi_writeback *wb = dtc->wb;
page-writeback.c:	unsigned long dirty = dtc->dirty;
page-writeback.c:	unsigned long freerun = dirty_freerun_ceiling(dtc->thresh, dtc->bg_thresh);
page-writeback.c:	unsigned long limit = hard_dirty_limit(dtc_dom(dtc), dtc->thresh);
page-writeback.c:	unsigned long write_bw = wb->avg_write_bandwidth;
page-writeback.c:	unsigned long dirty_ratelimit = wb->dirty_ratelimit;
page-writeback.c:	 * when dirty pages are truncated by userspace or re-dirtied by FS.
page-writeback.c:	dirty_rate = (dirtied - wb->dirtied_stamp) * HZ / elapsed;
page-writeback.c:					dtc->pos_ratio >> RATELIMIT_CALC_SHIFT;
page-writeback.c:	 *	wb->dirty_ratelimit = balanced_dirty_ratelimit;
page-writeback.c:	 *	task_ratelimit - dirty_ratelimit
page-writeback.c:	 *	= (pos_ratio - 1) * dirty_ratelimit
page-writeback.c:	 * - dirty_ratelimit > balanced_dirty_ratelimit
page-writeback.c:	 * - dirty_ratelimit > task_ratelimit (dirty pages are above setpoint)
page-writeback.c:	 * |task_ratelimit - dirty_ratelimit| is used to limit the step size
page-writeback.c:	if (unlikely(wb->bdi->capabilities & BDI_CAP_STRICTLIMIT)) {
page-writeback.c:		dirty = dtc->wb_dirty;
page-writeback.c:		if (dtc->wb_dirty < 8)
page-writeback.c:			setpoint = dtc->wb_dirty + 1;
page-writeback.c:			setpoint = (dtc->wb_thresh + dtc->wb_bg_thresh) / 2;
page-writeback.c:		x = min3(wb->balanced_dirty_ratelimit,
page-writeback.c:			step = x - dirty_ratelimit;
page-writeback.c:		x = max3(wb->balanced_dirty_ratelimit,
page-writeback.c:			step = dirty_ratelimit - x;
page-writeback.c:		dirty_ratelimit -= step;
page-writeback.c:	wb->dirty_ratelimit = max(dirty_ratelimit, 1UL);
page-writeback.c:	wb->balanced_dirty_ratelimit = balanced_dirty_ratelimit;
page-writeback.c:	struct bdi_writeback *wb = gdtc->wb;
page-writeback.c:	unsigned long elapsed = now - wb->bw_time_stamp;
page-writeback.c:	lockdep_assert_held(&wb->list_lock);
page-writeback.c:	 * rate-limit, only update once every 200ms.
page-writeback.c:	dirtied = percpu_counter_read(&wb->stat[WB_DIRTIED]);
page-writeback.c:	written = percpu_counter_read(&wb->stat[WB_WRITTEN]);
page-writeback.c:	 * Skip quiet periods when disk bandwidth is under-utilized.
page-writeback.c:	if (elapsed > HZ && time_before(wb->bw_time_stamp, start_time))
page-writeback.c:	wb->dirtied_stamp = dirtied;
page-writeback.c:	wb->written_stamp = written;
page-writeback.c:	wb->bw_time_stamp = now;
page-writeback.c: * global_zone_page_state() too often. So scale it near-sqrt to the safety margin
page-writeback.c:		return 1UL << (ilog2(thresh - dirty) >> 1);
page-writeback.c:	unsigned long bw = wb->avg_write_bandwidth;
page-writeback.c:	long hi = ilog2(wb->avg_write_bandwidth);
page-writeback.c:	long lo = ilog2(wb->dirty_ratelimit);
page-writeback.c:	/* target for 10ms pause on 1-dd case */
page-writeback.c:		t += (hi - lo) * (10 * HZ) / 1024;
page-writeback.c:	 * case fio-mmap-randwrite-64k, which does 16*{sync read, async write}.
page-writeback.c:	struct bdi_writeback *wb = dtc->wb;
page-writeback.c:	 * - in JBOD setup, wb_thresh can fluctuate a lot
page-writeback.c:	 * - in a system with HDD and USB key, the USB key may somehow
page-writeback.c:	dtc->wb_thresh = __wb_calc_thresh(dtc);
page-writeback.c:	dtc->wb_bg_thresh = dtc->thresh ?
page-writeback.c:		div_u64((u64)dtc->wb_thresh * dtc->bg_thresh, dtc->thresh) : 0;
page-writeback.c:	 * reported dirty, even though there are thresh-m pages
page-writeback.c:	if (dtc->wb_thresh < 2 * wb_stat_error(wb)) {
page-writeback.c:		dtc->wb_dirty = wb_reclaimable + wb_stat_sum(wb, WB_WRITEBACK);
page-writeback.c:		dtc->wb_dirty = wb_reclaimable + wb_stat(wb, WB_WRITEBACK);
page-writeback.c:	struct backing_dev_info *bdi = wb->bdi;
page-writeback.c:	bool strictlimit = bdi->capabilities & BDI_CAP_STRICTLIMIT;
page-writeback.c:		gdtc->avail = global_dirtyable_memory();
page-writeback.c:		gdtc->dirty = nr_reclaimable + global_node_page_state(NR_WRITEBACK);
page-writeback.c:			dirty = gdtc->wb_dirty;
page-writeback.c:			thresh = gdtc->wb_thresh;
page-writeback.c:			bg_thresh = gdtc->wb_bg_thresh;
page-writeback.c:			dirty = gdtc->dirty;
page-writeback.c:			thresh = gdtc->thresh;
page-writeback.c:			bg_thresh = gdtc->bg_thresh;
page-writeback.c:					    &mdtc->dirty, &writeback);
page-writeback.c:			mdtc->dirty += writeback;
page-writeback.c:				m_dirty = mdtc->wb_dirty;
page-writeback.c:				m_thresh = mdtc->wb_thresh;
page-writeback.c:				m_bg_thresh = mdtc->wb_bg_thresh;
page-writeback.c:				m_dirty = mdtc->dirty;
page-writeback.c:				m_thresh = mdtc->thresh;
page-writeback.c:				m_bg_thresh = mdtc->bg_thresh;
page-writeback.c:		 * catch-up. This avoids (excessively) small writeouts
page-writeback.c:		 * up are the price we consciously pay for strictlimit-ing.
page-writeback.c:			current->dirty_paused_when = now;
page-writeback.c:			current->nr_dirtied = 0;
page-writeback.c:			current->nr_dirtied_pause = min(intv, m_intv);
page-writeback.c:		dirty_exceeded = (gdtc->wb_dirty > gdtc->wb_thresh) &&
page-writeback.c:			((gdtc->dirty > gdtc->thresh) || strictlimit);
page-writeback.c:			dirty_exceeded |= (mdtc->wb_dirty > mdtc->wb_thresh) &&
page-writeback.c:				((mdtc->dirty > mdtc->thresh) || strictlimit);
page-writeback.c:			if (mdtc->pos_ratio < gdtc->pos_ratio)
page-writeback.c:		if (dirty_exceeded && !wb->dirty_exceeded)
page-writeback.c:			wb->dirty_exceeded = 1;
page-writeback.c:		if (time_is_before_jiffies(wb->bw_time_stamp +
page-writeback.c:			spin_lock(&wb->list_lock);
page-writeback.c:			spin_unlock(&wb->list_lock);
page-writeback.c:		dirty_ratelimit = wb->dirty_ratelimit;
page-writeback.c:		task_ratelimit = ((u64)dirty_ratelimit * sdtc->pos_ratio) >>
page-writeback.c:		max_pause = wb_max_pause(wb, sdtc->wb_dirty);
page-writeback.c:		if (current->dirty_paused_when)
page-writeback.c:			pause -= now - current->dirty_paused_when;
page-writeback.c:		 * for up to 800ms from time to time on 1-HDD; so does xfs,
page-writeback.c:						  sdtc->thresh,
page-writeback.c:						  sdtc->bg_thresh,
page-writeback.c:						  sdtc->dirty,
page-writeback.c:						  sdtc->wb_thresh,
page-writeback.c:						  sdtc->wb_dirty,
page-writeback.c:			if (pause < -HZ) {
page-writeback.c:				current->dirty_paused_when = now;
page-writeback.c:				current->nr_dirtied = 0;
page-writeback.c:				current->dirty_paused_when += period;
page-writeback.c:				current->nr_dirtied = 0;
page-writeback.c:			} else if (current->nr_dirtied_pause <= pages_dirtied)
page-writeback.c:				current->nr_dirtied_pause += pages_dirtied;
page-writeback.c:			now += min(pause - max_pause, max_pause);
page-writeback.c:					  sdtc->thresh,
page-writeback.c:					  sdtc->bg_thresh,
page-writeback.c:					  sdtc->dirty,
page-writeback.c:					  sdtc->wb_thresh,
page-writeback.c:					  sdtc->wb_dirty,
page-writeback.c:		wb->dirty_sleep = now;
page-writeback.c:		current->dirty_paused_when = now + pause;
page-writeback.c:		current->nr_dirtied = 0;
page-writeback.c:		current->nr_dirtied_pause = nr_dirtied_pause;
page-writeback.c:		 * In theory 1 page is enough to keep the consumer-producer
page-writeback.c:		if (sdtc->wb_dirty <= wb_stat_error(wb))
page-writeback.c:	if (!dirty_exceeded && wb->dirty_exceeded)
page-writeback.c:		wb->dirty_exceeded = 0;
page-writeback.c:	if (nr_reclaimable > gdtc->bg_thresh)
page-writeback.c: *		dirty tsk->nr_dirtied_pause pages;
page-writeback.c: * (tsk->nr_dirtied_pause - 1) pages, balance_dirty_pages() will never be
page-writeback.c: * balance_dirty_pages_ratelimited - balance dirty memory state
page-writeback.c:	struct inode *inode = mapping->host;
page-writeback.c:		wb = &bdi->wb;
page-writeback.c:	ratelimit = current->nr_dirtied_pause;
page-writeback.c:	if (wb->dirty_exceeded)
page-writeback.c:		ratelimit = min(ratelimit, 32 >> (PAGE_SHIFT - 10));
page-writeback.c:	 * time, hence all honoured too large initial task->nr_dirtied_pause.
page-writeback.c:	if (unlikely(current->nr_dirtied >= ratelimit))
page-writeback.c:	 * short-lived tasks (eg. gcc invocations in a kernel build) escaping
page-writeback.c:	 * the dirty throttling and livelock other long-run dirtiers.
page-writeback.c:	if (*p > 0 && current->nr_dirtied < ratelimit) {
page-writeback.c:		nr_pages_dirtied = min(*p, ratelimit - current->nr_dirtied);
page-writeback.c:		*p -= nr_pages_dirtied;
page-writeback.c:		current->nr_dirtied += nr_pages_dirtied;
page-writeback.c:	if (unlikely(current->nr_dirtied >= ratelimit))
page-writeback.c:		balance_dirty_pages(mapping, wb, current->nr_dirtied);
page-writeback.c: * wb_over_bg_thresh - does @wb need to be written back?
page-writeback.c:	gdtc->avail = global_dirtyable_memory();
page-writeback.c:	gdtc->dirty = global_node_page_state(NR_FILE_DIRTY) +
page-writeback.c:	if (gdtc->dirty > gdtc->bg_thresh)
page-writeback.c:	    wb_calc_thresh(gdtc->wb, gdtc->bg_thresh))
page-writeback.c:		mem_cgroup_wb_stats(wb, &filepages, &headroom, &mdtc->dirty,
page-writeback.c:		if (mdtc->dirty > mdtc->bg_thresh)
page-writeback.c:		    wb_calc_thresh(mdtc->wb, mdtc->bg_thresh))
page-writeback.c:	if (!bdi_has_dirty_io(q->backing_dev_info))
page-writeback.c:	list_for_each_entry_rcu(wb, &q->backing_dev_info->wb_list, bdi_node)
page-writeback.c: * then push it back - the user is still using the disk.
page-writeback.c:	mod_timer(&info->laptop_mode_wb_timer, jiffies + laptop_mode);
page-writeback.c:		del_timer(&bdi->laptop_mode_wb_timer);
page-writeback.c: * If ratelimit_pages is too high then we can get into dirty-data overload
page-writeback.c:	dom->dirty_limit = dirty_thresh;
page-writeback.c: * is now applied to total non-HIGHPAGE memory (by subtracting
page-writeback.c: * non-HIGHMEM memory.
page-writeback.c: * tag_pages_for_writeback - tag pages to be written by write_cache_pages
page-writeback.c:	spin_lock_irq(&mapping->tree_lock);
page-writeback.c:	radix_tree_for_each_tagged(slot, &mapping->page_tree, &iter, start,
page-writeback.c:		radix_tree_iter_tag_set(&mapping->page_tree, &iter,
page-writeback.c:		spin_unlock_irq(&mapping->tree_lock);
page-writeback.c:		spin_lock_irq(&mapping->tree_lock);
page-writeback.c:	spin_unlock_irq(&mapping->tree_lock);
page-writeback.c: * write_cache_pages - walk the list of dirty pages of the given address space and write all of them.
page-writeback.c: * @wbc: subtract the number of written pages from *@wbc->nr_to_write
page-writeback.c: * if it's dirty.  This is desirable behaviour for memory-cleaning writeback,
page-writeback.c: * but it is INCORRECT for data-integrity system calls such as fsync().  fsync()
page-writeback.c: * the call was made get new I/O started against them.  If wbc->sync_mode is
page-writeback.c: * writing them. For data-integrity sync we have to be careful so that we do
page-writeback.c:	if (wbc->range_cyclic) {
page-writeback.c:		writeback_index = mapping->writeback_index; /* prev offset */
page-writeback.c:		end = -1;
page-writeback.c:		index = wbc->range_start >> PAGE_SHIFT;
page-writeback.c:		end = wbc->range_end >> PAGE_SHIFT;
page-writeback.c:		if (wbc->range_start == 0 && wbc->range_end == LLONG_MAX)
page-writeback.c:	if (wbc->sync_mode == WB_SYNC_ALL || wbc->tagged_writepages)
page-writeback.c:	if (wbc->sync_mode == WB_SYNC_ALL || wbc->tagged_writepages)
page-writeback.c:			      min(end - index, (pgoff_t)PAGEVEC_SIZE-1) + 1);
page-writeback.c:			 * invalidated (changing page->mapping to NULL), or
page-writeback.c:			 * mapping. However, page->index will not change
page-writeback.c:			if (page->index > end) {
page-writeback.c:				 * end == -1 in that case.
page-writeback.c:			done_index = page->index;
page-writeback.c:			if (unlikely(page->mapping != mapping)) {
page-writeback.c:				if (wbc->sync_mode != WB_SYNC_NONE)
page-writeback.c:			trace_wbc_writepage(wbc, inode_to_bdi(mapping->host));
page-writeback.c:					done_index = page->index + 1;
page-writeback.c:			if (--wbc->nr_to_write <= 0 &&
page-writeback.c:			    wbc->sync_mode == WB_SYNC_NONE) {
page-writeback.c:		end = writeback_index - 1;
page-writeback.c:	if (wbc->range_cyclic || (range_whole && wbc->nr_to_write > 0))
page-writeback.c:		mapping->writeback_index = done_index;
page-writeback.c:	int ret = mapping->a_ops->writepage(page, wbc);
page-writeback.c: * generic_writepages - walk the list of dirty pages of the given address space and writepage() all of them.
page-writeback.c: * @wbc: subtract the number of written pages from *@wbc->nr_to_write
page-writeback.c:	if (!mapping->a_ops->writepage)
page-writeback.c:	if (wbc->nr_to_write <= 0)
page-writeback.c:		if (mapping->a_ops->writepages)
page-writeback.c:			ret = mapping->a_ops->writepages(mapping, wbc);
page-writeback.c:		if ((ret != -ENOMEM) || (wbc->sync_mode != WB_SYNC_ALL))
page-writeback.c: * write_one_page - write out a single page and wait on I/O
page-writeback.c:	struct address_space *mapping = page->mapping;
page-writeback.c:		ret = mapping->a_ops->writepage(page, &wbc);
page-writeback.c:	struct inode *inode = mapping->host;
page-writeback.c:		current->nr_dirtied++;
page-writeback.c: * page dirty in that case, but not all the buffers.  This is a "bottom-up"
page-writeback.c: * dirtying, whereas __set_page_dirty_buffers() is a "top-down" dirtying.
page-writeback.c:		spin_lock_irqsave(&mapping->tree_lock, flags);
page-writeback.c:		radix_tree_tag_set(&mapping->page_tree, page_index(page),
page-writeback.c:		spin_unlock_irqrestore(&mapping->tree_lock, flags);
page-writeback.c:		if (mapping->host) {
page-writeback.c:			__mark_inode_dirty(mapping->host, I_DIRTY_PAGES);
page-writeback.c: * Call this whenever redirtying a page, to de-account the dirty counters
page-writeback.c: * (NR_DIRTIED, BDI_DIRTIED, tsk->nr_dirtied), so that they match the written
page-writeback.c:	struct address_space *mapping = page->mapping;
page-writeback.c:		struct inode *inode = mapping->host;
page-writeback.c:		current->nr_dirtied--;
page-writeback.c:	wbc->pages_skipped++;
page-writeback.c:		int (*spd)(struct page *) = mapping->a_ops->set_page_dirty;
page-writeback.c: * page->mapping->host, and if the page is unlocked.  This is because another
page-writeback.c: * Usually, the page _is_ locked, or the caller is a user-space process which
page-writeback.c:		struct inode *inode = mapping->host;
page-writeback.c: * tagged as dirty in the radix tree so that a concurrent write-for-sync
page-writeback.c: * can discover it via a PAGECACHE_TAG_DIRTY walk.  The ->writepage
page-writeback.c: * at which stage we bring the page's dirty flag and radix-tree dirty tag
page-writeback.c: * This incoherency between the page's dirty flag and radix-tree tag is
page-writeback.c:		struct inode *inode = mapping->host;
page-writeback.c:		 *  (b) we tell the low-level filesystem to
page-writeback.c:		 * has no effect on the actual dirty bit - since
page-writeback.c:		struct inode *inode = mapping->host;
page-writeback.c:		spin_lock_irqsave(&mapping->tree_lock, flags);
page-writeback.c:			radix_tree_tag_clear(&mapping->page_tree,
page-writeback.c:		if (mapping->host && !mapping_tagged(mapping,
page-writeback.c:			sb_clear_inode_writeback(mapping->host);
page-writeback.c:		spin_unlock_irqrestore(&mapping->tree_lock, flags);
page-writeback.c:		struct inode *inode = mapping->host;
page-writeback.c:		spin_lock_irqsave(&mapping->tree_lock, flags);
page-writeback.c:			radix_tree_tag_set(&mapping->page_tree,
page-writeback.c:			if (mapping->host && !on_wblist)
page-writeback.c:				sb_mark_inode_writeback(mapping->host);
page-writeback.c:			radix_tree_tag_clear(&mapping->page_tree,
page-writeback.c:			radix_tree_tag_clear(&mapping->page_tree,
page-writeback.c:		spin_unlock_irqrestore(&mapping->tree_lock, flags);
page-writeback.c:	return radix_tree_tagged(&mapping->page_tree, tag);
page-writeback.c: * wait_for_stable_page() - wait for writeback to finish, if necessary.
page-writeback.c:	if (bdi_cap_stable_pages_required(inode_to_bdi(page->mapping->host)))
userfaultfd.c: *  the COPYING file in the top-level directory.
userfaultfd.c:		ret = -ENOMEM;
userfaultfd.c:			ret = -EFAULT;
userfaultfd.c:	ret = -ENOMEM;
userfaultfd.c:	_dst_pte = mk_pte(page, dst_vma->vm_page_prot);
userfaultfd.c:	if (dst_vma->vm_flags & VM_WRITE)
userfaultfd.c:	ret = -EEXIST;
userfaultfd.c:	/* No need to invalidate - it was non-present before */
userfaultfd.c:					 dst_vma->vm_page_prot));
userfaultfd.c:	ret = -EEXIST;
userfaultfd.c:	/* No need to invalidate - it was non-present before */
userfaultfd.c:	int vm_alloc_shared = dst_vma->vm_flags & VM_SHARED;
userfaultfd.c:	int vm_shared = dst_vma->vm_flags & VM_SHARED;
userfaultfd.c:		up_read(&dst_mm->mmap_sem);
userfaultfd.c:		return -EINVAL;
userfaultfd.c:	err = -EINVAL;
userfaultfd.c:	if (dst_start & (vma_hpagesize - 1) || len & (vma_hpagesize - 1))
userfaultfd.c:		err = -ENOENT;
userfaultfd.c:		if (!dst_vma->vm_userfaultfd_ctx.ctx)
userfaultfd.c:		if (dst_start < dst_vma->vm_start ||
userfaultfd.c:		    dst_start + len > dst_vma->vm_end)
userfaultfd.c:		err = -EINVAL;
userfaultfd.c:		vm_shared = dst_vma->vm_flags & VM_SHARED;
userfaultfd.c:	if (WARN_ON(dst_addr & (vma_hpagesize - 1) ||
userfaultfd.c:		    (len - copied) & (vma_hpagesize - 1)))
userfaultfd.c:	err = -ENOMEM;
userfaultfd.c:		mapping = dst_vma->vm_file->f_mapping;
userfaultfd.c:		err = -ENOMEM;
userfaultfd.c:		err = -EEXIST;
userfaultfd.c:		if (unlikely(err == -EFAULT)) {
userfaultfd.c:			up_read(&dst_mm->mmap_sem);
userfaultfd.c:				err = -EFAULT;
userfaultfd.c:			down_read(&dst_mm->mmap_sem);
userfaultfd.c:				err = -EINTR;
userfaultfd.c:	up_read(&dst_mm->mmap_sem);
userfaultfd.c:	/* Does the address range wrap, or is the span zero-sized? */
userfaultfd.c:	down_read(&dst_mm->mmap_sem);
userfaultfd.c:	err = -ENOENT;
userfaultfd.c:	if (!dst_vma->vm_userfaultfd_ctx.ctx)
userfaultfd.c:	if (dst_start < dst_vma->vm_start ||
userfaultfd.c:	    dst_start + len > dst_vma->vm_end)
userfaultfd.c:	err = -EINVAL;
userfaultfd.c:	    dst_vma->vm_flags & VM_SHARED))
userfaultfd.c:	err = -ENOMEM;
userfaultfd.c:			err = -ENOMEM;
userfaultfd.c:			err = -EEXIST;
userfaultfd.c:			err = -ENOMEM;
userfaultfd.c:			err = -EFAULT;
userfaultfd.c:		if (unlikely(err == -EFAULT)) {
userfaultfd.c:			up_read(&dst_mm->mmap_sem);
userfaultfd.c:				err = -EFAULT;
userfaultfd.c:				err = -EINTR;
userfaultfd.c:	up_read(&dst_mm->mmap_sem);
mmap.c:#include <linux/backing-dev.h>
mmap.c:/* Update vma->vm_page_prot to reflect vma->vm_flags. */
mmap.c:	unsigned long vm_flags = vma->vm_flags;
mmap.c:	vm_page_prot = vm_pgprot_modify(vma->vm_page_prot, vm_flags);
mmap.c:	/* remove_protection_ptes reads vma->vm_page_prot without mmap_sem */
mmap.c:	WRITE_ONCE(vma->vm_page_prot, vm_page_prot);
mmap.c: * Requires inode->i_mapping->i_mmap_rwsem
mmap.c:	if (vma->vm_flags & VM_DENYWRITE)
mmap.c:		atomic_inc(&file_inode(file)->i_writecount);
mmap.c:	if (vma->vm_flags & VM_SHARED)
mmap.c:	vma_interval_tree_remove(vma, &mapping->i_mmap);
mmap.c: * Unlink a file-based vm structure from its interval tree, to hide
mmap.c:	struct file *file = vma->vm_file;
mmap.c:		struct address_space *mapping = file->f_mapping;
mmap.c:	struct vm_area_struct *next = vma->vm_next;
mmap.c:	if (vma->vm_ops && vma->vm_ops->close)
mmap.c:		vma->vm_ops->close(vma);
mmap.c:	if (vma->vm_file)
mmap.c:		fput(vma->vm_file);
mmap.c:	struct mm_struct *mm = current->mm;
mmap.c:	if (down_write_killable(&mm->mmap_sem))
mmap.c:		return -EINTR;
mmap.c:	 * randomize_va_space to 2, which will still cause mm->start_brk
mmap.c:	if (current->brk_randomized)
mmap.c:		min_brk = mm->start_brk;
mmap.c:		min_brk = mm->end_data;
mmap.c:	min_brk = mm->start_brk;
mmap.c:	 * not page aligned -Ram Gupta
mmap.c:	if (check_data_rlimit(rlimit(RLIMIT_DATA), brk, mm->start_brk,
mmap.c:			      mm->end_data, mm->start_data))
mmap.c:	oldbrk = PAGE_ALIGN(mm->brk);
mmap.c:	if (brk <= mm->brk) {
mmap.c:		if (!do_munmap(mm, newbrk, oldbrk-newbrk, &uf))
mmap.c:	/* Ok, looks good - let it rip. */
mmap.c:	if (do_brk_flags(oldbrk, newbrk-oldbrk, 0, &uf) < 0)
mmap.c:	mm->brk = brk;
mmap.c:	populate = newbrk > oldbrk && (mm->def_flags & VM_LOCKED) != 0;
mmap.c:	up_write(&mm->mmap_sem);
mmap.c:		mm_populate(oldbrk, newbrk - oldbrk);
mmap.c:	retval = mm->brk;
mmap.c:	up_write(&mm->mmap_sem);
mmap.c:	if (vma->vm_prev) {
mmap.c:		prev_end = vm_end_gap(vma->vm_prev);
mmap.c:			max -= prev_end;
mmap.c:	if (vma->vm_rb.rb_left) {
mmap.c:		subtree_gap = rb_entry(vma->vm_rb.rb_left,
mmap.c:				struct vm_area_struct, vm_rb)->rb_subtree_gap;
mmap.c:	if (vma->vm_rb.rb_right) {
mmap.c:		subtree_gap = rb_entry(vma->vm_rb.rb_right,
mmap.c:				struct vm_area_struct, vm_rb)->rb_subtree_gap;
mmap.c:	struct rb_root *root = &mm->mm_rb;
mmap.c:		if (vma->vm_start < prev) {
mmap.c:				  vma->vm_start, prev);
mmap.c:		if (vma->vm_start < pend) {
mmap.c:				  vma->vm_start, pend);
mmap.c:		if (vma->vm_start > vma->vm_end) {
mmap.c:				  vma->vm_start, vma->vm_end);
mmap.c:		spin_lock(&mm->page_table_lock);
mmap.c:		if (vma->rb_subtree_gap != vma_compute_subtree_gap(vma)) {
mmap.c:			       vma->rb_subtree_gap,
mmap.c:		spin_unlock(&mm->page_table_lock);
mmap.c:		prev = vma->vm_start;
mmap.c:		pend = vma->vm_end;
mmap.c:	return bug ? -1 : i;
mmap.c:			vma->rb_subtree_gap != vma_compute_subtree_gap(vma),
mmap.c:	struct vm_area_struct *vma = mm->mmap;
mmap.c:		struct anon_vma *anon_vma = vma->anon_vma;
mmap.c:			list_for_each_entry(avc, &vma->anon_vma_chain, same_vma)
mmap.c:		vma = vma->vm_next;
mmap.c:	if (i != mm->map_count) {
mmap.c:		pr_emerg("map_count %d vm_next %d\n", mm->map_count, i);
mmap.c:	if (highest_address != mm->highest_vm_end) {
mmap.c:		pr_emerg("mm->highest_vm_end %lx, found %lx\n",
mmap.c:			  mm->highest_vm_end, highest_address);
mmap.c:	if (i != mm->map_count) {
mmap.c:		if (i != -1)
mmap.c:			pr_emerg("map_count %d rb %d\n", mm->map_count, i);
mmap.c: * Update augmented rbtree rb_subtree_gap values after vma->vm_start or
mmap.c: * vma->vm_prev->vm_end values changed, without modifying the vma's position
mmap.c:	vma_gap_callbacks_propagate(&vma->vm_rb, NULL);
mmap.c:	rb_insert_augmented(&vma->vm_rb, root, &vma_gap_callbacks);
mmap.c:	rb_erase_augmented(&vma->vm_rb, root, &vma_gap_callbacks);
mmap.c:	 * next->vm_start was reduced.
mmap.c:	list_for_each_entry(avc, &vma->anon_vma_chain, same_vma)
mmap.c:		anon_vma_interval_tree_remove(avc, &avc->anon_vma->rb_root);
mmap.c:	list_for_each_entry(avc, &vma->anon_vma_chain, same_vma)
mmap.c:		anon_vma_interval_tree_insert(avc, &avc->anon_vma->rb_root);
mmap.c:	__rb_link = &mm->mm_rb.rb_node;
mmap.c:		if (vma_tmp->vm_end > addr) {
mmap.c:			if (vma_tmp->vm_start < end)
mmap.c:				return -ENOMEM;
mmap.c:			__rb_link = &__rb_parent->rb_left;
mmap.c:			__rb_link = &__rb_parent->rb_right;
mmap.c:	nr_pages = (min(end, vma->vm_end) -
mmap.c:		max(addr, vma->vm_start)) >> PAGE_SHIFT;
mmap.c:	for (vma = vma->vm_next; vma; vma = vma->vm_next) {
mmap.c:		if (vma->vm_start > end)
mmap.c:		overlap_len = min(end, vma->vm_end) - vma->vm_start;
mmap.c:	if (vma->vm_next)
mmap.c:		vma_gap_update(vma->vm_next);
mmap.c:		mm->highest_vm_end = vm_end_gap(vma);
mmap.c:	 * vma->vm_prev wasn't known when we followed the rbtree to find the
mmap.c:	rb_link_node(&vma->vm_rb, rb_parent, rb_link);
mmap.c:	vma->rb_subtree_gap = 0;
mmap.c:	vma_rb_insert(vma, &mm->mm_rb);
mmap.c:	file = vma->vm_file;
mmap.c:		struct address_space *mapping = file->f_mapping;
mmap.c:		if (vma->vm_flags & VM_DENYWRITE)
mmap.c:			atomic_dec(&file_inode(file)->i_writecount);
mmap.c:		if (vma->vm_flags & VM_SHARED)
mmap.c:			atomic_inc(&mapping->i_mmap_writable);
mmap.c:		vma_interval_tree_insert(vma, &mapping->i_mmap);
mmap.c:	if (vma->vm_file) {
mmap.c:		mapping = vma->vm_file->f_mapping;
mmap.c:	mm->map_count++;
mmap.c:	if (find_vma_links(mm, vma->vm_start, vma->vm_end,
mmap.c:	mm->map_count++;
mmap.c:	vma_rb_erase_ignore(vma, &mm->mm_rb, ignore);
mmap.c:	next = vma->vm_next;
mmap.c:		prev->vm_next = next;
mmap.c:		prev = vma->vm_prev;
mmap.c:			prev->vm_next = next;
mmap.c:			mm->mmap = next;
mmap.c:		next->vm_prev = prev;
mmap.c:	struct mm_struct *mm = vma->vm_mm;
mmap.c:	struct vm_area_struct *next = vma->vm_next, *orig_vma = vma;
mmap.c:	struct file *file = vma->vm_file;
mmap.c:		if (end >= next->vm_end) {
mmap.c:				VM_WARN_ON(end != next->vm_end);
mmap.c:				VM_WARN_ON(file != next->vm_file);
mmap.c:				remove_next = 1 + (end > next->vm_end);
mmap.c:					   end != next->vm_next->vm_end);
mmap.c:					   end != next->vm_end);
mmap.c:				end = next->vm_end;
mmap.c:			if (remove_next == 2 && !next->anon_vma)
mmap.c:				exporter = next->vm_next;
mmap.c:		} else if (end > next->vm_start) {
mmap.c:			adjust_next = (end - next->vm_start) >> PAGE_SHIFT;
mmap.c:		} else if (end < vma->vm_end) {
mmap.c:			adjust_next = -((vma->vm_end - end) >> PAGE_SHIFT);
mmap.c:		if (exporter && exporter->anon_vma && !importer->anon_vma) {
mmap.c:			importer->anon_vma = exporter->anon_vma;
mmap.c:		mapping = file->f_mapping;
mmap.c:		root = &mapping->i_mmap;
mmap.c:		uprobe_munmap(vma, vma->vm_start, vma->vm_end);
mmap.c:			uprobe_munmap(next, next->vm_start, next->vm_end);
mmap.c:	anon_vma = vma->anon_vma;
mmap.c:		anon_vma = next->anon_vma;
mmap.c:		VM_WARN_ON(adjust_next && next->anon_vma &&
mmap.c:			   anon_vma != next->anon_vma);
mmap.c:	if (start != vma->vm_start) {
mmap.c:		vma->vm_start = start;
mmap.c:	if (end != vma->vm_end) {
mmap.c:		vma->vm_end = end;
mmap.c:	vma->vm_pgoff = pgoff;
mmap.c:		next->vm_start += adjust_next << PAGE_SHIFT;
mmap.c:		next->vm_pgoff += adjust_next;
mmap.c:			 * pre-swap() next->vm_start was reduced so
mmap.c:			 * tell validate_mm_rb to ignore pre-swap()
mmap.c:			 * "next" (which is stored in post-swap()
mmap.c:				mm->highest_vm_end = vm_end_gap(vma);
mmap.c:			uprobe_munmap(next, next->vm_start, next->vm_end);
mmap.c:		if (next->anon_vma)
mmap.c:		mm->map_count--;
mmap.c:			 * If "next" was removed and vma->vm_end was
mmap.c:			 * "next->vm_prev->vm_end" changed and the
mmap.c:			 * "vma->vm_next" gap must be updated.
mmap.c:			next = vma->vm_next;
mmap.c:			 * "vma" considered pre-swap(): if "vma" was
mmap.c:			 * removed, next->vm_start was expanded (down)
mmap.c:			 * Because of the swap() the post-swap() "vma"
mmap.c:			 * actually points to pre-swap() "next"
mmap.c:			 * (post-swap() "next" as opposed is now a
mmap.c:			end = next->vm_end;
mmap.c:			 * path because pre-swap() next is always not
mmap.c:			 * NULL. pre-swap() "next" is not being
mmap.c:			 * removed and its next->vm_end is not altered
mmap.c:			 * next->vm_end in remove_next == 3).
mmap.c:			 * case next->vm_end == "end" and the extended
mmap.c:			 * "vma" has vma->vm_end == next->vm_end so
mmap.c:			 * mm->highest_vm_end doesn't need any update
mmap.c:			VM_WARN_ON(mm->highest_vm_end != vm_end_gap(vma));
mmap.c: * If the vma has a ->close operation then the driver probably needs to release
mmap.c: * per-vma resources, so we don't attempt to merge those.
mmap.c:	 * match the flags but dirty bit -- the caller should mark
mmap.c:	if ((vma->vm_flags ^ vm_flags) & ~VM_SOFTDIRTY)
mmap.c:	if (vma->vm_file != file)
mmap.c:	if (vma->vm_ops && vma->vm_ops->close)
mmap.c:		list_is_singular(&vma->anon_vma_chain)))
mmap.c: * We cannot merge two vmas if they have differently assigned (non-NULL)
mmap.c: * wrap, nor mmaps which cover the final page at index -1UL.
mmap.c:	    is_mergeable_anon_vma(anon_vma, vma->anon_vma, vma)) {
mmap.c:		if (vma->vm_pgoff == vm_pgoff)
mmap.c: * We cannot merge two vmas if they have differently assigned (non-NULL)
mmap.c:	    is_mergeable_anon_vma(anon_vma, vma->anon_vma, vma)) {
mmap.c:		if (vma->vm_pgoff + vm_pglen == vm_pgoff)
mmap.c: * In most cases - when called for mmap, brk or mremap - [addr,end) is
mmap.c: * this area are about to be changed to vm_flags - and the no-change
mmap.c:	pgoff_t pglen = (end - addr) >> PAGE_SHIFT;
mmap.c:	 * We later require that vma->vm_flags == vm_flags,
mmap.c:	 * so this tests vma->vm_flags & VM_SPECIAL, too.
mmap.c:		next = prev->vm_next;
mmap.c:		next = mm->mmap;
mmap.c:	if (area && area->vm_end == end)		/* cases 6, 7, 8 */
mmap.c:		next = next->vm_next;
mmap.c:	VM_WARN_ON(prev && addr <= prev->vm_start);
mmap.c:	VM_WARN_ON(area && end > area->vm_end);
mmap.c:	if (prev && prev->vm_end == addr &&
mmap.c:		if (next && end == next->vm_start &&
mmap.c:				is_mergeable_anon_vma(prev->anon_vma,
mmap.c:						      next->anon_vma, NULL)) {
mmap.c:			err = __vma_adjust(prev, prev->vm_start,
mmap.c:					 next->vm_end, prev->vm_pgoff, NULL,
mmap.c:			err = __vma_adjust(prev, prev->vm_start,
mmap.c:					 end, prev->vm_pgoff, NULL, prev);
mmap.c:	if (next && end == next->vm_start &&
mmap.c:		if (prev && addr < prev->vm_end)	/* case 4 */
mmap.c:			err = __vma_adjust(prev, prev->vm_start,
mmap.c:					 addr, prev->vm_pgoff, NULL, next);
mmap.c:			err = __vma_adjust(area, addr, next->vm_end,
mmap.c:					 next->vm_pgoff - pglen, NULL, next);
mmap.c: * there is a vm_ops->close() function, because that indicates that the
mmap.c:	return a->vm_end == b->vm_start &&
mmap.c:		a->vm_file == b->vm_file &&
mmap.c:		!((a->vm_flags ^ b->vm_flags) & ~(VM_READ|VM_WRITE|VM_EXEC|VM_SOFTDIRTY)) &&
mmap.c:		b->vm_pgoff == a->vm_pgoff + ((b->vm_start - a->vm_start) >> PAGE_SHIFT);
mmap.c: * Do some basic sanity checking to see if we can re-use the anon_vma
mmap.c: * from 'old'. The 'a'/'b' vma's are in VM order - one of them will be
mmap.c: * we do that READ_ONCE() to make sure that we never re-load the pointer.
mmap.c:		struct anon_vma *anon_vma = READ_ONCE(old->anon_vma);
mmap.c:		if (anon_vma && list_is_singular(&old->anon_vma_chain))
mmap.c:	near = vma->vm_next;
mmap.c:	near = vma->vm_prev;
mmap.c:		locked += mm->locked_vm;
mmap.c:			return -EAGAIN;
mmap.c:	if (S_ISREG(inode->i_mode))
mmap.c:	if (S_ISBLK(inode->i_mode))
mmap.c:	if (file->f_mode & FMODE_UNSIGNED_OFFSET)
mmap.c:	maxsize -= len;
mmap.c: * The caller must hold down_write(&current->mm->mmap_sem).
mmap.c:	struct mm_struct *mm = current->mm;
mmap.c:		return -EINVAL;
mmap.c:	if ((prot & PROT_READ) && (current->personality & READ_IMPLIES_EXEC))
mmap.c:		if (!(file && path_noexec(&file->f_path)))
mmap.c:		return -ENOMEM;
mmap.c:		return -EOVERFLOW;
mmap.c:	if (mm->map_count > sysctl_max_map_count)
mmap.c:		return -ENOMEM;
mmap.c:	/* Do simple checking here so the lower-level routines won't have
mmap.c:			mm->def_flags | VM_MAYREAD | VM_MAYWRITE | VM_MAYEXEC;
mmap.c:			return -EPERM;
mmap.c:		return -EAGAIN;
mmap.c:			return -EOVERFLOW;
mmap.c:			if ((prot&PROT_WRITE) && !(file->f_mode&FMODE_WRITE))
mmap.c:				return -EACCES;
mmap.c:			 * Make sure we don't allow writing to an append-only
mmap.c:			if (IS_APPEND(inode) && (file->f_mode & FMODE_WRITE))
mmap.c:				return -EACCES;
mmap.c:				return -EAGAIN;
mmap.c:			if (!(file->f_mode & FMODE_WRITE))
mmap.c:			if (!(file->f_mode & FMODE_READ))
mmap.c:				return -EACCES;
mmap.c:			if (path_noexec(&file->f_path)) {
mmap.c:					return -EPERM;
mmap.c:			if (!file->f_op->mmap)
mmap.c:				return -ENODEV;
mmap.c:				return -EINVAL;
mmap.c:			return -EINVAL;
mmap.c:				return -EINVAL;
mmap.c:			return -EINVAL;
mmap.c:			return -EBADF;
mmap.c:		retval = -EINVAL;
mmap.c:			return -EINVAL;
mmap.c:		 * taken when vm_ops->mmap() is called
mmap.c:		return -EFAULT;
mmap.c:		return -EINVAL;
mmap.c: * Some shared mappigns will want the pages marked read-only
mmap.c:	vm_flags_t vm_flags = vma->vm_flags;
mmap.c:	const struct vm_operations_struct *vm_ops = vma->vm_ops;
mmap.c:	/* If it was private or non-writable, the write bit is already clear */
mmap.c:	if (vm_ops && (vm_ops->page_mkwrite || vm_ops->pfn_mkwrite))
mmap.c:	return vma->vm_file && vma->vm_file->f_mapping &&
mmap.c:		mapping_cap_account_dirty(vma->vm_file->f_mapping);
mmap.c:	struct mm_struct *mm = current->mm;
mmap.c:					(len >> PAGE_SHIFT) - nr_pages))
mmap.c:			return -ENOMEM;
mmap.c:			return -ENOMEM;
mmap.c:			return -ENOMEM;
mmap.c:		error = -ENOMEM;
mmap.c:	vma->vm_mm = mm;
mmap.c:	vma->vm_start = addr;
mmap.c:	vma->vm_end = addr + len;
mmap.c:	vma->vm_flags = vm_flags;
mmap.c:	vma->vm_page_prot = vm_get_page_prot(vm_flags);
mmap.c:	vma->vm_pgoff = pgoff;
mmap.c:	INIT_LIST_HEAD(&vma->anon_vma_chain);
mmap.c:			error = mapping_map_writable(file->f_mapping);
mmap.c:		/* ->mmap() can change vma->vm_file, but must guarantee that
mmap.c:		 * vma_link() below can deny write-access if VM_DENYWRITE is set
mmap.c:		 * new file must not have been exposed to user-space, yet.
mmap.c:		vma->vm_file = get_file(file);
mmap.c:		 *         f_op->mmap method. -DaveM
mmap.c:		WARN_ON_ONCE(addr != vma->vm_start);
mmap.c:		addr = vma->vm_start;
mmap.c:		vm_flags = vma->vm_flags;
mmap.c:			mapping_unmap_writable(file->f_mapping);
mmap.c:	file = vma->vm_file;
mmap.c:					vma == get_gate_vma(current->mm)))
mmap.c:			mm->locked_vm += (len >> PAGE_SHIFT);
mmap.c:			vma->vm_flags &= VM_LOCKED_CLEAR_MASK;
mmap.c:	 * Otherwise user-space soft-dirty page tracker won't
mmap.c:	 * then new mapped in-place (which must be aimed as
mmap.c:	vma->vm_flags |= VM_SOFTDIRTY;
mmap.c:	vma->vm_file = NULL;
mmap.c:	unmap_region(mm, vma, prev, vma->vm_start, vma->vm_end);
mmap.c:		mapping_unmap_writable(file->f_mapping);
mmap.c:	 * - gap_start = vma->vm_prev->vm_end <= info->high_limit - length;
mmap.c:	 * - gap_end   = vma->vm_start        >= info->low_limit  + length;
mmap.c:	 * - gap_end - gap_start >= length
mmap.c:	struct mm_struct *mm = current->mm;
mmap.c:	length = info->length + info->align_mask;
mmap.c:	if (length < info->length)
mmap.c:		return -ENOMEM;
mmap.c:	if (info->high_limit < length)
mmap.c:		return -ENOMEM;
mmap.c:	high_limit = info->high_limit - length;
mmap.c:	if (info->low_limit > high_limit)
mmap.c:		return -ENOMEM;
mmap.c:	low_limit = info->low_limit + length;
mmap.c:	if (RB_EMPTY_ROOT(&mm->mm_rb))
mmap.c:	vma = rb_entry(mm->mm_rb.rb_node, struct vm_area_struct, vm_rb);
mmap.c:	if (vma->rb_subtree_gap < length)
mmap.c:		if (gap_end >= low_limit && vma->vm_rb.rb_left) {
mmap.c:				rb_entry(vma->vm_rb.rb_left,
mmap.c:			if (left->rb_subtree_gap >= length) {
mmap.c:		gap_start = vma->vm_prev ? vm_end_gap(vma->vm_prev) : 0;
mmap.c:			return -ENOMEM;
mmap.c:		    gap_end > gap_start && gap_end - gap_start >= length)
mmap.c:		if (vma->vm_rb.rb_right) {
mmap.c:				rb_entry(vma->vm_rb.rb_right,
mmap.c:			if (right->rb_subtree_gap >= length) {
mmap.c:			struct rb_node *prev = &vma->vm_rb;
mmap.c:			if (prev == vma->vm_rb.rb_left) {
mmap.c:				gap_start = vm_end_gap(vma->vm_prev);
mmap.c:	gap_start = mm->highest_vm_end;
mmap.c:		return -ENOMEM;
mmap.c:	if (gap_start < info->low_limit)
mmap.c:		gap_start = info->low_limit;
mmap.c:	gap_start += (info->align_offset - gap_start) & info->align_mask;
mmap.c:	VM_BUG_ON(gap_start + info->length > info->high_limit);
mmap.c:	VM_BUG_ON(gap_start + info->length > gap_end);
mmap.c:	struct mm_struct *mm = current->mm;
mmap.c:	length = info->length + info->align_mask;
mmap.c:	if (length < info->length)
mmap.c:		return -ENOMEM;
mmap.c:	gap_end = info->high_limit;
mmap.c:		return -ENOMEM;
mmap.c:	high_limit = gap_end - length;
mmap.c:	if (info->low_limit > high_limit)
mmap.c:		return -ENOMEM;
mmap.c:	low_limit = info->low_limit + length;
mmap.c:	gap_start = mm->highest_vm_end;
mmap.c:	if (RB_EMPTY_ROOT(&mm->mm_rb))
mmap.c:		return -ENOMEM;
mmap.c:	vma = rb_entry(mm->mm_rb.rb_node, struct vm_area_struct, vm_rb);
mmap.c:	if (vma->rb_subtree_gap < length)
mmap.c:		return -ENOMEM;
mmap.c:		gap_start = vma->vm_prev ? vm_end_gap(vma->vm_prev) : 0;
mmap.c:		if (gap_start <= high_limit && vma->vm_rb.rb_right) {
mmap.c:				rb_entry(vma->vm_rb.rb_right,
mmap.c:			if (right->rb_subtree_gap >= length) {
mmap.c:			return -ENOMEM;
mmap.c:		    gap_end > gap_start && gap_end - gap_start >= length)
mmap.c:		if (vma->vm_rb.rb_left) {
mmap.c:				rb_entry(vma->vm_rb.rb_left,
mmap.c:			if (left->rb_subtree_gap >= length) {
mmap.c:			struct rb_node *prev = &vma->vm_rb;
mmap.c:				return -ENOMEM;
mmap.c:			if (prev == vma->vm_rb.rb_right) {
mmap.c:				gap_start = vma->vm_prev ?
mmap.c:					vm_end_gap(vma->vm_prev) : 0;
mmap.c:	if (gap_end > info->high_limit)
mmap.c:		gap_end = info->high_limit;
mmap.c:	gap_end -= info->length;
mmap.c:	gap_end -= (gap_end - info->align_offset) & info->align_mask;
mmap.c:	VM_BUG_ON(gap_end < info->low_limit);
mmap.c: * This function "knows" that -ENOMEM has the bits set.
mmap.c:	struct mm_struct *mm = current->mm;
mmap.c:	if (len > TASK_SIZE - mmap_min_addr)
mmap.c:		return -ENOMEM;
mmap.c:		if (TASK_SIZE - len >= addr && addr >= mmap_min_addr &&
mmap.c:	info.low_limit = mm->mmap_base;
mmap.c: * This mmap-allocator allocates new areas top-down from below the
mmap.c:	struct mm_struct *mm = current->mm;
mmap.c:	if (len > TASK_SIZE - mmap_min_addr)
mmap.c:		return -ENOMEM;
mmap.c:		if (TASK_SIZE - len >= addr && addr >= mmap_min_addr &&
mmap.c:	info.high_limit = mm->mmap_base;
mmap.c:	 * so fall back to the bottom-up function here. This scenario
mmap.c:		VM_BUG_ON(addr != -ENOMEM);
mmap.c:		return -ENOMEM;
mmap.c:	get_area = current->mm->get_unmapped_area;
mmap.c:		if (file->f_op->get_unmapped_area)
mmap.c:			get_area = file->f_op->get_unmapped_area;
mmap.c:	if (addr > TASK_SIZE - len)
mmap.c:		return -ENOMEM;
mmap.c:		return -EINVAL;
mmap.c:	rb_node = mm->mm_rb.rb_node;
mmap.c:		if (tmp->vm_end > addr) {
mmap.c:			if (tmp->vm_start <= addr)
mmap.c:			rb_node = rb_node->rb_left;
mmap.c:			rb_node = rb_node->rb_right;
mmap.c:		*pprev = vma->vm_prev;
mmap.c:		struct rb_node *rb_node = mm->mm_rb.rb_node;
mmap.c:			rb_node = rb_node->rb_right;
mmap.c: * grow-up and grow-down cases.
mmap.c:	struct mm_struct *mm = vma->vm_mm;
mmap.c:	if (!may_expand_vm(mm, vma->vm_flags, grow))
mmap.c:		return -ENOMEM;
mmap.c:		return -ENOMEM;
mmap.c:	if (vma->vm_flags & VM_LOCKED) {
mmap.c:		locked = mm->locked_vm + grow;
mmap.c:			return -ENOMEM;
mmap.c:	/* Check to ensure the stack will not grow into a hugetlb-only region */
mmap.c:	new_start = (vma->vm_flags & VM_GROWSUP) ? vma->vm_start :
mmap.c:			vma->vm_end - size;
mmap.c:	if (is_hugepage_only_range(vma->vm_mm, new_start, size))
mmap.c:		return -EFAULT;
mmap.c:		return -ENOMEM;
mmap.c: * PA-RISC uses this for its stack; IA64 for its Register Backing Store.
mmap.c: * vma is the last one with address > vma->vm_end.  Have to extend vma.
mmap.c:	struct mm_struct *mm = vma->vm_mm;
mmap.c:	if (!(vma->vm_flags & VM_GROWSUP))
mmap.c:		return -EFAULT;
mmap.c:		return -ENOMEM;
mmap.c:	next = vma->vm_next;
mmap.c:	if (next && next->vm_start < gap_addr &&
mmap.c:			(next->vm_flags & (VM_WRITE|VM_READ|VM_EXEC))) {
mmap.c:		if (!(next->vm_flags & VM_GROWSUP))
mmap.c:			return -ENOMEM;
mmap.c:		return -ENOMEM;
mmap.c:	 * vma->vm_start/vm_end cannot change under us because the caller
mmap.c:	anon_vma_lock_write(vma->anon_vma);
mmap.c:	if (address > vma->vm_end) {
mmap.c:		size = address - vma->vm_start;
mmap.c:		grow = (address - vma->vm_end) >> PAGE_SHIFT;
mmap.c:		error = -ENOMEM;
mmap.c:		if (vma->vm_pgoff + (size >> PAGE_SHIFT) >= vma->vm_pgoff) {
mmap.c:				 * So, we reuse mm->page_table_lock to guard
mmap.c:				spin_lock(&mm->page_table_lock);
mmap.c:				if (vma->vm_flags & VM_LOCKED)
mmap.c:					mm->locked_vm += grow;
mmap.c:				vm_stat_account(mm, vma->vm_flags, grow);
mmap.c:				vma->vm_end = address;
mmap.c:				if (vma->vm_next)
mmap.c:					vma_gap_update(vma->vm_next);
mmap.c:					mm->highest_vm_end = vm_end_gap(vma);
mmap.c:				spin_unlock(&mm->page_table_lock);
mmap.c:	anon_vma_unlock_write(vma->anon_vma);
mmap.c:	khugepaged_enter_vma_merge(vma, vma->vm_flags);
mmap.c: * vma is the first one with address < vma->vm_start.  Have to extend vma.
mmap.c:	struct mm_struct *mm = vma->vm_mm;
mmap.c:	prev = vma->vm_prev;
mmap.c:	if (prev && !(prev->vm_flags & VM_GROWSDOWN) &&
mmap.c:			(prev->vm_flags & (VM_WRITE|VM_READ|VM_EXEC))) {
mmap.c:		if (address - prev->vm_end < stack_guard_gap)
mmap.c:			return -ENOMEM;
mmap.c:		return -ENOMEM;
mmap.c:	 * vma->vm_start/vm_end cannot change under us because the caller
mmap.c:	anon_vma_lock_write(vma->anon_vma);
mmap.c:	if (address < vma->vm_start) {
mmap.c:		size = vma->vm_end - address;
mmap.c:		grow = (vma->vm_start - address) >> PAGE_SHIFT;
mmap.c:		error = -ENOMEM;
mmap.c:		if (grow <= vma->vm_pgoff) {
mmap.c:				 * So, we reuse mm->page_table_lock to guard
mmap.c:				spin_lock(&mm->page_table_lock);
mmap.c:				if (vma->vm_flags & VM_LOCKED)
mmap.c:					mm->locked_vm += grow;
mmap.c:				vm_stat_account(mm, vma->vm_flags, grow);
mmap.c:				vma->vm_start = address;
mmap.c:				vma->vm_pgoff -= grow;
mmap.c:				spin_unlock(&mm->page_table_lock);
mmap.c:	anon_vma_unlock_write(vma->anon_vma);
mmap.c:	khugepaged_enter_vma_merge(vma, vma->vm_flags);
mmap.c:	if (vma && (vma->vm_start <= addr))
mmap.c:	if (prev->vm_flags & VM_LOCKED)
mmap.c:		populate_vma_page_range(prev, addr, prev->vm_end, NULL);
mmap.c:	if (vma->vm_start <= addr)
mmap.c:	if (!(vma->vm_flags & VM_GROWSDOWN))
mmap.c:	start = vma->vm_start;
mmap.c:	if (vma->vm_flags & VM_LOCKED)
mmap.c: * Ok - we have the memory areas we should free on the vma list,
mmap.c:		if (vma->vm_flags & VM_ACCOUNT)
mmap.c:		vm_stat_account(mm, vma->vm_flags, -nrpages);
mmap.c:	struct vm_area_struct *next = prev ? prev->vm_next : mm->mmap;
mmap.c:	free_pgtables(&tlb, vma, prev ? prev->vm_end : FIRST_USER_ADDRESS,
mmap.c:				 next ? next->vm_start : USER_PGTABLES_CEILING);
mmap.c:	insertion_point = (prev ? &prev->vm_next : &mm->mmap);
mmap.c:	vma->vm_prev = NULL;
mmap.c:		vma_rb_erase(vma, &mm->mm_rb);
mmap.c:		mm->map_count--;
mmap.c:		vma = vma->vm_next;
mmap.c:	} while (vma && vma->vm_start < end);
mmap.c:		vma->vm_prev = prev;
mmap.c:		mm->highest_vm_end = prev ? vm_end_gap(prev) : 0;
mmap.c:	tail_vma->vm_next = NULL;
mmap.c:	if (vma->vm_ops && vma->vm_ops->split) {
mmap.c:		err = vma->vm_ops->split(vma, addr);
mmap.c:		return -ENOMEM;
mmap.c:	INIT_LIST_HEAD(&new->anon_vma_chain);
mmap.c:		new->vm_end = addr;
mmap.c:		new->vm_start = addr;
mmap.c:		new->vm_pgoff += ((addr - vma->vm_start) >> PAGE_SHIFT);
mmap.c:	if (new->vm_file)
mmap.c:		get_file(new->vm_file);
mmap.c:	if (new->vm_ops && new->vm_ops->open)
mmap.c:		new->vm_ops->open(new);
mmap.c:		err = vma_adjust(vma, addr, vma->vm_end, vma->vm_pgoff +
mmap.c:			((addr - new->vm_start) >> PAGE_SHIFT), new);
mmap.c:		err = vma_adjust(vma, vma->vm_start, addr, vma->vm_pgoff, new);
mmap.c:	if (new->vm_ops && new->vm_ops->close)
mmap.c:		new->vm_ops->close(new);
mmap.c:	if (new->vm_file)
mmap.c:		fput(new->vm_file);
mmap.c:	if (mm->map_count >= sysctl_max_map_count)
mmap.c:		return -ENOMEM;
mmap.c:/* Munmap is split into 2 main parts -- this part which finds
mmap.c:	if ((offset_in_page(start)) || start > TASK_SIZE || len > TASK_SIZE-start)
mmap.c:		return -EINVAL;
mmap.c:		return -EINVAL;
mmap.c:	prev = vma->vm_prev;
mmap.c:	/* we have  start < vma->vm_end  */
mmap.c:	if (vma->vm_start >= end)
mmap.c:	if (start > vma->vm_start) {
mmap.c:		if (end < vma->vm_end && mm->map_count >= sysctl_max_map_count)
mmap.c:			return -ENOMEM;
mmap.c:	if (last && end > last->vm_start) {
mmap.c:	vma = prev ? prev->vm_next : mm->mmap;
mmap.c:	if (mm->locked_vm) {
mmap.c:		while (tmp && tmp->vm_start < end) {
mmap.c:			if (tmp->vm_flags & VM_LOCKED) {
mmap.c:				mm->locked_vm -= vma_pages(tmp);
mmap.c:			tmp = tmp->vm_next;
mmap.c:	struct mm_struct *mm = current->mm;
mmap.c:	if (down_write_killable(&mm->mmap_sem))
mmap.c:		return -EINTR;
mmap.c:	up_write(&mm->mmap_sem);
mmap.c:	struct mm_struct *mm = current->mm;
mmap.c:	unsigned long ret = -EINVAL;
mmap.c:		     current->comm, current->pid);
mmap.c:	if (down_write_killable(&mm->mmap_sem))
mmap.c:		return -EINTR;
mmap.c:	if (!vma || !(vma->vm_flags & VM_SHARED))
mmap.c:	if (start < vma->vm_start)
mmap.c:	if (start + size > vma->vm_end) {
mmap.c:		for (next = vma->vm_next; next; next = next->vm_next) {
mmap.c:			if (next->vm_start != next->vm_prev->vm_end)
mmap.c:			if (next->vm_file != vma->vm_file)
mmap.c:			if (next->vm_flags != vma->vm_flags)
mmap.c:			if (start + size <= next->vm_end)
mmap.c:	prot |= vma->vm_flags & VM_READ ? PROT_READ : 0;
mmap.c:	prot |= vma->vm_flags & VM_WRITE ? PROT_WRITE : 0;
mmap.c:	prot |= vma->vm_flags & VM_EXEC ? PROT_EXEC : 0;
mmap.c:	if (vma->vm_flags & VM_LOCKED) {
mmap.c:		/* drop PG_Mlocked flag for over-mapped range */
mmap.c:		for (tmp = vma; tmp->vm_start >= start + size;
mmap.c:				tmp = tmp->vm_next) {
mmap.c:					max(tmp->vm_start, start),
mmap.c:					min(tmp->vm_end, start + size));
mmap.c:	file = get_file(vma->vm_file);
mmap.c:	ret = do_mmap_pgoff(vma->vm_file, start, size,
mmap.c:	up_write(&mm->mmap_sem);
mmap.c:	if (unlikely(down_read_trylock(&mm->mmap_sem))) {
mmap.c:		up_read(&mm->mmap_sem);
mmap.c: *  brk-specific accounting here.
mmap.c:	struct mm_struct *mm = current->mm;
mmap.c:		return -EINVAL;
mmap.c:	flags |= VM_DATA_DEFAULT_FLAGS | VM_ACCOUNT | mm->def_flags;
mmap.c:	error = mlock_future_check(mm, mm->def_flags, len);
mmap.c:	 * mm->mmap_sem is required to protect against another thread
mmap.c:			return -ENOMEM;
mmap.c:		return -ENOMEM;
mmap.c:	if (mm->map_count > sysctl_max_map_count)
mmap.c:		return -ENOMEM;
mmap.c:		return -ENOMEM;
mmap.c:		return -ENOMEM;
mmap.c:	INIT_LIST_HEAD(&vma->anon_vma_chain);
mmap.c:	vma->vm_mm = mm;
mmap.c:	vma->vm_start = addr;
mmap.c:	vma->vm_end = addr + len;
mmap.c:	vma->vm_pgoff = pgoff;
mmap.c:	vma->vm_flags = flags;
mmap.c:	vma->vm_page_prot = vm_get_page_prot(flags);
mmap.c:	mm->total_vm += len >> PAGE_SHIFT;
mmap.c:	mm->data_vm += len >> PAGE_SHIFT;
mmap.c:		mm->locked_vm += (len >> PAGE_SHIFT);
mmap.c:	vma->vm_flags |= VM_SOFTDIRTY;
mmap.c:	struct mm_struct *mm = current->mm;
mmap.c:		return -ENOMEM;
mmap.c:	if (down_write_killable(&mm->mmap_sem))
mmap.c:		return -EINTR;
mmap.c:	populate = ((mm->def_flags & VM_LOCKED) != 0);
mmap.c:	up_write(&mm->mmap_sem);
mmap.c:		 * this mm from further consideration.  Taking mm->mmap_sem for
mmap.c:		 * Nothing can be holding mm->mmap_sem here and the above call
mmap.c:		set_bit(MMF_OOM_SKIP, &mm->flags);
mmap.c:		down_write(&mm->mmap_sem);
mmap.c:		up_write(&mm->mmap_sem);
mmap.c:	if (mm->locked_vm) {
mmap.c:		vma = mm->mmap;
mmap.c:			if (vma->vm_flags & VM_LOCKED)
mmap.c:			vma = vma->vm_next;
mmap.c:	vma = mm->mmap;
mmap.c:	tlb_gather_mmu(&tlb, mm, 0, -1);
mmap.c:	/* Use -1 here to ensure all VMAs in the mm are unmapped */
mmap.c:	unmap_vmas(&tlb, vma, 0, -1);
mmap.c:	tlb_finish_mmu(&tlb, 0, -1);
mmap.c:		if (vma->vm_flags & VM_ACCOUNT)
mmap.c: * and into the inode's i_mmap tree.  If vm_file is non-NULL
mmap.c:	if (find_vma_links(mm, vma->vm_start, vma->vm_end,
mmap.c:		return -ENOMEM;
mmap.c:	if ((vma->vm_flags & VM_ACCOUNT) &&
mmap.c:		return -ENOMEM;
mmap.c:		BUG_ON(vma->anon_vma);
mmap.c:		vma->vm_pgoff = vma->vm_start >> PAGE_SHIFT;
mmap.c:	unsigned long vma_start = vma->vm_start;
mmap.c:	struct mm_struct *mm = vma->vm_mm;
mmap.c:	if (unlikely(vma_is_anonymous(vma) && !vma->anon_vma)) {
mmap.c:	new_vma = vma_merge(mm, prev, addr, addr + len, vma->vm_flags,
mmap.c:			    vma->anon_vma, vma->vm_file, pgoff, vma_policy(vma),
mmap.c:			    vma->vm_userfaultfd_ctx);
mmap.c:		if (unlikely(vma_start >= new_vma->vm_start &&
mmap.c:			     vma_start < new_vma->vm_end)) {
mmap.c:			 * reset the dst vma->vm_pgoff to the
mmap.c:		*need_rmap_locks = (new_vma->vm_pgoff <= vma->vm_pgoff);
mmap.c:		new_vma->vm_start = addr;
mmap.c:		new_vma->vm_end = addr + len;
mmap.c:		new_vma->vm_pgoff = pgoff;
mmap.c:		INIT_LIST_HEAD(&new_vma->anon_vma_chain);
mmap.c:		if (new_vma->vm_file)
mmap.c:			get_file(new_vma->vm_file);
mmap.c:		if (new_vma->vm_ops && new_vma->vm_ops->open)
mmap.c:			new_vma->vm_ops->open(new_vma);
mmap.c:	if (mm->total_vm + npages > rlimit(RLIMIT_AS) >> PAGE_SHIFT)
mmap.c:	    mm->data_vm + npages > rlimit(RLIMIT_DATA) >> PAGE_SHIFT) {
mmap.c:		    mm->data_vm + npages <= rlimit_max(RLIMIT_DATA) >> PAGE_SHIFT)
mmap.c:				     current->comm, current->pid,
mmap.c:				     (mm->data_vm + npages) << PAGE_SHIFT,
mmap.c:	mm->total_vm += npages;
mmap.c:		mm->exec_vm += npages;
mmap.c:		mm->stack_vm += npages;
mmap.c:		mm->data_vm += npages;
mmap.c:	return ((struct vm_special_mapping *)vma->vm_private_data)->name;
mmap.c:	struct vm_special_mapping *sm = new_vma->vm_private_data;
mmap.c:	if (WARN_ON_ONCE(current->mm != new_vma->vm_mm))
mmap.c:		return -EFAULT;
mmap.c:	if (sm->mremap)
mmap.c:		return sm->mremap(sm, new_vma);
mmap.c:	struct vm_area_struct *vma = vmf->vma;
mmap.c:	if (vma->vm_ops == &legacy_special_mapping_vmops) {
mmap.c:		pages = vma->vm_private_data;
mmap.c:		struct vm_special_mapping *sm = vma->vm_private_data;
mmap.c:		if (sm->fault)
mmap.c:			return sm->fault(sm, vmf->vma, vmf);
mmap.c:		pages = sm->pages;
mmap.c:	for (pgoff = vmf->pgoff; pgoff && *pages; ++pages)
mmap.c:		pgoff--;
mmap.c:		vmf->page = page;
mmap.c:		return ERR_PTR(-ENOMEM);
mmap.c:	INIT_LIST_HEAD(&vma->anon_vma_chain);
mmap.c:	vma->vm_mm = mm;
mmap.c:	vma->vm_start = addr;
mmap.c:	vma->vm_end = addr + len;
mmap.c:	vma->vm_flags = vm_flags | mm->def_flags | VM_DONTEXPAND | VM_SOFTDIRTY;
mmap.c:	vma->vm_page_prot = vm_get_page_prot(vma->vm_flags);
mmap.c:	vma->vm_ops = ops;
mmap.c:	vma->vm_private_data = priv;
mmap.c:	vm_stat_account(mm, vma->vm_flags, len >> PAGE_SHIFT);
mmap.c:	return vma->vm_private_data == sm &&
mmap.c:		(vma->vm_ops == &special_mapping_vmops ||
mmap.c:		 vma->vm_ops == &legacy_special_mapping_vmops);
mmap.c: * Called with mm->mmap_sem held for writing.
mmap.c: * The array can be shorter than len >> PAGE_SHIFT if it's null-terminated.
mmap.c:	if (!test_bit(0, (unsigned long *) &anon_vma->root->rb_root.rb_root.rb_node)) {
mmap.c:		down_write_nest_lock(&anon_vma->root->rwsem, &mm->mmap_sem);
mmap.c:		 * anon_vma->root->rwsem. If some other vma in this mm shares
mmap.c:		 * anon_vma->root->rwsem.
mmap.c:				       &anon_vma->root->rb_root.rb_root.rb_node))
mmap.c:	if (!test_bit(AS_MM_ALL_LOCKS, &mapping->flags)) {
mmap.c:		 * Operations on ->flags have to be atomic because
mmap.c:		if (test_and_set_bit(AS_MM_ALL_LOCKS, &mapping->flags))
mmap.c:		down_write_nest_lock(&mapping->i_mmap_rwsem, &mm->mmap_sem);
mmap.c: * The LSB in anon_vma->rb_root.rb_node and the AS_MM_ALL_LOCKS bitflag in
mmap.c: * mapping->flags avoid to take the same lock twice, if more than one
mmap.c: *   - all hugetlbfs_i_mmap_rwsem_key locks (aka mapping->i_mmap_rwsem for
mmap.c: *   - all i_mmap_rwsem locks;
mmap.c: *   - all anon_vma->rwseml
mmap.c:	BUG_ON(down_read_trylock(&mm->mmap_sem));
mmap.c:	for (vma = mm->mmap; vma; vma = vma->vm_next) {
mmap.c:		if (vma->vm_file && vma->vm_file->f_mapping &&
mmap.c:			vm_lock_mapping(mm, vma->vm_file->f_mapping);
mmap.c:	for (vma = mm->mmap; vma; vma = vma->vm_next) {
mmap.c:		if (vma->vm_file && vma->vm_file->f_mapping &&
mmap.c:			vm_lock_mapping(mm, vma->vm_file->f_mapping);
mmap.c:	for (vma = mm->mmap; vma; vma = vma->vm_next) {
mmap.c:		if (vma->anon_vma)
mmap.c:			list_for_each_entry(avc, &vma->anon_vma_chain, same_vma)
mmap.c:				vm_lock_anon_vma(mm, avc->anon_vma);
mmap.c:	return -EINTR;
mmap.c:	if (test_bit(0, (unsigned long *) &anon_vma->root->rb_root.rb_root.rb_node)) {
mmap.c:		 * the vma so the users using the anon_vma->rb_root will
mmap.c:		 * anon_vma->root->rwsem.
mmap.c:					  &anon_vma->root->rb_root.rb_root.rb_node))
mmap.c:	if (test_bit(AS_MM_ALL_LOCKS, &mapping->flags)) {
mmap.c:					&mapping->flags))
mmap.c:	BUG_ON(down_read_trylock(&mm->mmap_sem));
mmap.c:	for (vma = mm->mmap; vma; vma = vma->vm_next) {
mmap.c:		if (vma->anon_vma)
mmap.c:			list_for_each_entry(avc, &vma->anon_vma_chain, same_vma)
mmap.c:				vm_unlock_anon_vma(avc->anon_vma);
mmap.c:		if (vma->vm_file && vma->vm_file->f_mapping)
mmap.c:			vm_unlock_mapping(vma->vm_file->f_mapping);
mmap.c:	free_kbytes = global_zone_page_state(NR_FREE_PAGES) << (PAGE_SHIFT - 10);
mmap.c:	free_kbytes = global_zone_page_state(NR_FREE_PAGES) << (PAGE_SHIFT - 10);
mmap.c:		free_kbytes = global_zone_page_state(NR_FREE_PAGES) << (PAGE_SHIFT - 10);
backing-dev.c:#include <linux/backing-dev.h>
backing-dev.c:	struct backing_dev_info *bdi = m->private;
backing-dev.c:	struct bdi_writeback *wb = &bdi->wb;
backing-dev.c:	spin_lock(&wb->list_lock);
backing-dev.c:	list_for_each_entry(inode, &wb->b_dirty, i_io_list)
backing-dev.c:	list_for_each_entry(inode, &wb->b_io, i_io_list)
backing-dev.c:	list_for_each_entry(inode, &wb->b_more_io, i_io_list)
backing-dev.c:	list_for_each_entry(inode, &wb->b_dirty_time, i_io_list)
backing-dev.c:		if (inode->i_state & I_DIRTY_TIME)
backing-dev.c:	spin_unlock(&wb->list_lock);
backing-dev.c:#define K(x) ((x) << (PAGE_SHIFT - 10))
backing-dev.c:		   (unsigned long) K(wb->write_bandwidth),
backing-dev.c:		   !list_empty(&bdi->bdi_list), bdi->wb.state);
backing-dev.c:	return single_open(file, bdi_debug_stats_show, inode->i_private);
backing-dev.c:	bdi->debug_dir = debugfs_create_dir(name, bdi_debug_root);
backing-dev.c:	bdi->debug_stats = debugfs_create_file("stats", 0444, bdi->debug_dir,
backing-dev.c:	debugfs_remove(bdi->debug_stats);
backing-dev.c:	debugfs_remove(bdi->debug_dir);
backing-dev.c:	bdi->ra_pages = read_ahead_kb >> (PAGE_SHIFT - 10);
backing-dev.c:#define K(pages) ((pages) << (PAGE_SHIFT - 10))
backing-dev.c:	return snprintf(page, PAGE_SIZE-1, "%lld\n", (long long)expr);	\
backing-dev.c:BDI_SHOW(read_ahead_kb, K(bdi->ra_pages))
backing-dev.c:BDI_SHOW(min_ratio, bdi->min_ratio)
backing-dev.c:BDI_SHOW(max_ratio, bdi->max_ratio)
backing-dev.c:	return snprintf(page, PAGE_SIZE-1, "%d\n",
backing-dev.c:	bdi_class->dev_groups = bdi_dev_groups;
backing-dev.c:		return -ENOMEM;
backing-dev.c: * wakes-up the corresponding bdi thread which should then take care of the
backing-dev.c: * periodic background write-out of dirty inodes. Since the write-out would
backing-dev.c: * fast-path (used by '__mark_inode_dirty()'), so we save few context switches
backing-dev.c: * by delaying the wake-up.
backing-dev.c:	spin_lock_bh(&wb->work_lock);
backing-dev.c:	if (test_bit(WB_registered, &wb->state))
backing-dev.c:		queue_delayed_work(bdi_wq, &wb->dwork, timeout);
backing-dev.c:	spin_unlock_bh(&wb->work_lock);
backing-dev.c:#define INIT_BW		(100 << (20 - PAGE_SHIFT))
backing-dev.c:	if (wb != &bdi->wb)
backing-dev.c:	wb->bdi = bdi;
backing-dev.c:	wb->last_old_flush = jiffies;
backing-dev.c:	INIT_LIST_HEAD(&wb->b_dirty);
backing-dev.c:	INIT_LIST_HEAD(&wb->b_io);
backing-dev.c:	INIT_LIST_HEAD(&wb->b_more_io);
backing-dev.c:	INIT_LIST_HEAD(&wb->b_dirty_time);
backing-dev.c:	spin_lock_init(&wb->list_lock);
backing-dev.c:	wb->bw_time_stamp = jiffies;
backing-dev.c:	wb->balanced_dirty_ratelimit = INIT_BW;
backing-dev.c:	wb->dirty_ratelimit = INIT_BW;
backing-dev.c:	wb->write_bandwidth = INIT_BW;
backing-dev.c:	wb->avg_write_bandwidth = INIT_BW;
backing-dev.c:	spin_lock_init(&wb->work_lock);
backing-dev.c:	INIT_LIST_HEAD(&wb->work_list);
backing-dev.c:	INIT_DELAYED_WORK(&wb->dwork, wb_workfn);
backing-dev.c:	wb->dirty_sleep = jiffies;
backing-dev.c:	wb->congested = wb_congested_get_create(bdi, blkcg_id, gfp);
backing-dev.c:	if (!wb->congested) {
backing-dev.c:		err = -ENOMEM;
backing-dev.c:	err = fprop_local_init_percpu(&wb->completions, gfp);
backing-dev.c:		err = percpu_counter_init(&wb->stat[i], 0, gfp);
backing-dev.c:	while (i--)
backing-dev.c:		percpu_counter_destroy(&wb->stat[i]);
backing-dev.c:	fprop_local_destroy_percpu(&wb->completions);
backing-dev.c:	wb_congested_put(wb->congested);
backing-dev.c:	if (wb != &bdi->wb)
backing-dev.c:	spin_lock_bh(&wb->work_lock);
backing-dev.c:	if (!test_and_clear_bit(WB_registered, &wb->state)) {
backing-dev.c:		spin_unlock_bh(&wb->work_lock);
backing-dev.c:		wait_on_bit(&wb->state, WB_shutting_down, TASK_UNINTERRUPTIBLE);
backing-dev.c:	set_bit(WB_shutting_down, &wb->state);
backing-dev.c:	spin_unlock_bh(&wb->work_lock);
backing-dev.c:	mod_delayed_work(bdi_wq, &wb->dwork, 0);
backing-dev.c:	flush_delayed_work(&wb->dwork);
backing-dev.c:	WARN_ON(!list_empty(&wb->work_list));
backing-dev.c:	clear_and_wake_up_bit(WB_shutting_down, &wb->state);
backing-dev.c:	WARN_ON(delayed_work_pending(&wb->dwork));
backing-dev.c:		percpu_counter_destroy(&wb->stat[i]);
backing-dev.c:	fprop_local_destroy_percpu(&wb->completions);
backing-dev.c:	wb_congested_put(wb->congested);
backing-dev.c:	if (wb != &wb->bdi->wb)
backing-dev.c:		bdi_put(wb->bdi);
backing-dev.c: * cgwb_lock protects bdi->cgwb_tree, bdi->cgwb_congested_tree,
backing-dev.c: * blkcg->cgwb_list, and memcg->cgwb_list.  bdi->cgwb_tree is also RCU
backing-dev.c: * wb_congested_get_create - get or create a wb_congested
backing-dev.c:	node = &bdi->cgwb_congested_tree.rb_node;
backing-dev.c:		if (congested->blkcg_id < blkcg_id)
backing-dev.c:			node = &parent->rb_left;
backing-dev.c:		else if (congested->blkcg_id > blkcg_id)
backing-dev.c:			node = &parent->rb_right;
backing-dev.c:		rb_link_node(&congested->rb_node, parent, node);
backing-dev.c:		rb_insert_color(&congested->rb_node, &bdi->cgwb_congested_tree);
backing-dev.c:	atomic_set(&new_congested->refcnt, 0);
backing-dev.c:	new_congested->__bdi = bdi;
backing-dev.c:	new_congested->blkcg_id = blkcg_id;
backing-dev.c:	atomic_inc(&congested->refcnt);
backing-dev.c: * wb_congested_put - put a wb_congested
backing-dev.c:	if (!atomic_dec_and_lock(&congested->refcnt, &cgwb_lock)) {
backing-dev.c:	if (congested->__bdi) {
backing-dev.c:		rb_erase(&congested->rb_node,
backing-dev.c:			 &congested->__bdi->cgwb_congested_tree);
backing-dev.c:		congested->__bdi = NULL;
backing-dev.c:	css_put(wb->memcg_css);
backing-dev.c:	css_put(wb->blkcg_css);
backing-dev.c:	fprop_local_destroy_percpu(&wb->memcg_completions);
backing-dev.c:	percpu_ref_exit(&wb->refcnt);
backing-dev.c:	queue_work(cgwb_release_wq, &wb->release_work);
backing-dev.c:	WARN_ON(!radix_tree_delete(&wb->bdi->cgwb_tree, wb->memcg_css->id));
backing-dev.c:	list_del(&wb->memcg_node);
backing-dev.c:	list_del(&wb->blkcg_node);
backing-dev.c:	percpu_ref_kill(&wb->refcnt);
backing-dev.c:	list_del_rcu(&wb->bdi_node);
backing-dev.c:	blkcg_css = cgroup_get_e_css(memcg_css->cgroup, &io_cgrp_subsys);
backing-dev.c:	blkcg_cgwb_list = &blkcg->cgwb_list;
backing-dev.c:	wb = radix_tree_lookup(&bdi->cgwb_tree, memcg_css->id);
backing-dev.c:	if (wb && wb->blkcg_css != blkcg_css) {
backing-dev.c:		ret = -ENOMEM;
backing-dev.c:	ret = wb_init(wb, bdi, blkcg_css->id, gfp);
backing-dev.c:	ret = percpu_ref_init(&wb->refcnt, cgwb_release, 0, gfp);
backing-dev.c:	ret = fprop_local_init_percpu(&wb->memcg_completions, gfp);
backing-dev.c:	wb->memcg_css = memcg_css;
backing-dev.c:	wb->blkcg_css = blkcg_css;
backing-dev.c:	INIT_WORK(&wb->release_work, cgwb_release_workfn);
backing-dev.c:	set_bit(WB_registered, &wb->state);
backing-dev.c:	ret = -ENODEV;
backing-dev.c:	if (test_bit(WB_registered, &bdi->wb.state) &&
backing-dev.c:	    blkcg_cgwb_list->next && memcg_cgwb_list->next) {
backing-dev.c:		ret = radix_tree_insert(&bdi->cgwb_tree, memcg_css->id, wb);
backing-dev.c:			list_add_tail_rcu(&wb->bdi_node, &bdi->wb_list);
backing-dev.c:			list_add(&wb->memcg_node, memcg_cgwb_list);
backing-dev.c:			list_add(&wb->blkcg_node, blkcg_cgwb_list);
backing-dev.c:		if (ret == -EEXIST)
backing-dev.c:	fprop_local_destroy_percpu(&wb->memcg_completions);
backing-dev.c:	percpu_ref_exit(&wb->refcnt);
backing-dev.c: * wb_get_create - get wb for a given memcg, create if necessary
backing-dev.c:	if (!memcg_css->parent)
backing-dev.c:		return &bdi->wb;
backing-dev.c:		wb = radix_tree_lookup(&bdi->cgwb_tree, memcg_css->id);
backing-dev.c:			blkcg_css = cgroup_get_e_css(memcg_css->cgroup,
backing-dev.c:			if (unlikely(wb->blkcg_css != blkcg_css ||
backing-dev.c:	INIT_RADIX_TREE(&bdi->cgwb_tree, GFP_ATOMIC);
backing-dev.c:	bdi->cgwb_congested_tree = RB_ROOT;
backing-dev.c:	ret = wb_init(&bdi->wb, bdi, 1, GFP_KERNEL);
backing-dev.c:		bdi->wb.memcg_css = &root_mem_cgroup->css;
backing-dev.c:		bdi->wb.blkcg_css = blkcg_root_css;
backing-dev.c:	WARN_ON(test_bit(WB_registered, &bdi->wb.state));
backing-dev.c:	radix_tree_for_each_slot(slot, &bdi->cgwb_tree, &iter, 0)
backing-dev.c:	while (!list_empty(&bdi->wb_list)) {
backing-dev.c:		wb = list_first_entry(&bdi->wb_list, struct bdi_writeback,
backing-dev.c: * wb_memcg_offline - kill all wb's associated with a memcg being offlined
backing-dev.c:	memcg_cgwb_list->next = NULL;	/* prevent new wb's */
backing-dev.c: * wb_blkcg_offline - kill all wb's associated with a blkcg being offlined
backing-dev.c:	list_for_each_entry_safe(wb, next, &blkcg->cgwb_list, blkcg_node)
backing-dev.c:	blkcg->cgwb_list.next = NULL;	/* prevent new wb's */
backing-dev.c:	while ((rbn = rb_first(&bdi->cgwb_congested_tree))) {
backing-dev.c:		rb_erase(rbn, &bdi->cgwb_congested_tree);
backing-dev.c:		congested->__bdi = NULL;	/* mark @congested unlinked */
backing-dev.c:	list_add_tail_rcu(&bdi->wb.bdi_node, &bdi->wb_list);
backing-dev.c:		return -ENOMEM;
backing-dev.c:	bdi->wb_congested = kzalloc(sizeof(*bdi->wb_congested), GFP_KERNEL);
backing-dev.c:	if (!bdi->wb_congested)
backing-dev.c:		return -ENOMEM;
backing-dev.c:	atomic_set(&bdi->wb_congested->refcnt, 1);
backing-dev.c:	err = wb_init(&bdi->wb, bdi, 1, GFP_KERNEL);
backing-dev.c:		wb_congested_put(bdi->wb_congested);
backing-dev.c:	wb_congested_put(bdi->wb_congested);
backing-dev.c:	list_add_tail_rcu(&bdi->wb.bdi_node, &bdi->wb_list);
backing-dev.c:	list_del_rcu(&wb->bdi_node);
backing-dev.c:	bdi->dev = NULL;
backing-dev.c:	kref_init(&bdi->refcnt);
backing-dev.c:	bdi->min_ratio = 0;
backing-dev.c:	bdi->max_ratio = 100;
backing-dev.c:	bdi->max_prop_frac = FPROP_FRAC_BASE;
backing-dev.c:	INIT_LIST_HEAD(&bdi->bdi_list);
backing-dev.c:	INIT_LIST_HEAD(&bdi->wb_list);
backing-dev.c:	init_waitqueue_head(&bdi->wb_waitq);
backing-dev.c:	if (bdi->dev)	/* The driver needs to use separate queues per device */
backing-dev.c:	bdi->dev = dev;
backing-dev.c:	set_bit(WB_registered, &bdi->wb.state);
backing-dev.c:	list_add_tail_rcu(&bdi->bdi_list, &bdi_list);
backing-dev.c:	rc = bdi_register(bdi, "%u:%u", MAJOR(owner->devt), MINOR(owner->devt));
backing-dev.c:	WARN_ON(bdi->owner);
backing-dev.c:	bdi->owner = owner;
backing-dev.c:	list_del_rcu(&bdi->bdi_list);
backing-dev.c:	wb_shutdown(&bdi->wb);
backing-dev.c:	if (bdi->dev) {
backing-dev.c:		device_unregister(bdi->dev);
backing-dev.c:		bdi->dev = NULL;
backing-dev.c:	if (bdi->owner) {
backing-dev.c:		put_device(bdi->owner);
backing-dev.c:		bdi->owner = NULL;
backing-dev.c:	if (test_bit(WB_registered, &bdi->wb.state))
backing-dev.c:	WARN_ON_ONCE(bdi->dev);
backing-dev.c:	wb_exit(&bdi->wb);
backing-dev.c:	kref_put(&bdi->refcnt, release_bdi);
backing-dev.c:	if (test_and_clear_bit(bit, &congested->state))
backing-dev.c:	if (!test_and_set_bit(bit, &congested->state))
backing-dev.c: * congestion_wait - wait for a backing_dev to become uncongested
backing-dev.c:					jiffies_to_usecs(jiffies - start));
backing-dev.c: * wait_iff_congested - Conditionally wait for a backing_dev to become uncongested or a pgdat to complete writes
backing-dev.c:	    !test_bit(PGDAT_CONGESTED, &pgdat->flags)) {
backing-dev.c:		ret = timeout - (jiffies - start);
backing-dev.c:					jiffies_to_usecs(jiffies - start));
backing-dev.c:		return -EFAULT;
backing-dev.c:		     table->procname);
truncate.c: * mm/truncate.c - code for taking down pages from address_spaces
truncate.c:#include <linux/backing-dev.h>
truncate.c:	spin_lock_irq(&mapping->tree_lock);
truncate.c:	if (!__radix_tree_lookup(&mapping->page_tree, index, &node, &slot))
truncate.c:	__radix_tree_replace(&mapping->page_tree, node, slot, NULL,
truncate.c:	mapping->nrexceptional--;
truncate.c:	spin_unlock_irq(&mapping->tree_lock);
truncate.c: * do_invalidatepage - invalidate part or all of a page
truncate.c: * blocks on-disk.
truncate.c:	invalidatepage = page->mapping->a_ops->invalidatepage;
truncate.c: * If truncate cannot remove the fs-private metadata from the page, the page
truncate.c: * We need to bale out if page->mapping is no longer equal to the original
truncate.c:	if (page->mapping != mapping)
truncate.c:		return -EIO;
truncate.c:	 * Some filesystems seem to re-dirty the page even after
truncate.c: * Returns non-zero if the page was successfully invalidated.
truncate.c:	if (page->mapping != mapping)
truncate.c:				   (loff_t)page->index << PAGE_SHIFT,
truncate.c:		return -EINVAL;
truncate.c:	if (!S_ISREG(mapping->host->i_mode))
truncate.c:		return -EIO;
truncate.c: * truncate_inode_pages_range - truncate range of pages specified by start & end byte offsets
truncate.c: * Truncate takes two passes - the first pass is nonblocking.  It will not
truncate.c: * We pass down the cache-hot hint to the page freeing code.  Even if the
truncate.c: * Note that since ->invalidatepage() accepts range to invalidate
truncate.c:	if (mapping->nrpages == 0 && mapping->nrexceptional == 0)
truncate.c:	partial_start = lstart & (PAGE_SIZE - 1);
truncate.c:	partial_end = (lend + 1) & (PAGE_SIZE - 1);
truncate.c:	start = (lstart + PAGE_SIZE - 1) >> PAGE_SHIFT;
truncate.c:	if (lend == -1)
truncate.c:		 * lend == -1 indicates end-of-file so we have to set 'end'
truncate.c:		 * unsigned we're using -1.
truncate.c:		end = -1;
truncate.c:			min(end - index, (pgoff_t)PAGEVEC_SIZE),
truncate.c:			/* We rely upon deletion not changing page->index */
truncate.c:		struct page *page = find_lock_page(mapping, start - 1);
truncate.c:						  top - partial_start);
truncate.c:			min(end - index, (pgoff_t)PAGEVEC_SIZE), indices)) {
truncate.c:			/* We rely upon deletion not changing page->index */
truncate.c:				index = start - 1;
truncate.c: * truncate_inode_pages - truncate *all* the pages from an offset
truncate.c: * Called under (and serialised by) inode->i_mutex.
truncate.c: * mapping->nrpages can be non-zero when this function returns even after
truncate.c:	truncate_inode_pages_range(mapping, lstart, (loff_t)-1);
truncate.c: * truncate_inode_pages_final - truncate *all* pages before inode dies
truncate.c: * Called under (and serialized by) inode->i_mutex.
truncate.c:	nrpages = mapping->nrpages;
truncate.c:	nrexceptional = mapping->nrexceptional;
truncate.c:		spin_lock_irq(&mapping->tree_lock);
truncate.c:		spin_unlock_irq(&mapping->tree_lock);
truncate.c: * invalidate_mapping_pages - Invalidate all the unlocked pages of one inode
truncate.c:			min(end - index, (pgoff_t)PAGEVEC_SIZE - 1) + 1,
truncate.c:			/* We rely upon deletion not changing page->index */
truncate.c:				index += HPAGE_PMD_NR - 1;
truncate.c:				i += HPAGE_PMD_NR - 1;
truncate.c:	if (page->mapping != mapping)
truncate.c:	spin_lock_irqsave(&mapping->tree_lock, flags);
truncate.c:	spin_unlock_irqrestore(&mapping->tree_lock, flags);
truncate.c:	if (mapping->a_ops->freepage)
truncate.c:		mapping->a_ops->freepage(page);
truncate.c:	spin_unlock_irqrestore(&mapping->tree_lock, flags);
truncate.c:	if (page->mapping != mapping || mapping->a_ops->launder_page == NULL)
truncate.c:	return mapping->a_ops->launder_page(page);
truncate.c: * invalidate_inode_pages2_range - remove range of pages from an address_space
truncate.c: * Returns -EBUSY if any pages could not be invalidated.
truncate.c:	if (mapping->nrpages == 0 && mapping->nrexceptional == 0)
truncate.c:			min(end - index, (pgoff_t)PAGEVEC_SIZE - 1) + 1,
truncate.c:			/* We rely upon deletion not changing page->index */
truncate.c:					ret = -EBUSY;
truncate.c:			if (page->mapping != mapping) {
truncate.c:					   (loff_t)(1 + end - index)
truncate.c:					ret2 = -EBUSY;
truncate.c:				    (loff_t)(end - start + 1) << PAGE_SHIFT, 0);
truncate.c: * invalidate_inode_pages2 - remove all pages from an address_space
truncate.c: * Returns -EBUSY if any pages could not be invalidated.
truncate.c:	return invalidate_inode_pages2_range(mapping, 0, -1);
truncate.c: * truncate_pagecache - unmap and remove pagecache that has been truncated
truncate.c: * with on-disk format, and the filesystem would not have to deal with
truncate.c:	struct address_space *mapping = inode->i_mapping;
truncate.c:	 * single-page unmaps.  However after this first call, and
truncate.c: * truncate_setsize - update inode and pagecache for a new file size
truncate.c:	loff_t oldsize = inode->i_size;
truncate.c: * pagecache_isize_extended - update pagecache after extension of i_size
truncate.c: * The function must be called while we still hold i_mutex - this not only
truncate.c:	WARN_ON(to > inode->i_size);
truncate.c:	if (to <= rounded_from || !(rounded_from & (PAGE_SIZE - 1)))
truncate.c:	page = find_lock_page(inode->i_mapping, index);
truncate.c: * truncate_pagecache_range - unmap and remove pagecache that is hole-punched
truncate.c: * with on-disk format, and the filesystem would not have to deal with
truncate.c:	struct address_space *mapping = inode->i_mapping;
truncate.c:	loff_t unmap_end = round_down(1 + lend, PAGE_SIZE) - 1;
truncate.c:	 * allows holelen 0 for all, and we allow lend -1 for end of file.
truncate.c:	 * hole-punching should not remove private COWed pages from the hole.
truncate.c:				    1 + unmap_end - unmap_start, 0);
swapfile.c:#include <linux/backing-dev.h>
swapfile.c:static int least_priority = -1;
swapfile.c: * swap_info_struct changes between not-full/full, it needs to
swapfile.c: * add/remove itself to/from this list, but the swap_info_struct->lock
swapfile.c: * before any swap_info_struct->lock.
swapfile.c:	swp_entry_t entry = swp_entry(si->type, offset);
swapfile.c: * to allow the swap device to optimize its wear-levelling.
swapfile.c:	se = &si->first_swap_extent;
swapfile.c:	start_block = (se->start_block + 1) << (PAGE_SHIFT - 9);
swapfile.c:	nr_blocks = ((sector_t)se->nr_pages - 1) << (PAGE_SHIFT - 9);
swapfile.c:		err = blkdev_issue_discard(si->bdev, start_block,
swapfile.c:	list_for_each_entry(se, &si->first_swap_extent.list, list) {
swapfile.c:		start_block = se->start_block << (PAGE_SHIFT - 9);
swapfile.c:		nr_blocks = (sector_t)se->nr_pages << (PAGE_SHIFT - 9);
swapfile.c:		err = blkdev_issue_discard(si->bdev, start_block,
swapfile.c:	return err;		/* That will often be -EOPNOTSUPP */
swapfile.c: * to allow the swap device to optimize its wear-levelling.
swapfile.c:	struct swap_extent *se = si->curr_swap_extent;
swapfile.c:		if (se->start_page <= start_page &&
swapfile.c:		    start_page < se->start_page + se->nr_pages) {
swapfile.c:			pgoff_t offset = start_page - se->start_page;
swapfile.c:			sector_t start_block = se->start_block + offset;
swapfile.c:			sector_t nr_blocks = se->nr_pages - offset;
swapfile.c:			nr_pages -= nr_blocks;
swapfile.c:				si->curr_swap_extent = se;
swapfile.c:			start_block <<= PAGE_SHIFT - 9;
swapfile.c:			nr_blocks <<= PAGE_SHIFT - 9;
swapfile.c:			if (blkdev_issue_discard(si->bdev, start_block,
swapfile.c:	info->flags = flag;
swapfile.c:	return info->data;
swapfile.c:	info->data = c;
swapfile.c:	info->flags = f;
swapfile.c:	info->data = c;
swapfile.c:	return info->data;
swapfile.c:	info->data = n;
swapfile.c:	info->flags = f;
swapfile.c:	info->data = n;
swapfile.c:	return info->flags & CLUSTER_FLAG_FREE;
swapfile.c:	return info->flags & CLUSTER_FLAG_NEXT_NULL;
swapfile.c:	info->flags = CLUSTER_FLAG_NEXT_NULL;
swapfile.c:	info->data = 0;
swapfile.c:	return info->flags & CLUSTER_FLAG_HUGE;
swapfile.c:	info->flags &= ~CLUSTER_FLAG_HUGE;
swapfile.c:	ci = si->cluster_info;
swapfile.c:		spin_lock(&ci->lock);
swapfile.c:		spin_unlock(&ci->lock);
swapfile.c:		spin_lock(&si->lock);
swapfile.c:		spin_unlock(&si->lock);
swapfile.c:	return cluster_is_null(&list->head);
swapfile.c:	return cluster_next(&list->head);
swapfile.c:	cluster_set_null(&list->head);
swapfile.c:	cluster_set_null(&list->tail);
swapfile.c:		cluster_set_next_flag(&list->head, idx, 0);
swapfile.c:		cluster_set_next_flag(&list->tail, idx, 0);
swapfile.c:		unsigned int tail = cluster_next(&list->tail);
swapfile.c:		 * only acquired when we held swap_info_struct->lock
swapfile.c:		spin_lock_nested(&ci_tail->lock, SINGLE_DEPTH_NESTING);
swapfile.c:		spin_unlock(&ci_tail->lock);
swapfile.c:		cluster_set_next_flag(&list->tail, idx, 0);
swapfile.c:	idx = cluster_next(&list->head);
swapfile.c:	if (cluster_next(&list->tail) == idx) {
swapfile.c:		cluster_set_null(&list->head);
swapfile.c:		cluster_set_null(&list->tail);
swapfile.c:		cluster_set_next_flag(&list->head,
swapfile.c:	 * si->swap_map directly. To make sure the discarding cluster isn't
swapfile.c:	memset(si->swap_map + idx * SWAPFILE_CLUSTER,
swapfile.c:	cluster_list_add_tail(&si->discard_clusters, si->cluster_info, idx);
swapfile.c:	schedule_work(&si->discard_work);
swapfile.c:	struct swap_cluster_info *ci = si->cluster_info;
swapfile.c:	cluster_list_add_tail(&si->free_clusters, ci, idx);
swapfile.c: * will be added to free cluster list. caller should hold si->lock.
swapfile.c:	info = si->cluster_info;
swapfile.c:	while (!cluster_list_empty(&si->discard_clusters)) {
swapfile.c:		idx = cluster_list_del_first(&si->discard_clusters, info);
swapfile.c:		spin_unlock(&si->lock);
swapfile.c:		spin_lock(&si->lock);
swapfile.c:		memset(si->swap_map + idx * SWAPFILE_CLUSTER,
swapfile.c:	spin_lock(&si->lock);
swapfile.c:	spin_unlock(&si->lock);
swapfile.c:	struct swap_cluster_info *ci = si->cluster_info;
swapfile.c:	VM_BUG_ON(cluster_list_first(&si->free_clusters) != idx);
swapfile.c:	cluster_list_del_first(&si->free_clusters, ci);
swapfile.c:	struct swap_cluster_info *ci = si->cluster_info + idx;
swapfile.c:	if ((si->flags & (SWP_WRITEOK | SWP_PAGE_DISCARD)) ==
swapfile.c:		cluster_count(&cluster_info[idx]) - 1);
swapfile.c:	conflict = !cluster_list_empty(&si->free_clusters) &&
swapfile.c:		offset != cluster_list_first(&si->free_clusters) &&
swapfile.c:		cluster_is_free(&si->cluster_info[offset]);
swapfile.c:	percpu_cluster = this_cpu_ptr(si->percpu_cluster);
swapfile.c:	cluster_set_null(&percpu_cluster->index);
swapfile.c:	cluster = this_cpu_ptr(si->percpu_cluster);
swapfile.c:	if (cluster_is_null(&cluster->index)) {
swapfile.c:		if (!cluster_list_empty(&si->free_clusters)) {
swapfile.c:			cluster->index = si->free_clusters.head;
swapfile.c:			cluster->next = cluster_next(&cluster->index) *
swapfile.c:		} else if (!cluster_list_empty(&si->discard_clusters)) {
swapfile.c:			*scan_base = *offset = si->cluster_next;
swapfile.c:	tmp = cluster->next;
swapfile.c:	max = min_t(unsigned long, si->max,
swapfile.c:		    (cluster_next(&cluster->index) + 1) * SWAPFILE_CLUSTER);
swapfile.c:		cluster_set_null(&cluster->index);
swapfile.c:		if (!si->swap_map[tmp]) {
swapfile.c:		cluster_set_null(&cluster->index);
swapfile.c:	cluster->next = tmp + 1;
swapfile.c:		plist_del(&p->avail_lists[nid], &swap_avail_heads[nid]);
swapfile.c:	unsigned int end = offset + nr_entries - 1;
swapfile.c:	if (offset == si->lowest_bit)
swapfile.c:		si->lowest_bit += nr_entries;
swapfile.c:	if (end == si->highest_bit)
swapfile.c:		si->highest_bit -= nr_entries;
swapfile.c:	si->inuse_pages += nr_entries;
swapfile.c:	if (si->inuse_pages == si->pages) {
swapfile.c:		si->lowest_bit = si->max;
swapfile.c:		si->highest_bit = 0;
swapfile.c:		WARN_ON(!plist_node_empty(&p->avail_lists[nid]));
swapfile.c:		plist_add(&p->avail_lists[nid], &swap_avail_heads[nid]);
swapfile.c:	unsigned long end = offset + nr_entries - 1;
swapfile.c:	if (offset < si->lowest_bit)
swapfile.c:		si->lowest_bit = offset;
swapfile.c:	if (end > si->highest_bit) {
swapfile.c:		bool was_full = !si->highest_bit;
swapfile.c:		si->highest_bit = end;
swapfile.c:		if (was_full && (si->flags & SWP_WRITEOK))
swapfile.c:	si->inuse_pages -= nr_entries;
swapfile.c:	if (si->flags & SWP_BLKDEV)
swapfile.c:			si->bdev->bd_disk->fops->swap_slot_free_notify;
swapfile.c:		frontswap_invalidate_page(si->type, offset);
swapfile.c:			swap_slot_free_notify(si->bdev, offset);
swapfile.c:	 * way, however, we resort to first-free allocation, starting
swapfile.c:	 * overall disk seek times between swap pages.  -- sct
swapfile.c:	 * But we do now try to find an empty cluster.  -Andrea
swapfile.c:	si->flags += SWP_SCANNING;
swapfile.c:	scan_base = offset = si->cluster_next;
swapfile.c:	if (si->cluster_info) {
swapfile.c:	if (unlikely(!si->cluster_nr--)) {
swapfile.c:		if (si->pages - si->inuse_pages < SWAPFILE_CLUSTER) {
swapfile.c:			si->cluster_nr = SWAPFILE_CLUSTER - 1;
swapfile.c:		spin_unlock(&si->lock);
swapfile.c:		 * If seek is cheap, that is the SWP_SOLIDSTATE si->cluster_info
swapfile.c:		scan_base = offset = si->lowest_bit;
swapfile.c:		last_in_cluster = offset + SWAPFILE_CLUSTER - 1;
swapfile.c:		for (; last_in_cluster <= si->highest_bit; offset++) {
swapfile.c:			if (si->swap_map[offset])
swapfile.c:				spin_lock(&si->lock);
swapfile.c:				offset -= SWAPFILE_CLUSTER - 1;
swapfile.c:				si->cluster_next = offset;
swapfile.c:				si->cluster_nr = SWAPFILE_CLUSTER - 1;
swapfile.c:			if (unlikely(--latency_ration < 0)) {
swapfile.c:		spin_lock(&si->lock);
swapfile.c:		si->cluster_nr = SWAPFILE_CLUSTER - 1;
swapfile.c:	if (si->cluster_info) {
swapfile.c:	if (!(si->flags & SWP_WRITEOK))
swapfile.c:	if (!si->highest_bit)
swapfile.c:	if (offset > si->highest_bit)
swapfile.c:		scan_base = offset = si->lowest_bit;
swapfile.c:	/* reuse swap entry of cache-only swap if not busy. */
swapfile.c:	if (vm_swap_full() && si->swap_map[offset] == SWAP_HAS_CACHE) {
swapfile.c:		spin_unlock(&si->lock);
swapfile.c:		spin_lock(&si->lock);
swapfile.c:	if (si->swap_map[offset]) {
swapfile.c:	si->swap_map[offset] = usage;
swapfile.c:	inc_cluster_info_page(si, si->cluster_info, offset);
swapfile.c:	si->cluster_next = offset + 1;
swapfile.c:	slots[n_ret++] = swp_entry(si->type, offset);
swapfile.c:	if ((n_ret == nr) || (offset >= si->highest_bit))
swapfile.c:	if (unlikely(--latency_ration < 0)) {
swapfile.c:		spin_unlock(&si->lock);
swapfile.c:		spin_lock(&si->lock);
swapfile.c:	if (si->cluster_info) {
swapfile.c:	/* non-ssd case */
swapfile.c:	/* non-ssd case, still more slots in cluster? */
swapfile.c:	if (si->cluster_nr && !si->swap_map[offset]) {
swapfile.c:		--si->cluster_nr;
swapfile.c:	si->flags -= SWP_SCANNING;
swapfile.c:	spin_unlock(&si->lock);
swapfile.c:	while (++offset <= si->highest_bit) {
swapfile.c:		if (!si->swap_map[offset]) {
swapfile.c:			spin_lock(&si->lock);
swapfile.c:		if (vm_swap_full() && si->swap_map[offset] == SWAP_HAS_CACHE) {
swapfile.c:			spin_lock(&si->lock);
swapfile.c:		if (unlikely(--latency_ration < 0)) {
swapfile.c:	offset = si->lowest_bit;
swapfile.c:		if (!si->swap_map[offset]) {
swapfile.c:			spin_lock(&si->lock);
swapfile.c:		if (vm_swap_full() && si->swap_map[offset] == SWAP_HAS_CACHE) {
swapfile.c:			spin_lock(&si->lock);
swapfile.c:		if (unlikely(--latency_ration < 0)) {
swapfile.c:	spin_lock(&si->lock);
swapfile.c:	si->flags -= SWP_SCANNING;
swapfile.c:	if (cluster_list_empty(&si->free_clusters))
swapfile.c:	idx = cluster_list_first(&si->free_clusters);
swapfile.c:	map = si->swap_map + offset;
swapfile.c:	*slot = swp_entry(si->type, offset);
swapfile.c:		/* requeue si to after same-priority siblings */
swapfile.c:		plist_requeue(&si->avail_lists[node], &swap_avail_heads[node]);
swapfile.c:		spin_lock(&si->lock);
swapfile.c:		if (!si->highest_bit || !(si->flags & SWP_WRITEOK)) {
swapfile.c:			if (plist_node_empty(&si->avail_lists[node])) {
swapfile.c:				spin_unlock(&si->lock);
swapfile.c:			WARN(!si->highest_bit,
swapfile.c:			     si->type);
swapfile.c:			WARN(!(si->flags & SWP_WRITEOK),
swapfile.c:			     si->type);
swapfile.c:			spin_unlock(&si->lock);
swapfile.c:			if (!(si->flags & SWP_FILE))
swapfile.c:		spin_unlock(&si->lock);
swapfile.c:			si->type);
swapfile.c:		 * and since scan_swap_map() can drop the si->lock, multiple
swapfile.c:		 * up between us dropping swap_avail_lock and taking si->lock.
swapfile.c:		if (plist_node_empty(&next->avail_lists[node]))
swapfile.c:		atomic_long_add((long)(n_goal - n_ret) * nr_pages,
swapfile.c:	spin_lock(&si->lock);
swapfile.c:	if (si && (si->flags & SWP_WRITEOK)) {
swapfile.c:			spin_unlock(&si->lock);
swapfile.c:	spin_unlock(&si->lock);
swapfile.c:	if (!(p->flags & SWP_USED))
swapfile.c:	if (offset >= p->max)
swapfile.c:	if (!p->swap_map[swp_offset(entry)])
swapfile.c:		spin_lock(&p->lock);
swapfile.c:			spin_unlock(&q->lock);
swapfile.c:			spin_lock(&p->lock);
swapfile.c:	count = p->swap_map[offset];
swapfile.c:			count--;
swapfile.c:	p->swap_map[offset] = usage ? : SWAP_HAS_CACHE;
swapfile.c:	count = p->swap_map[offset];
swapfile.c:	p->swap_map[offset] = 0;
swapfile.c:	dec_cluster_info_page(p, p->cluster_info, offset);
swapfile.c:	map = si->swap_map + offset;
swapfile.c:		spin_lock(&si->lock);
swapfile.c:		spin_unlock(&si->lock);
swapfile.c:		return -EBUSY;
swapfile.c:	return (int)swp_type(*e1) - (int)swp_type(*e2);
swapfile.c:		spin_unlock(&p->lock);
swapfile.c:		count = swap_count(p->swap_map[offset]);
swapfile.c:	count = swap_count(si->swap_map[offset]);
swapfile.c:	count = swap_count(p->swap_map[offset]);
swapfile.c:	page = vmalloc_to_page(p->swap_map + offset);
swapfile.c:	unsigned char *map = si->swap_map;
swapfile.c:		mapcount = atomic_read(&page->_mapcount) + 1;
swapfile.c:			map = si->swap_map;
swapfile.c:		map_swapcount -= 1;
swapfile.c:		_total_mapcount -= HPAGE_PMD_NR;
swapfile.c: * to it.  And as a side-effect, free up its swap: because the old content
swapfile.c:			if (p->flags & SWP_STABLE_WRITES) {
swapfile.c:				spin_unlock(&p->lock);
swapfile.c:			spin_unlock(&p->lock);
swapfile.c:	 * - most probably a call from __try_to_reclaim_swap() while
swapfile.c:	 * but conceivably even a call from memory reclaim - will free
swapfile.c: * @offset - number of the PAGE_SIZE-sized block of the device, starting
swapfile.c:		if (!(sis->flags & SWP_WRITEOK))
swapfile.c:				*bdev_p = bdgrab(sis->bdev);
swapfile.c:		if (bdev == sis->bdev) {
swapfile.c:			struct swap_extent *se = &sis->first_swap_extent;
swapfile.c:			if (se->start_block == offset) {
swapfile.c:					*bdev_p = bdgrab(sis->bdev);
swapfile.c:	return -ENODEV;
swapfile.c:	if (!(swap_info[type]->flags & SWP_WRITEOK))
swapfile.c:		spin_lock(&sis->lock);
swapfile.c:		if (sis->flags & SWP_WRITEOK) {
swapfile.c:			n = sis->pages;
swapfile.c:				n -= sis->inuse_pages;
swapfile.c:		spin_unlock(&sis->lock);
swapfile.c: * just let do_wp_page work it out if a write is requested later - to
swapfile.c:		return -ENOMEM;
swapfile.c:	if (mem_cgroup_try_charge(page, vma->vm_mm, GFP_KERNEL,
swapfile.c:		ret = -ENOMEM;
swapfile.c:	pte = pte_offset_map_lock(vma->vm_mm, pmd, addr, &ptl);
swapfile.c:	dec_mm_counter(vma->vm_mm, MM_SWAPENTS);
swapfile.c:	inc_mm_counter(vma->vm_mm, MM_ANONPAGES);
swapfile.c:	set_pte_at(vma->vm_mm, addr, pte,
swapfile.c:		   pte_mkold(mk_pte(page, vma->vm_page_prot)));
swapfile.c:	pte_unmap(pte - 1);
swapfile.c:		if (addr == -EFAULT)
swapfile.c:		addr = vma->vm_start;
swapfile.c:		end = vma->vm_end;
swapfile.c:	pgd = pgd_offset(vma->vm_mm, addr);
swapfile.c:	if (!down_read_trylock(&mm->mmap_sem)) {
swapfile.c:		down_read(&mm->mmap_sem);
swapfile.c:	for (vma = mm->mmap; vma; vma = vma->vm_next) {
swapfile.c:		if (vma->anon_vma && (ret = unuse_vma(vma, entry, page)))
swapfile.c:	up_read(&mm->mmap_sem);
swapfile.c:	unsigned int max = si->max;
swapfile.c:		count = READ_ONCE(si->swap_map[i]);
swapfile.c:			retval = -EINTR;
swapfile.c:		swap_map = &si->swap_map[i];
swapfile.c:			retval = -ENOMEM;
swapfile.c:		if (atomic_read(&start_mm->mm_users) == 1) {
swapfile.c:		 * defer to do_swap_page in such a case - in some tests,
swapfile.c:			struct list_head *p = &start_mm->mmlist;
swapfile.c:					(p = p->next) != &start_mm->mmlist) {
swapfile.c:		 * then re-duplicate the entry once we drop page lock,
swapfile.c:		 * after ensuring that the data has been saved to disk -
swapfile.c:		 * handle where KSM pages have been swapped out: re-reading
swapfile.c:			if (!--pages_to_unuse)
swapfile.c: * added to the mmlist just after page_duplicate - before would be racy.
swapfile.c:		if (swap_info[type]->inuse_pages)
swapfile.c:	*bdev = sis->bdev;
swapfile.c:	start_se = sis->curr_swap_extent;
swapfile.c:		if (se->start_page <= offset &&
swapfile.c:				offset < (se->start_page + se->nr_pages)) {
swapfile.c:			return se->start_block + (offset - se->start_page);
swapfile.c:		sis->curr_swap_extent = se;
swapfile.c:	while (!list_empty(&sis->first_swap_extent.list)) {
swapfile.c:		se = list_first_entry(&sis->first_swap_extent.list,
swapfile.c:		list_del(&se->list);
swapfile.c:	if (sis->flags & SWP_FILE) {
swapfile.c:		struct file *swap_file = sis->swap_file;
swapfile.c:		struct address_space *mapping = swap_file->f_mapping;
swapfile.c:		sis->flags &= ~SWP_FILE;
swapfile.c:		mapping->a_ops->swap_deactivate(swap_file);
swapfile.c:		se = &sis->first_swap_extent;
swapfile.c:		sis->curr_swap_extent = se;
swapfile.c:		se->start_page = 0;
swapfile.c:		se->nr_pages = nr_pages;
swapfile.c:		se->start_block = start_block;
swapfile.c:		lh = sis->first_swap_extent.list.prev;	/* Highest extent */
swapfile.c:		BUG_ON(se->start_page + se->nr_pages != start_page);
swapfile.c:		if (se->start_block + se->nr_pages == start_block) {
swapfile.c:			se->nr_pages += nr_pages;
swapfile.c:		return -ENOMEM;
swapfile.c:	new_se->start_page = start_page;
swapfile.c:	new_se->nr_pages = nr_pages;
swapfile.c:	new_se->start_block = start_block;
swapfile.c:	list_add_tail(&new_se->list, &sis->first_swap_extent.list);
swapfile.c: * requirements, they are simply tossed out - we will never use those blocks
swapfile.c: * prevents root from shooting her foot off by ftruncating an in-use swapfile,
swapfile.c: * Typically it is in the 1-4 megabyte range.  So we can have hundreds of
swapfile.c: * map_swap_page() has been measured at about 0.3 per page.  - akpm.
swapfile.c:	struct file *swap_file = sis->swap_file;
swapfile.c:	struct address_space *mapping = swap_file->f_mapping;
swapfile.c:	struct inode *inode = mapping->host;
swapfile.c:	if (S_ISBLK(inode->i_mode)) {
swapfile.c:		ret = add_swap_extent(sis, 0, sis->max, 0);
swapfile.c:		*span = sis->pages;
swapfile.c:	if (mapping->a_ops->swap_activate) {
swapfile.c:		ret = mapping->a_ops->swap_activate(sis, swap_file, span);
swapfile.c:			sis->flags |= SWP_FILE;
swapfile.c:			ret = add_swap_extent(sis, 0, sis->max, 0);
swapfile.c:			*span = sis->pages;
swapfile.c:	if (p->bdev)
swapfile.c:		bdev = p->bdev;
swapfile.c:		bdev = p->swap_file->f_inode->i_sb->s_bdev;
swapfile.c:	return bdev ? bdev->bd_disk->node_id : NUMA_NO_NODE;
swapfile.c:		p->prio = prio;
swapfile.c:		p->prio = --least_priority;
swapfile.c:	 * low-to-high, while swap ordering is high-to-low
swapfile.c:	p->list.prio = -p->prio;
swapfile.c:		if (p->prio >= 0)
swapfile.c:			p->avail_lists[i].prio = -p->prio;
swapfile.c:				p->avail_lists[i].prio = 1;
swapfile.c:				p->avail_lists[i].prio = -p->prio;
swapfile.c:	p->swap_map = swap_map;
swapfile.c:	p->cluster_info = cluster_info;
swapfile.c:	p->flags |= SWP_WRITEOK;
swapfile.c:	atomic_long_add(p->pages, &nr_swap_pages);
swapfile.c:	total_swap_pages += p->pages;
swapfile.c:	 * which on removal of any swap_info_struct with an auto-assigned
swapfile.c:	 * (i.e. negative) priority increments the auto-assigned priority
swapfile.c:	 * of any lower-priority swap_info_structs.
swapfile.c:	plist_add(&p->list, &swap_active_head);
swapfile.c:	frontswap_init(p->type, frontswap_map);
swapfile.c:	spin_lock(&p->lock);
swapfile.c:	spin_unlock(&p->lock);
swapfile.c:	spin_lock(&p->lock);
swapfile.c:	_enable_swap_info(p, p->prio, p->swap_map, p->cluster_info);
swapfile.c:	spin_unlock(&p->lock);
swapfile.c:		return -EPERM;
swapfile.c:	BUG_ON(!current->mm);
swapfile.c:	mapping = victim->f_mapping;
swapfile.c:		if (p->flags & SWP_WRITEOK) {
swapfile.c:			if (p->swap_file->f_mapping == mapping) {
swapfile.c:		err = -EINVAL;
swapfile.c:	if (!security_vm_enough_memory_mm(current->mm, p->pages))
swapfile.c:		vm_unacct_memory(p->pages);
swapfile.c:		err = -ENOMEM;
swapfile.c:	spin_lock(&p->lock);
swapfile.c:	if (p->prio < 0) {
swapfile.c:			si->prio++;
swapfile.c:			si->list.prio--;
swapfile.c:				if (si->avail_lists[nid].prio != 1)
swapfile.c:					si->avail_lists[nid].prio--;
swapfile.c:	plist_del(&p->list, &swap_active_head);
swapfile.c:	atomic_long_sub(p->pages, &nr_swap_pages);
swapfile.c:	total_swap_pages -= p->pages;
swapfile.c:	p->flags &= ~SWP_WRITEOK;
swapfile.c:	spin_unlock(&p->lock);
swapfile.c:	err = try_to_unuse(p->type, false, 0); /* force unuse all pages */
swapfile.c:		/* re-insert swap space back into swap_list */
swapfile.c:	flush_work(&p->discard_work);
swapfile.c:	if (p->flags & SWP_CONTINUED)
swapfile.c:	if (!p->bdev || !blk_queue_nonrot(bdev_get_queue(p->bdev)))
swapfile.c:	spin_lock(&p->lock);
swapfile.c:	p->highest_bit = 0;		/* cuts scans short */
swapfile.c:	while (p->flags >= SWP_SCANNING) {
swapfile.c:		spin_unlock(&p->lock);
swapfile.c:		spin_lock(&p->lock);
swapfile.c:	swap_file = p->swap_file;
swapfile.c:	old_block_size = p->old_block_size;
swapfile.c:	p->swap_file = NULL;
swapfile.c:	p->max = 0;
swapfile.c:	swap_map = p->swap_map;
swapfile.c:	p->swap_map = NULL;
swapfile.c:	cluster_info = p->cluster_info;
swapfile.c:	p->cluster_info = NULL;
swapfile.c:	spin_unlock(&p->lock);
swapfile.c:	frontswap_invalidate_area(p->type);
swapfile.c:	free_percpu(p->percpu_cluster);
swapfile.c:	p->percpu_cluster = NULL;
swapfile.c:	swap_cgroup_swapoff(p->type);
swapfile.c:	exit_swap_address_space(p->type);
swapfile.c:	inode = mapping->host;
swapfile.c:	if (S_ISBLK(inode->i_mode)) {
swapfile.c:		inode->i_flags &= ~S_SWAPFILE;
swapfile.c:	 * not hold p->lock after we cleared its SWP_WRITEOK.
swapfile.c:	p->flags = 0;
swapfile.c:	struct seq_file *seq = file->private_data;
swapfile.c:	if (seq->poll_event != atomic_read(&proc_poll_event)) {
swapfile.c:		seq->poll_event = atomic_read(&proc_poll_event);
swapfile.c:		if (!(si->flags & SWP_USED) || !si->swap_map)
swapfile.c:		if (!--l)
swapfile.c:		type = si->type + 1;
swapfile.c:		if (!(si->flags & SWP_USED) || !si->swap_map)
swapfile.c:	file = si->swap_file;
swapfile.c:			len < 40 ? 40 - len : 1, " ",
swapfile.c:			S_ISBLK(file_inode(file)->i_mode) ?
swapfile.c:			si->pages << (PAGE_SHIFT - 10),
swapfile.c:			si->inuse_pages << (PAGE_SHIFT - 10),
swapfile.c:			si->prio);
swapfile.c:	seq = file->private_data;
swapfile.c:	seq->poll_event = atomic_read(&proc_poll_event);
swapfile.c:		return ERR_PTR(-ENOMEM);
swapfile.c:		if (!(swap_info[type]->flags & SWP_USED))
swapfile.c:		return ERR_PTR(-EPERM);
swapfile.c:		p->type = type;
swapfile.c:		 * would be relying on p->type to remain valid.
swapfile.c:	INIT_LIST_HEAD(&p->first_swap_extent.list);
swapfile.c:	plist_node_init(&p->list, 0);
swapfile.c:		plist_node_init(&p->avail_lists[i], 0);
swapfile.c:	p->flags = SWP_USED;
swapfile.c:	spin_lock_init(&p->lock);
swapfile.c:	spin_lock_init(&p->cont_lock);
swapfile.c:	if (S_ISBLK(inode->i_mode)) {
swapfile.c:		p->bdev = bdgrab(I_BDEV(inode));
swapfile.c:		error = blkdev_get(p->bdev,
swapfile.c:			p->bdev = NULL;
swapfile.c:		p->old_block_size = block_size(p->bdev);
swapfile.c:		error = set_blocksize(p->bdev, PAGE_SIZE);
swapfile.c:		p->flags |= SWP_BLKDEV;
swapfile.c:	} else if (S_ISREG(inode->i_mode)) {
swapfile.c:		p->bdev = inode->i_sb->s_bdev;
swapfile.c:			return -EBUSY;
swapfile.c:		return -EINVAL;
swapfile.c:	if (memcmp("SWAPSPACE2", swap_header->magic.magic, 10)) {
swapfile.c:		pr_err("Unable to find swap-space signature\n");
swapfile.c:	if (swab32(swap_header->info.version) == 1) {
swapfile.c:		swab32s(&swap_header->info.version);
swapfile.c:		swab32s(&swap_header->info.last_page);
swapfile.c:		swab32s(&swap_header->info.nr_badpages);
swapfile.c:		if (swap_header->info.nr_badpages > MAX_SWAP_BADPAGES)
swapfile.c:		for (i = 0; i < swap_header->info.nr_badpages; i++)
swapfile.c:			swab32s(&swap_header->info.badpages[i]);
swapfile.c:	/* Check the swap header's sub-version */
swapfile.c:	if (swap_header->info.version != 1) {
swapfile.c:			swap_header->info.version);
swapfile.c:	p->lowest_bit  = 1;
swapfile.c:	p->cluster_next = 1;
swapfile.c:	p->cluster_nr = 0;
swapfile.c:	last_page = swap_header->info.last_page;
swapfile.c:		pr_warn("Empty swap-file\n");
swapfile.c:			maxpages << (PAGE_SHIFT - 10),
swapfile.c:			last_page << (PAGE_SHIFT - 10));
swapfile.c:		/* p->max is an unsigned int: don't overflow it */
swapfile.c:	p->highest_bit = maxpages - 1;
swapfile.c:	if (swap_header->info.nr_badpages && S_ISREG(inode->i_mode))
swapfile.c:	if (swap_header->info.nr_badpages > MAX_SWAP_BADPAGES)
swapfile.c:	unsigned long col = p->cluster_next / SWAPFILE_CLUSTER % SWAP_CLUSTER_COLS;
swapfile.c:	nr_good_pages = maxpages - 1;	/* omit header page */
swapfile.c:	cluster_list_init(&p->free_clusters);
swapfile.c:	cluster_list_init(&p->discard_clusters);
swapfile.c:	for (i = 0; i < swap_header->info.nr_badpages; i++) {
swapfile.c:		unsigned int page_nr = swap_header->info.badpages[i];
swapfile.c:		if (page_nr == 0 || page_nr > swap_header->info.last_page)
swapfile.c:			return -EINVAL;
swapfile.c:			nr_good_pages--;
swapfile.c:		p->max = maxpages;
swapfile.c:		p->pages = nr_good_pages;
swapfile.c:		nr_good_pages = p->pages;
swapfile.c:		pr_warn("Empty swap-file\n");
swapfile.c:		return -EINVAL;
swapfile.c:			cluster_list_add_tail(&p->free_clusters, cluster_info,
swapfile.c:	struct request_queue *q = bdev_get_queue(si->bdev);
swapfile.c:		return -EINVAL;
swapfile.c:		return -EPERM;
swapfile.c:		return -ENOMEM;
swapfile.c:	INIT_WORK(&p->discard_work, swap_discard_work);
swapfile.c:	p->swap_file = swap_file;
swapfile.c:	mapping = swap_file->f_mapping;
swapfile.c:	inode = mapping->host;
swapfile.c:	/* If S_ISREG(inode->i_mode) will do inode_lock(inode); */
swapfile.c:	if (!mapping->a_ops->readpage) {
swapfile.c:		error = -EINVAL;
swapfile.c:		error = -EINVAL;
swapfile.c:		error = -ENOMEM;
swapfile.c:		p->flags |= SWP_STABLE_WRITES;
swapfile.c:	if (p->bdev && blk_queue_nonrot(bdev_get_queue(p->bdev))) {
swapfile.c:		p->flags |= SWP_SOLIDSTATE;
swapfile.c:		p->cluster_next = 1 + (prandom_u32() % p->highest_bit);
swapfile.c:			error = -ENOMEM;
swapfile.c:			spin_lock_init(&((cluster_info + ci)->lock));
swapfile.c:		p->percpu_cluster = alloc_percpu(struct percpu_cluster);
swapfile.c:		if (!p->percpu_cluster) {
swapfile.c:			error = -ENOMEM;
swapfile.c:			cluster = per_cpu_ptr(p->percpu_cluster, cpu);
swapfile.c:			cluster_set_null(&cluster->index);
swapfile.c:	error = swap_cgroup_swapon(p->type, maxpages);
swapfile.c:	/* frontswap enabled? set up bit-per-page map for frontswap */
swapfile.c:	if (p->bdev &&(swap_flags & SWAP_FLAG_DISCARD) && swap_discardable(p)) {
swapfile.c:		p->flags |= (SWP_DISCARDABLE | SWP_AREA_DISCARD |
swapfile.c:		 * either do single-time area discards only, or to just
swapfile.c:		 * perform discards for released swap page-clusters.
swapfile.c:		 * Now it's time to adjust the p->flags accordingly.
swapfile.c:			p->flags &= ~SWP_PAGE_DISCARD;
swapfile.c:			p->flags &= ~SWP_AREA_DISCARD;
swapfile.c:		/* issue a swapon-time discard if it's still required */
swapfile.c:		if (p->flags & SWP_AREA_DISCARD) {
swapfile.c:	error = init_swap_address_space(p->type, maxpages);
swapfile.c:	prio = -1;
swapfile.c:		p->pages<<(PAGE_SHIFT-10), name->name, p->prio,
swapfile.c:		nr_extents, (unsigned long long)span<<(PAGE_SHIFT-10),
swapfile.c:		(p->flags & SWP_SOLIDSTATE) ? "SS" : "",
swapfile.c:		(p->flags & SWP_DISCARDABLE) ? "D" : "",
swapfile.c:		(p->flags & SWP_AREA_DISCARD) ? "s" : "",
swapfile.c:		(p->flags & SWP_PAGE_DISCARD) ? "c" : "",
swapfile.c:	if (S_ISREG(inode->i_mode))
swapfile.c:		inode->i_flags |= S_SWAPFILE;
swapfile.c:	free_percpu(p->percpu_cluster);
swapfile.c:	p->percpu_cluster = NULL;
swapfile.c:	if (inode && S_ISBLK(inode->i_mode) && p->bdev) {
swapfile.c:		set_blocksize(p->bdev, p->old_block_size);
swapfile.c:		blkdev_put(p->bdev, FMODE_READ | FMODE_WRITE | FMODE_EXCL);
swapfile.c:	swap_cgroup_swapoff(p->type);
swapfile.c:	p->swap_file = NULL;
swapfile.c:	p->flags = 0;
swapfile.c:		if (inode && S_ISREG(inode->i_mode)) {
swapfile.c:	if (inode && S_ISREG(inode->i_mode))
swapfile.c:		if ((si->flags & SWP_USED) && !(si->flags & SWP_WRITEOK))
swapfile.c:			nr_to_be_unused += si->inuse_pages;
swapfile.c:	val->freeswap = atomic_long_read(&nr_swap_pages) + nr_to_be_unused;
swapfile.c:	val->totalswap = total_swap_pages + nr_to_be_unused;
swapfile.c: * - success -> 0
swapfile.c: * - swp_entry is invalid -> EINVAL
swapfile.c: * - swp_entry is migration entry -> EINVAL
swapfile.c: * - swap-cache reference is requested but there is already one. -> EEXIST
swapfile.c: * - swap-cache reference is requested but the entry is not used. -> ENOENT
swapfile.c: * - swap-mapped reference requested but needs continued swap count. -> ENOMEM
swapfile.c:	int err = -EINVAL;
swapfile.c:	if (unlikely(offset >= p->max))
swapfile.c:	count = p->swap_map[offset];
swapfile.c:		err = -ENOENT;
swapfile.c:			err = -EEXIST;
swapfile.c:			err = -ENOENT;
swapfile.c:			err = -EINVAL;
swapfile.c:			err = -ENOMEM;
swapfile.c:		err = -ENOENT;			/* unused swap entry */
swapfile.c:	p->swap_map[offset] = count | has_cache;
swapfile.c: * Returns 0 for success, or -ENOMEM if a swap_count_continuation is required
swapfile.c: * if __swap_duplicate() fails for another reason (-EINVAL or -ENOENT), which
swapfile.c:	while (!err && __swap_duplicate(entry, 1) == -ENOMEM)
swapfile.c: * -EBUSY means there is a swap cache.
swapfile.c: * out-of-line __page_file_ methods to avoid include hell.
swapfile.c:	return page_swap_info(page)->swap_file->f_mapping;
swapfile.c: * add_swap_count_continuation - called when a swap count is duplicated
swapfile.c:	count = si->swap_map[offset] & ~SWAP_HAS_CACHE;
swapfile.c:		 * over-provisioning.
swapfile.c:		spin_unlock(&si->lock);
swapfile.c:		return -ENOMEM;
swapfile.c:	head = vmalloc_to_page(si->swap_map + offset);
swapfile.c:	spin_lock(&si->cont_lock);
swapfile.c:		INIT_LIST_HEAD(&head->lru);
swapfile.c:		si->flags |= SWP_CONTINUED;
swapfile.c:	list_for_each_entry(list_page, &head->lru, lru) {
swapfile.c:	list_add_tail(&page->lru, &head->lru);
swapfile.c:	spin_unlock(&si->cont_lock);
swapfile.c:	spin_unlock(&si->lock);
swapfile.c: * swap_count_continued - when the original swap_map count is incremented
swapfile.c:	head = vmalloc_to_page(si->swap_map + offset);
swapfile.c:	spin_lock(&si->cont_lock);
swapfile.c:	page = list_entry(head->lru.next, struct page, lru);
swapfile.c:			page = list_entry(page->lru.next, struct page, lru);
swapfile.c:			page = list_entry(page->lru.next, struct page, lru);
swapfile.c:		page = list_entry(page->lru.prev, struct page, lru);
swapfile.c:			page = list_entry(page->lru.prev, struct page, lru);
swapfile.c:			page = list_entry(page->lru.next, struct page, lru);
swapfile.c:		*map -= 1;
swapfile.c:		page = list_entry(page->lru.prev, struct page, lru);
swapfile.c:			page = list_entry(page->lru.prev, struct page, lru);
swapfile.c:	spin_unlock(&si->cont_lock);
swapfile.c: * free_swap_count_continuations - swapoff free all the continuation pages
swapfile.c:	for (offset = 0; offset < si->max; offset += PAGE_SIZE) {
swapfile.c:		head = vmalloc_to_page(si->swap_map + offset);
swapfile.c:			list_for_each_entry_safe(page, next, &head->lru, lru) {
swapfile.c:				list_del(&page->lru);
swapfile.c:		return -ENOMEM;
mempool.c:// SPDX-License-Identifier: GPL-2.0
mempool.c: *  for guaranteed, deadlock-free memory allocations during
mempool.c:	const int nr = pool->curr_nr;
mempool.c:	const int start = max_t(int, byte - (BITS_PER_LONG / 8), 0);
mempool.c:		u8 exp = (i < size - 1) ? POISON_FREE : POISON_END;
mempool.c:	if (pool->free == mempool_free_slab || pool->free == mempool_kfree)
mempool.c:	if (pool->free == mempool_free_pages) {
mempool.c:		int order = (int)(long)pool->pool_data;
mempool.c:	memset(obj, POISON_FREE, size - 1);
mempool.c:	obj[size - 1] = POISON_END;
mempool.c:	if (pool->alloc == mempool_alloc_slab || pool->alloc == mempool_kmalloc)
mempool.c:	if (pool->alloc == mempool_alloc_pages) {
mempool.c:		int order = (int)(long)pool->pool_data;
mempool.c:	if (pool->alloc == mempool_alloc_slab || pool->alloc == mempool_kmalloc)
mempool.c:	if (pool->alloc == mempool_alloc_pages)
mempool.c:		kasan_free_pages(element, (unsigned long)pool->pool_data);
mempool.c:	if (pool->alloc == mempool_alloc_slab || pool->alloc == mempool_kmalloc)
mempool.c:	if (pool->alloc == mempool_alloc_pages)
mempool.c:		kasan_alloc_pages(element, (unsigned long)pool->pool_data);
mempool.c:	BUG_ON(pool->curr_nr >= pool->min_nr);
mempool.c:	pool->elements[pool->curr_nr++] = element;
mempool.c:	void *element = pool->elements[--pool->curr_nr];
mempool.c:	BUG_ON(pool->curr_nr < 0);
mempool.c: * mempool_destroy - deallocate a memory pool
mempool.c:	while (pool->curr_nr) {
mempool.c:		pool->free(element, pool->pool_data);
mempool.c:	kfree(pool->elements);
mempool.c: * mempool_create - create a memory pool
mempool.c: * @alloc_fn:  user-defined element-allocation function.
mempool.c: * @free_fn:   user-defined element-freeing function.
mempool.c: * @pool_data: optional private data available to the user-defined functions.
mempool.c: * functions might sleep - as long as the mempool_alloc() function is not called
mempool.c:	pool->elements = kmalloc_node(min_nr * sizeof(void *),
mempool.c:	if (!pool->elements) {
mempool.c:	spin_lock_init(&pool->lock);
mempool.c:	pool->min_nr = min_nr;
mempool.c:	pool->pool_data = pool_data;
mempool.c:	init_waitqueue_head(&pool->wait);
mempool.c:	pool->alloc = alloc_fn;
mempool.c:	pool->free = free_fn;
mempool.c:	 * First pre-allocate the guaranteed number of buffers.
mempool.c:	while (pool->curr_nr < pool->min_nr) {
mempool.c:		element = pool->alloc(gfp_mask, pool->pool_data);
mempool.c: * mempool_resize - resize an existing memory pool
mempool.c:	spin_lock_irqsave(&pool->lock, flags);
mempool.c:	if (new_min_nr <= pool->min_nr) {
mempool.c:		while (new_min_nr < pool->curr_nr) {
mempool.c:			spin_unlock_irqrestore(&pool->lock, flags);
mempool.c:			pool->free(element, pool->pool_data);
mempool.c:			spin_lock_irqsave(&pool->lock, flags);
mempool.c:		pool->min_nr = new_min_nr;
mempool.c:	spin_unlock_irqrestore(&pool->lock, flags);
mempool.c:		return -ENOMEM;
mempool.c:	spin_lock_irqsave(&pool->lock, flags);
mempool.c:	if (unlikely(new_min_nr <= pool->min_nr)) {
mempool.c:		spin_unlock_irqrestore(&pool->lock, flags);
mempool.c:	memcpy(new_elements, pool->elements,
mempool.c:			pool->curr_nr * sizeof(*new_elements));
mempool.c:	kfree(pool->elements);
mempool.c:	pool->elements = new_elements;
mempool.c:	pool->min_nr = new_min_nr;
mempool.c:	while (pool->curr_nr < pool->min_nr) {
mempool.c:		spin_unlock_irqrestore(&pool->lock, flags);
mempool.c:		element = pool->alloc(GFP_KERNEL, pool->pool_data);
mempool.c:		spin_lock_irqsave(&pool->lock, flags);
mempool.c:		if (pool->curr_nr < pool->min_nr) {
mempool.c:			spin_unlock_irqrestore(&pool->lock, flags);
mempool.c:			pool->free(element, pool->pool_data);	/* Raced */
mempool.c:	spin_unlock_irqrestore(&pool->lock, flags);
mempool.c: * mempool_alloc - allocate an element from a specific memory pool
mempool.c:	element = pool->alloc(gfp_temp, pool->pool_data);
mempool.c:	spin_lock_irqsave(&pool->lock, flags);
mempool.c:	if (likely(pool->curr_nr)) {
mempool.c:		spin_unlock_irqrestore(&pool->lock, flags);
mempool.c:		spin_unlock_irqrestore(&pool->lock, flags);
mempool.c:		spin_unlock_irqrestore(&pool->lock, flags);
mempool.c:	prepare_to_wait(&pool->wait, &wait, TASK_UNINTERRUPTIBLE);
mempool.c:	spin_unlock_irqrestore(&pool->lock, flags);
mempool.c:	finish_wait(&pool->wait, &wait);
mempool.c: * mempool_free - return an element to the pool.
mempool.c:	 * for @element and the following @pool->curr_nr.  This ensures
mempool.c:	 * that the visible value of @pool->curr_nr is from after the
mempool.c:	if (unlikely(pool->curr_nr < pool->min_nr)) {
mempool.c:		spin_lock_irqsave(&pool->lock, flags);
mempool.c:		if (likely(pool->curr_nr < pool->min_nr)) {
mempool.c:			spin_unlock_irqrestore(&pool->lock, flags);
mempool.c:			wake_up(&pool->wait);
mempool.c:		spin_unlock_irqrestore(&pool->lock, flags);
mempool.c:	pool->free(element, pool->pool_data);
mempool.c:	VM_BUG_ON(mem->ctor);
mempool.c: * A simple mempool-backed page allocator that allocates pages
fadvise.c:// SPDX-License-Identifier: GPL-2.0
fadvise.c:#include <linux/backing-dev.h>
fadvise.c:		return -EBADF;
fadvise.c:	if (S_ISFIFO(inode->i_mode)) {
fadvise.c:		ret = -ESPIPE;
fadvise.c:	mapping = f.file->f_mapping;
fadvise.c:		ret = -EINVAL;
fadvise.c:	bdi = inode_to_bdi(mapping->host);
fadvise.c:			ret = -EINVAL;
fadvise.c:		endbyte = -1;
fadvise.c:		endbyte--;		/* inclusive */
fadvise.c:		f.file->f_ra.ra_pages = bdi->ra_pages;
fadvise.c:		spin_lock(&f.file->f_lock);
fadvise.c:		f.file->f_mode &= ~FMODE_RANDOM;
fadvise.c:		spin_unlock(&f.file->f_lock);
fadvise.c:		spin_lock(&f.file->f_lock);
fadvise.c:		f.file->f_mode |= FMODE_RANDOM;
fadvise.c:		spin_unlock(&f.file->f_lock);
fadvise.c:		f.file->f_ra.ra_pages = bdi->ra_pages * 2;
fadvise.c:		spin_lock(&f.file->f_lock);
fadvise.c:		f.file->f_mode &= ~FMODE_RANDOM;
fadvise.c:		spin_unlock(&f.file->f_lock);
fadvise.c:		nrpages = end_index - start_index + 1;
fadvise.c:		if (!inode_write_congested(mapping->host))
fadvise.c:		start_index = (offset+(PAGE_SIZE-1)) >> PAGE_SHIFT;
fadvise.c:		 * that page - discarding the last page is safe enough.
fadvise.c:				endbyte != inode->i_size - 1) {
fadvise.c:			/* First page is tricky as 0 - 1 = -1, but pgoff_t
fadvise.c:			end_index--;
fadvise.c:			 * a per-cpu pagevec for a remote CPU. Drain all
fadvise.c:			if (count < (end_index - start_index + 1)) {
fadvise.c:		ret = -EINVAL;
nobootmem.c:// SPDX-License-Identifier: GPL-2.0
nobootmem.c: *  bootmem - A boot-time physical memory allocator and configurator
nobootmem.c: * free_bootmem_late - free bootmem pages directly to page allocator
nobootmem.c:		order = min(MAX_ORDER - 1UL, __ffs(start));
nobootmem.c:			order--;
nobootmem.c:	return end_pfn - start_pfn;
nobootmem.c:	memblock_clear_hotplug(0, -1);
nobootmem.c:	 * We need to use NUMA_NO_NODE instead of NODE_DATA(0)->node_id
nobootmem.c:	for (z = pgdat->node_zones; z < pgdat->node_zones + MAX_NR_ZONES; z++)
nobootmem.c:		z->managed_pages = 0;
nobootmem.c: * free_all_bootmem - release free pages to the buddy allocator
nobootmem.c: * free_bootmem_node - mark a page range as usable
nobootmem.c: * free_bootmem - mark a page range as usable
nobootmem.c: * __alloc_bootmem_nopanic - allocate boot memory without panicking
nobootmem.c:	unsigned long limit = -1UL;
nobootmem.c: * __alloc_bootmem - allocate boot memory
nobootmem.c:	unsigned long limit = -1UL;
nobootmem.c:	ptr = __alloc_memory_core_early(pgdat->node_id, size, align,
nobootmem.c:		return kzalloc_node(size, GFP_NOWAIT, pgdat->node_id);
nobootmem.c: * __alloc_bootmem_node - allocate boot memory from a specific node
nobootmem.c:		return kzalloc_node(size, GFP_NOWAIT, pgdat->node_id);
nobootmem.c: * __alloc_bootmem_low - allocate low boot memory
nobootmem.c: * __alloc_bootmem_low_node - allocate low boot memory from a specific node
nobootmem.c:		return kzalloc_node(size, GFP_NOWAIT, pgdat->node_id);
mempolicy.c: * run-time system-wide default policy => local allocation
mempolicy.c:	struct mempolicy *pol = p->mempolicy;
mempolicy.c:		if (pol->mode)
mempolicy.c:	return pol->flags & MPOL_MODE_FLAGS;
mempolicy.c:		return -EINVAL;
mempolicy.c:	pol->v.nodes = *nodes;
mempolicy.c:		pol->flags |= MPOL_F_LOCAL;	/* local allocation */
mempolicy.c:		return -EINVAL;			/*  no allowed nodes */
mempolicy.c:		pol->v.preferred_node = first_node(*nodes);
mempolicy.c:		return -EINVAL;
mempolicy.c:	pol->v.nodes = *nodes;
mempolicy.c:	nodes_and(nsc->mask1,
mempolicy.c:	if (pol->mode == MPOL_PREFERRED && nodes_empty(*nodes))
mempolicy.c:		if (pol->flags & MPOL_F_RELATIVE_NODES)
mempolicy.c:			mpol_relative_nodemask(&nsc->mask2, nodes, &nsc->mask1);
mempolicy.c:			nodes_and(nsc->mask2, *nodes, nsc->mask1);
mempolicy.c:			pol->w.user_nodemask = *nodes;
mempolicy.c:			pol->w.cpuset_mems_allowed =
mempolicy.c:		ret = mpol_ops[pol->mode].create(pol, &nsc->mask2);
mempolicy.c:		ret = mpol_ops[pol->mode].create(pol, NULL);
mempolicy.c:			return ERR_PTR(-EINVAL);
mempolicy.c:	 * All other modes require a valid pointer to a non-empty nodemask.
mempolicy.c:				return ERR_PTR(-EINVAL);
mempolicy.c:			return ERR_PTR(-EINVAL);
mempolicy.c:		return ERR_PTR(-EINVAL);
mempolicy.c:		return ERR_PTR(-ENOMEM);
mempolicy.c:	atomic_set(&policy->refcnt, 1);
mempolicy.c:	policy->mode = mode;
mempolicy.c:	policy->flags = flags;
mempolicy.c:	if (!atomic_dec_and_test(&p->refcnt))
mempolicy.c:	if (pol->flags & MPOL_F_STATIC_NODES)
mempolicy.c:		nodes_and(tmp, pol->w.user_nodemask, *nodes);
mempolicy.c:	else if (pol->flags & MPOL_F_RELATIVE_NODES)
mempolicy.c:		mpol_relative_nodemask(&tmp, &pol->w.user_nodemask, nodes);
mempolicy.c:		nodes_remap(tmp, pol->v.nodes,pol->w.cpuset_mems_allowed,
mempolicy.c:		pol->w.cpuset_mems_allowed = tmp;
mempolicy.c:	pol->v.nodes = tmp;
mempolicy.c:	if (pol->flags & MPOL_F_STATIC_NODES) {
mempolicy.c:		int node = first_node(pol->w.user_nodemask);
mempolicy.c:			pol->v.preferred_node = node;
mempolicy.c:			pol->flags &= ~MPOL_F_LOCAL;
mempolicy.c:			pol->flags |= MPOL_F_LOCAL;
mempolicy.c:	} else if (pol->flags & MPOL_F_RELATIVE_NODES) {
mempolicy.c:		mpol_relative_nodemask(&tmp, &pol->w.user_nodemask, nodes);
mempolicy.c:		pol->v.preferred_node = first_node(tmp);
mempolicy.c:	} else if (!(pol->flags & MPOL_F_LOCAL)) {
mempolicy.c:		pol->v.preferred_node = node_remap(pol->v.preferred_node,
mempolicy.c:						   pol->w.cpuset_mems_allowed,
mempolicy.c:		pol->w.cpuset_mems_allowed = *nodes;
mempolicy.c: * mpol_rebind_policy - Migrate a policy to a different set of nodes
mempolicy.c: * Per-vma policies are protected by mmap_sem. Allocations using per-task
mempolicy.c: * policies are protected by task->mems_allowed_seq to prevent a premature
mempolicy.c:	    nodes_equal(pol->w.cpuset_mems_allowed, *newmask))
mempolicy.c:	mpol_ops[pol->mode].rebind(pol, newmask);
mempolicy.c:	mpol_rebind_policy(tsk->mempolicy, new);
mempolicy.c: * Call holding a reference to mm.  Takes mm->mmap_sem during call.
mempolicy.c:	down_write(&mm->mmap_sem);
mempolicy.c:	for (vma = mm->mmap; vma; vma = vma->vm_next)
mempolicy.c:		mpol_rebind_policy(vma->vm_policy, new);
mempolicy.c:	up_write(&mm->mmap_sem);
mempolicy.c: * Check if the page's nid is in qp->nmask.
mempolicy.c: * If MPOL_MF_INVERT is set in qp->flags, check if the nid is
mempolicy.c: * in the invert of qp->nmask.
mempolicy.c:	unsigned long flags = qp->flags;
mempolicy.c:	return node_isset(nid, *qp->nmask) == !(flags & MPOL_MF_INVERT);
mempolicy.c:	struct queue_pages *qp = walk->private;
mempolicy.c:		__split_huge_pmd(walk->vma, pmd, addr, false, NULL);
mempolicy.c:	flags = qp->flags;
mempolicy.c:		migrate_page_add(page, qp->pagelist, flags);
mempolicy.c:	struct vm_area_struct *vma = walk->vma;
mempolicy.c:	struct queue_pages *qp = walk->private;
mempolicy.c:	unsigned long flags = qp->flags;
mempolicy.c:	pte = pte_offset_map_lock(walk->mm, pmd, addr, &ptl);
mempolicy.c:			/* Failed to split -- skip. */
mempolicy.c:				pte = pte_offset_map_lock(walk->mm, pmd,
mempolicy.c:		migrate_page_add(page, qp->pagelist, flags);
mempolicy.c:	pte_unmap_unlock(pte - 1, ptl);
mempolicy.c:	struct queue_pages *qp = walk->private;
mempolicy.c:	unsigned long flags = qp->flags;
mempolicy.c:	ptl = huge_pte_lock(hstate_vma(walk->vma), walk->mm, pte);
mempolicy.c:		isolate_huge_page(page, qp->pagelist);
mempolicy.c:	struct vm_area_struct *vma = walk->vma;
mempolicy.c:	struct queue_pages *qp = walk->private;
mempolicy.c:	unsigned long endvma = vma->vm_end;
mempolicy.c:	unsigned long flags = qp->flags;
mempolicy.c:	if (vma->vm_start > start)
mempolicy.c:		start = vma->vm_start;
mempolicy.c:		if (!vma->vm_next && vma->vm_end < end)
mempolicy.c:			return -EFAULT;
mempolicy.c:		if (qp->prev && qp->prev->vm_end < vma->vm_start)
mempolicy.c:			return -EFAULT;
mempolicy.c:	qp->prev = vma;
mempolicy.c:			(vma->vm_flags & (VM_READ | VM_EXEC | VM_WRITE)) &&
mempolicy.c:			!(vma->vm_flags & VM_MIXEDMAP))
mempolicy.c:	pr_debug("vma %lx-%lx/%lx vm_ops %p vm_file %p set_policy %p\n",
mempolicy.c:		 vma->vm_start, vma->vm_end, vma->vm_pgoff,
mempolicy.c:		 vma->vm_ops, vma->vm_file,
mempolicy.c:		 vma->vm_ops ? vma->vm_ops->set_policy : NULL);
mempolicy.c:	if (vma->vm_ops && vma->vm_ops->set_policy) {
mempolicy.c:		err = vma->vm_ops->set_policy(vma, new);
mempolicy.c:	old = vma->vm_policy;
mempolicy.c:	vma->vm_policy = new; /* protected by mmap_sem */
mempolicy.c:	if (!vma || vma->vm_start > start)
mempolicy.c:		return -EFAULT;
mempolicy.c:	prev = vma->vm_prev;
mempolicy.c:	if (start > vma->vm_start)
mempolicy.c:	for (; vma && vma->vm_start < end; prev = vma, vma = next) {
mempolicy.c:		next = vma->vm_next;
mempolicy.c:		vmstart = max(start, vma->vm_start);
mempolicy.c:		vmend   = min(end, vma->vm_end);
mempolicy.c:		pgoff = vma->vm_pgoff +
mempolicy.c:			((vmstart - vma->vm_start) >> PAGE_SHIFT);
mempolicy.c:		prev = vma_merge(mm, prev, vmstart, vmend, vma->vm_flags,
mempolicy.c:				 vma->anon_vma, vma->vm_file, pgoff,
mempolicy.c:				 new_pol, vma->vm_userfaultfd_ctx);
mempolicy.c:			next = vma->vm_next;
mempolicy.c:			/* vma_merge() joined vma && vma->next, case 8 */
mempolicy.c:		if (vma->vm_start != vmstart) {
mempolicy.c:			err = split_vma(vma->vm_mm, vma, vmstart, 1);
mempolicy.c:		if (vma->vm_end != vmend) {
mempolicy.c:			err = split_vma(vma->vm_mm, vma, vmend, 0);
mempolicy.c:		return -ENOMEM;
mempolicy.c:	old = current->mempolicy;
mempolicy.c:	current->mempolicy = new;
mempolicy.c:	if (new && new->mode == MPOL_INTERLEAVE)
mempolicy.c:		current->il_prev = MAX_NUMNODES-1;
mempolicy.c:	switch (p->mode) {
mempolicy.c:		*nodes = p->v.nodes;
mempolicy.c:		if (!(p->flags & MPOL_F_LOCAL))
mempolicy.c:			node_set(p->v.preferred_node, *nodes);
mempolicy.c:	struct mm_struct *mm = current->mm;
mempolicy.c:	struct mempolicy *pol = current->mempolicy;
mempolicy.c:		return -EINVAL;
mempolicy.c:			return -EINVAL;
mempolicy.c:		down_read(&mm->mmap_sem);
mempolicy.c:			up_read(&mm->mmap_sem);
mempolicy.c:			return -EFAULT;
mempolicy.c:		if (vma->vm_ops && vma->vm_ops->get_policy)
mempolicy.c:			pol = vma->vm_ops->get_policy(vma, addr);
mempolicy.c:			pol = vma->vm_policy;
mempolicy.c:		return -EINVAL;
mempolicy.c:		} else if (pol == current->mempolicy &&
mempolicy.c:				pol->mode == MPOL_INTERLEAVE) {
mempolicy.c:			*policy = next_node_in(current->il_prev, pol->v.nodes);
mempolicy.c:			err = -EINVAL;
mempolicy.c:						pol->mode;
mempolicy.c:		*policy |= (pol->flags & MPOL_MODE_FLAGS);
mempolicy.c:			*nmask = pol->w.user_nodemask;
mempolicy.c:		up_read(&current->mm->mmap_sem);
mempolicy.c:			list_add_tail(&head->lru, pagelist);
mempolicy.c:	queue_pages_range(mm, mm->mmap->vm_start, mm->task_size, &nmask,
mempolicy.c:	down_read(&mm->mmap_sem);
mempolicy.c:			 * Example: [2,3,4] -> [3,4,5] moves everything.
mempolicy.c:			 *          [0-7] - > [3,4,5] moves only 0,1,2,6,7.
mempolicy.c:	up_read(&mm->mmap_sem);
mempolicy.c: * list of pages handed to migrate_pages()--which is how we get here--
mempolicy.c:	vma = find_vma(current->mm, start);
mempolicy.c:		if (address != -EFAULT)
mempolicy.c:		vma = vma->vm_next;
mempolicy.c:	return -ENOSYS;
mempolicy.c:	struct mm_struct *mm = current->mm;
mempolicy.c:		return -EINVAL;
mempolicy.c:		return -EPERM;
mempolicy.c:		return -EINVAL;
mempolicy.c:	len = (len + PAGE_SIZE - 1) & PAGE_MASK;
mempolicy.c:		return -EINVAL;
mempolicy.c:		new->flags |= MPOL_F_MOF;
mempolicy.c:	pr_debug("mbind %lx-%lx mode:%d flags:%d nodes:%lx\n",
mempolicy.c:			down_write(&mm->mmap_sem);
mempolicy.c:				up_write(&mm->mmap_sem);
mempolicy.c:			err = -ENOMEM;
mempolicy.c:			err = -EIO;
mempolicy.c:	up_write(&mm->mmap_sem);
mempolicy.c:	--maxnode;
mempolicy.c:		return -EINVAL;
mempolicy.c:		endmask = (1UL << (maxnode % BITS_PER_LONG)) - 1;
mempolicy.c:			return -EINVAL;
mempolicy.c:				return -EFAULT;
mempolicy.c:			if (k == nlongs - 1) {
mempolicy.c:					return -EINVAL;
mempolicy.c:				return -EINVAL;
mempolicy.c:		valid_mask &= ~((1UL << (MAX_NUMNODES % BITS_PER_LONG)) - 1);
mempolicy.c:		if (get_user(t, nmask + nlongs - 1))
mempolicy.c:			return -EFAULT;
mempolicy.c:			return -EINVAL;
mempolicy.c:		return -EFAULT;
mempolicy.c:	nodes_addr(*nodes)[nlongs-1] &= endmask;
mempolicy.c:	unsigned long copy = ALIGN(maxnode-1, 64) / 8;
mempolicy.c:			return -EINVAL;
mempolicy.c:		if (clear_user((char __user *)mask + nbytes, copy - nbytes))
mempolicy.c:			return -EFAULT;
mempolicy.c:	return copy_to_user(mask, nodes_addr(*nodes), copy) ? -EFAULT : 0;
mempolicy.c:		return -EINVAL;
mempolicy.c:		return -EINVAL;
mempolicy.c:		return -EINVAL;
mempolicy.c:		return -EINVAL;
mempolicy.c:		return -ENOMEM;
mempolicy.c:	old = &scratch->mask1;
mempolicy.c:	new = &scratch->mask2;
mempolicy.c:		err = -ESRCH;
mempolicy.c:	err = -EINVAL;
mempolicy.c:	if (!uid_eq(cred->euid, tcred->suid) && !uid_eq(cred->euid, tcred->uid) &&
mempolicy.c:	    !uid_eq(cred->uid,  tcred->suid) && !uid_eq(cred->uid,  tcred->uid) &&
mempolicy.c:		err = -EPERM;
mempolicy.c:		err = -EPERM;
mempolicy.c:		err = -EINVAL;
mempolicy.c:		return -EINVAL;
mempolicy.c:		return -EFAULT;
mempolicy.c:	nr_bits = min_t(unsigned long, maxnode-1, MAX_NUMNODES);
mempolicy.c:		err |= clear_user(nmask, ALIGN(maxnode-1, 8) / 8);
mempolicy.c:	nr_bits = min_t(unsigned long, maxnode-1, MAX_NUMNODES);
mempolicy.c:			return -EFAULT;
mempolicy.c:			return -EFAULT;
mempolicy.c:	nr_bits = min_t(unsigned long, maxnode-1, MAX_NUMNODES);
mempolicy.c:			return -EFAULT;
mempolicy.c:			return -EFAULT;
mempolicy.c:		if (vma->vm_ops && vma->vm_ops->get_policy) {
mempolicy.c:			pol = vma->vm_ops->get_policy(vma, addr);
mempolicy.c:		} else if (vma->vm_policy) {
mempolicy.c:			pol = vma->vm_policy;
mempolicy.c:			 * a pseudo vma whose vma->vm_ops=NULL. Take a reference
mempolicy.c: * Falls back to current->mempolicy or system default policy, as necessary.
mempolicy.c: * count--added by the get_policy() vm_op, as appropriate--to protect against
mempolicy.c:	if (vma->vm_ops && vma->vm_ops->get_policy) {
mempolicy.c:		pol = vma->vm_ops->get_policy(vma, vma->vm_start);
mempolicy.c:		if (pol && (pol->flags & MPOL_F_MOF))
mempolicy.c:	pol = vma->vm_policy;
mempolicy.c:	return pol->flags & MPOL_F_MOF;
mempolicy.c:	 * if policy->v.nodes has movable memory only,
mempolicy.c:	 * policy->v.nodes is intersect with node_states[N_MEMORY].
mempolicy.c:	 * policy->v.nodes has movable memory only.
mempolicy.c:	if (!nodes_intersects(policy->v.nodes, node_states[N_HIGH_MEMORY]))
mempolicy.c:	if (unlikely(policy->mode == MPOL_BIND) &&
mempolicy.c:			cpuset_nodemask_valid_mems_allowed(&policy->v.nodes))
mempolicy.c:		return &policy->v.nodes;
mempolicy.c:	if (policy->mode == MPOL_PREFERRED && !(policy->flags & MPOL_F_LOCAL))
mempolicy.c:		nd = policy->v.preferred_node;
mempolicy.c:		WARN_ON_ONCE(policy->mode == MPOL_BIND && (gfp & __GFP_THISNODE));
mempolicy.c:	next = next_node_in(me->il_prev, policy->v.nodes);
mempolicy.c:		me->il_prev = next;
mempolicy.c:	policy = current->mempolicy;
mempolicy.c:	if (!policy || policy->flags & MPOL_F_LOCAL)
mempolicy.c:	switch (policy->mode) {
mempolicy.c:		return policy->v.preferred_node;
mempolicy.c:		zonelist = &NODE_DATA(node)->node_zonelists[ZONELIST_FALLBACK];
mempolicy.c:							&policy->v.nodes);
mempolicy.c:		return z->zone ? z->zone->node : node;
mempolicy.c: * node in pol->v.nodes (starting from n=0), wrapping around if n exceeds the
mempolicy.c:	unsigned nnodes = nodes_weight(pol->v.nodes);
mempolicy.c:	nid = first_node(pol->v.nodes);
mempolicy.c:		nid = next_node(nid, pol->v.nodes);
mempolicy.c:		 * shift and PAGE_SHIFT, so the bit-shift is safe.
mempolicy.c:		off = vma->vm_pgoff >> (shift - PAGE_SHIFT);
mempolicy.c:		off += (addr - vma->vm_start) >> shift;
mempolicy.c:	if (unlikely((*mpol)->mode == MPOL_INTERLEAVE)) {
mempolicy.c:		if ((*mpol)->mode == MPOL_BIND)
mempolicy.c:			*nodemask = &(*mpol)->v.nodes;
mempolicy.c: * of non-default mempolicy.
mempolicy.c:	if (!(mask && current->mempolicy))
mempolicy.c:	mempolicy = current->mempolicy;
mempolicy.c:	switch (mempolicy->mode) {
mempolicy.c:		if (mempolicy->flags & MPOL_F_LOCAL)
mempolicy.c:			nid = mempolicy->v.preferred_node;
mempolicy.c:		*mask =  mempolicy->v.nodes;
mempolicy.c:	mempolicy = tsk->mempolicy;
mempolicy.c:	switch (mempolicy->mode) {
mempolicy.c:		ret = nodes_intersects(mempolicy->v.nodes, *mask);
mempolicy.c: * 	alloc_pages_vma	- Allocate a page for a VMA.
mempolicy.c:	if (pol->mode == MPOL_INTERLEAVE) {
mempolicy.c:		 * For hugepage allocation and non-interleave policy which
mempolicy.c:		if (pol->mode == MPOL_PREFERRED &&
mempolicy.c:						!(pol->flags & MPOL_F_LOCAL))
mempolicy.c:			hpage_node = pol->v.preferred_node;
mempolicy.c: * 	alloc_pages_current - Allocate pages.
mempolicy.c:	 * No reference counting needed for current->mempolicy
mempolicy.c:	if (pol->mode == MPOL_INTERLEAVE)
mempolicy.c:	dst->vm_policy = pol;
mempolicy.c: * If mpol_dup() sees current->cpuset == cpuset_being_rebound, then it
mempolicy.c:		return ERR_PTR(-ENOMEM);
mempolicy.c:	if (old == current->mempolicy) {
mempolicy.c:	atomic_set(&new->refcnt, 1);
mempolicy.c:	if (a->mode != b->mode)
mempolicy.c:	if (a->flags != b->flags)
mempolicy.c:		if (!nodes_equal(a->w.user_nodemask, b->w.user_nodemask))
mempolicy.c:	switch (a->mode) {
mempolicy.c:		return !!nodes_equal(a->v.nodes, b->v.nodes);
mempolicy.c:		/* a's ->flags is the same as b's */
mempolicy.c:		if (a->flags & MPOL_F_LOCAL)
mempolicy.c:		return a->v.preferred_node == b->v.preferred_node;
mempolicy.c: * The policies are kept in Red-Black tree linked from the inode.
mempolicy.c: * They are protected by the sp->lock rwlock, which should be held
mempolicy.c: * lookup first element intersecting start-end.  Caller holds sp->lock for
mempolicy.c:	struct rb_node *n = sp->root.rb_node;
mempolicy.c:		if (start >= p->end)
mempolicy.c:			n = n->rb_right;
mempolicy.c:		else if (end <= p->start)
mempolicy.c:			n = n->rb_left;
mempolicy.c:		if (w->end <= start)
mempolicy.c: * Insert a new shared policy into the list.  Caller holds sp->lock for
mempolicy.c:	struct rb_node **p = &sp->root.rb_node;
mempolicy.c:		if (new->start < nd->start)
mempolicy.c:			p = &(*p)->rb_left;
mempolicy.c:		else if (new->end > nd->end)
mempolicy.c:			p = &(*p)->rb_right;
mempolicy.c:	rb_link_node(&new->nd, parent, p);
mempolicy.c:	rb_insert_color(&new->nd, &sp->root);
mempolicy.c:	pr_debug("inserting %lx-%lx: %d\n", new->start, new->end,
mempolicy.c:		 new->policy ? new->policy->mode : 0);
mempolicy.c:	if (!sp->root.rb_node)
mempolicy.c:	read_lock(&sp->lock);
mempolicy.c:		mpol_get(sn->policy);
mempolicy.c:		pol = sn->policy;
mempolicy.c:	read_unlock(&sp->lock);
mempolicy.c:	mpol_put(n->policy);
mempolicy.c: * mpol_misplaced - check whether current page node is valid in policy
mempolicy.c: *	-1	- not misplaced, page is in the right node
mempolicy.c: *	node	- node id where the page should be
mempolicy.c:	int polnid = -1;
mempolicy.c:	int ret = -1;
mempolicy.c:	if (!(pol->flags & MPOL_F_MOF))
mempolicy.c:	switch (pol->mode) {
mempolicy.c:		pgoff = vma->vm_pgoff;
mempolicy.c:		pgoff += (addr - vma->vm_start) >> PAGE_SHIFT;
mempolicy.c:		if (pol->flags & MPOL_F_LOCAL)
mempolicy.c:			polnid = pol->v.preferred_node;
mempolicy.c:		if (node_isset(curnid, pol->v.nodes))
mempolicy.c:				&pol->v.nodes);
mempolicy.c:		polnid = z->zone->node;
mempolicy.c:	if (pol->flags & MPOL_F_MORON) {
mempolicy.c: * Drop the (possibly final) reference to task->mempolicy.  It needs to be
mempolicy.c: * dropped after task->mempolicy is set to NULL so that any allocation done as
mempolicy.c:	pol = task->mempolicy;
mempolicy.c:	task->mempolicy = NULL;
mempolicy.c:	pr_debug("deleting %lx-l%lx\n", n->start, n->end);
mempolicy.c:	rb_erase(&n->nd, &sp->root);
mempolicy.c:	node->start = start;
mempolicy.c:	node->end = end;
mempolicy.c:	node->policy = pol;
mempolicy.c:	newpol->flags |= MPOL_F_SHARED;
mempolicy.c:	write_lock(&sp->lock);
mempolicy.c:	while (n && n->start < end) {
mempolicy.c:		struct rb_node *next = rb_next(&n->nd);
mempolicy.c:		if (n->start >= start) {
mempolicy.c:			if (n->end <= end)
mempolicy.c:				n->start = end;
mempolicy.c:			if (n->end > end) {
mempolicy.c:				*mpol_new = *n->policy;
mempolicy.c:				atomic_set(&mpol_new->refcnt, 1);
mempolicy.c:				sp_node_init(n_new, end, n->end, mpol_new);
mempolicy.c:				n->end = start;
mempolicy.c:				n->end = start;
mempolicy.c:	write_unlock(&sp->lock);
mempolicy.c:	write_unlock(&sp->lock);
mempolicy.c:	ret = -ENOMEM;
mempolicy.c: * mpol_shared_policy_init - initialize shared policy for inode
mempolicy.c: * Install non-NULL @mpol in inode's shared policy rb-tree.
mempolicy.c: * On entry, the current task has a reference on a non-NULL @mpol.
mempolicy.c:	sp->root = RB_ROOT;		/* empty tree == default mempolicy */
mempolicy.c:	rwlock_init(&sp->lock);
mempolicy.c:		new = mpol_new(mpol->mode, mpol->flags, &mpol->w.user_nodemask);
mempolicy.c:		ret = mpol_set_nodemask(new, &mpol->w.user_nodemask, scratch);
mempolicy.c:		/* Create pseudo-vma that contains just the policy */
mempolicy.c:		 vma->vm_pgoff,
mempolicy.c:		 sz, npol ? npol->mode : -1,
mempolicy.c:		 npol ? npol->flags : -1,
mempolicy.c:		 npol ? nodes_addr(npol->v.nodes)[0] : NUMA_NO_NODE);
mempolicy.c:		new = sp_alloc(vma->vm_pgoff, vma->vm_pgoff + sz, npol);
mempolicy.c:			return -ENOMEM;
mempolicy.c:	err = shared_policy_replace(info, vma->vm_pgoff, vma->vm_pgoff+sz, new);
mempolicy.c:	if (!p->root.rb_node)
mempolicy.c:	write_lock(&p->lock);
mempolicy.c:	next = rb_first(&p->root);
mempolicy.c:		next = rb_next(&n->nd);
mempolicy.c:	write_unlock(&p->lock);
mempolicy.c:	/* Parsed by setup_numabalancing. override == 1 enables, -1 disables */
mempolicy.c:		numabalancing_override = -1;
mempolicy.c: * mpol_parse_str - parse string to mempolicy, for tmpfs mpol mount option.
mempolicy.c:		/* NUL-terminate mode or flags string */
mempolicy.c:		new->v.nodes = nodes;
mempolicy.c:		new->v.preferred_node = first_node(nodes);
mempolicy.c:		new->flags |= MPOL_F_LOCAL;
mempolicy.c:	new->w.user_nodemask = nodes;
mempolicy.c:		*--nodelist = ':';
mempolicy.c:		*--flags = '=';
mempolicy.c: * mpol_to_str - format a mempolicy structure for printing
mempolicy.c:	if (pol && pol != &default_policy && !(pol->flags & MPOL_F_MORON)) {
mempolicy.c:		mode = pol->mode;
mempolicy.c:		flags = pol->flags;
mempolicy.c:			node_set(pol->v.preferred_node, nodes);
mempolicy.c:		nodes = pol->v.nodes;
mempolicy.c:		p += snprintf(p, buffer + maxlen - p, "=");
mempolicy.c:			p += snprintf(p, buffer + maxlen - p, "static");
mempolicy.c:			p += snprintf(p, buffer + maxlen - p, "relative");
mempolicy.c:		p += scnprintf(p, buffer + maxlen - p, ":%*pbl",
util.c: * kfree_const - conditionally free memory
util.c: * kstrdup - allocate space for and copy an existing string
util.c: * kstrdup_const - conditionally duplicate an existing const string
util.c: * kstrndup - allocate space for and copy an existing string
util.c: * kmemdup - duplicate region of memory
util.c: * kmemdup_nul - Create a NUL-terminated string from unterminated data
util.c: * memdup_user - duplicate memory region from user space
util.c:		return ERR_PTR(-ENOMEM);
util.c:		return ERR_PTR(-EFAULT);
util.c: * strndup_user - duplicate an existing string from user space
util.c:		return ERR_PTR(-EFAULT);
util.c:		return ERR_PTR(-EINVAL);
util.c:	p[length - 1] = '\0';
util.c: * memdup_user_nul - duplicate memory region from user space and NUL-terminate
util.c:		return ERR_PTR(-ENOMEM);
util.c:		return ERR_PTR(-EFAULT);
util.c:	vma->vm_prev = prev;
util.c:		next = prev->vm_next;
util.c:		prev->vm_next = vma;
util.c:		mm->mmap = vma;
util.c:	vma->vm_next = next;
util.c:		next->vm_prev = vma;
util.c:	return (vma->vm_start <= KSTK_ESP(t) && vma->vm_end >= KSTK_ESP(t));
util.c:	mm->mmap_base = TASK_UNMAPPED_BASE;
util.c:	mm->get_unmapped_area = arch_get_unmapped_area;
util.c: * Like get_user_pages_fast() except its IRQ-safe in that it won't fall
util.c: * get_user_pages_fast() - pin user pages in memory
util.c: * were pinned, returns -errno.
util.c: * operating on current and current->mm, with force=0 and vma=NULL. However
util.c:	struct mm_struct *mm = current->mm;
util.c:		if (down_write_killable(&mm->mmap_sem))
util.c:			return -EINTR;
util.c:		up_write(&mm->mmap_sem);
util.c:		return -EINVAL;
util.c:		return -EINVAL;
util.c: * kvmalloc_node - attempt to allocate physically contiguous memory, but upon
util.c: * failure, fall back to non-contiguous (vmalloc) allocation.
util.c: * @flags: gfp mask for the allocation - must be compatible (superset) with GFP_KERNEL.
util.c: * Reclaim modifiers - __GFP_NORETRY and __GFP_NOFAIL are not supported.
util.c:	 * However make sure that larger requests are not too disruptive - no
util.c:	mapping = (unsigned long)page->mapping;
util.c:/* Neutral page->mapping pointer to address_space or anon_vma or other */
util.c:		return atomic_read(&page->_mapcount) >= 0;
util.c:	mapping = (unsigned long)page->mapping;
util.c:	mapping = page->mapping;
util.c:	ret = atomic_read(&page->_mapcount) + 1;
util.c:	 * For file THP page->_mapcount contains total number of mapping
util.c:		ret--;
util.c:		allowed = sysctl_overcommit_kbytes >> (PAGE_SHIFT - 10);
util.c:		allowed = ((totalram_pages - hugetlb_total_pages())
util.c: * as a guest. On Hyper-V, the host implements a policy engine for dynamically
util.c: * succeed and -ENOMEM implies there is not.
util.c: * vm.overcommit_memory sysctl.  See Documentation/vm/overcommit-accounting
util.c:			-(s64)vm_committed_as_batch * num_online_cpus(),
util.c:		free -= global_node_page_state(NR_SHMEM);
util.c:			free -= totalreserve_pages;
util.c:			free -= sysctl_admin_reserve_kbytes >> (PAGE_SHIFT - 10);
util.c:		allowed -= sysctl_admin_reserve_kbytes >> (PAGE_SHIFT - 10);
util.c:		reserve = sysctl_user_reserve_kbytes >> (PAGE_SHIFT - 10);
util.c:		allowed -= min_t(long, mm->total_vm / 32, reserve);
util.c:	return -ENOMEM;
util.c: * get_cmdline() - copy the cmdline value to a buffer.
util.c:	if (!mm->arg_end)
util.c:	down_read(&mm->mmap_sem);
util.c:	arg_start = mm->arg_start;
util.c:	arg_end = mm->arg_end;
util.c:	env_start = mm->env_start;
util.c:	env_end = mm->env_end;
util.c:	up_read(&mm->mmap_sem);
util.c:	len = arg_end - arg_start;
util.c:	if (res > 0 && buffer[res-1] != '\0' && len < buflen) {
util.c:			len = env_end - env_start;
util.c:			if (len > buflen - res)
util.c:				len = buflen - res;
maccess.c: * happens, handle that and return -EFAULT.
maccess.c:	return ret ? -EFAULT : 0;
maccess.c: * happens, handle that and return -EFAULT.
maccess.c:	return ret ? -EFAULT : 0;
maccess.c: * strncpy_from_unsafe: - Copy a NUL terminated string from unsafe address.
maccess.c: * Copies a NUL-terminated string from unsafe address to kernel buffer.
maccess.c: * If access fails, returns -EFAULT (some data may have been copied
maccess.c: * If @count is smaller than the length of the string, copies @count-1 bytes,
maccess.c:	} while (dst[-1] && ret == 0 && src - unsafe_addr < count);
maccess.c:	dst[-1] = '\0';
maccess.c:	return ret ? -EFAULT : src - unsafe_addr;
debug_page_ref.c:// SPDX-License-Identifier: GPL-2.0
rmap.c: * mm/rmap.c - physical to virtual reverse mappings
rmap.c: * inode->i_mutex	(while writing or truncating, not reading or faulting)
rmap.c: *   mm->mmap_sem
rmap.c: *     page->flags PG_locked (lock_page)
rmap.c: *         mapping->i_mmap_rwsem
rmap.c: *           anon_vma->rwsem
rmap.c: *             mm->page_table_lock or pte_lock
rmap.c: *                 mapping->private_lock (in __set_page_dirty_buffers)
rmap.c: *                   mem_cgroup_{begin,end}_page_stat (memcg->move_lock)
rmap.c: *                     mapping->tree_lock (widely used)
rmap.c: *                 inode->i_lock (in set_page_dirty's __mark_inode_dirty)
rmap.c: *                 bdi.wb->list_lock (in set_page_dirty's __mark_inode_dirty)
rmap.c: *                   sb_lock (within inode_lock in fs/fs-writeback.c)
rmap.c: *                   mapping->tree_lock (widely used, in set_page_dirty,
rmap.c: *                             in arch-dependent flush_dcache_mmap_lock,
rmap.c: *                             within bdi.wb->list_lock in __sync_single_inode)
rmap.c: * anon_vma->rwsem,mapping->i_mutex      (memory_failure, collect_procs_anon)
rmap.c: *   ->tasklist_lock
rmap.c:#include <linux/backing-dev.h>
rmap.c:		atomic_set(&anon_vma->refcount, 1);
rmap.c:		anon_vma->degree = 1;	/* Reference for first vma */
rmap.c:		anon_vma->parent = anon_vma;
rmap.c:		anon_vma->root = anon_vma;
rmap.c:	VM_BUG_ON(atomic_read(&anon_vma->refcount));
rmap.c:	if (rwsem_is_locked(&anon_vma->root->rwsem)) {
rmap.c:	avc->vma = vma;
rmap.c:	avc->anon_vma = anon_vma;
rmap.c:	list_add(&avc->same_vma, &vma->anon_vma_chain);
rmap.c:	anon_vma_interval_tree_insert(avc, &anon_vma->rb_root);
rmap.c: * __anon_vma_prepare - attach an anon_vma to a memory region
rmap.c: * can re-use the anon_vma from (very common when the only
rmap.c: * Anon-vma allocations are very subtle, because we may have
rmap.c:	struct mm_struct *mm = vma->vm_mm;
rmap.c:	spin_lock(&mm->page_table_lock);
rmap.c:	if (likely(!vma->anon_vma)) {
rmap.c:		vma->anon_vma = anon_vma;
rmap.c:		/* vma reference or self-parent link for new root */
rmap.c:		anon_vma->degree++;
rmap.c:	spin_unlock(&mm->page_table_lock);
rmap.c:	return -ENOMEM;
rmap.c: * we traverse the vma->anon_vma_chain, looping over anon_vma's that
rmap.c:	struct anon_vma *new_root = anon_vma->root;
rmap.c:			up_write(&root->rwsem);
rmap.c:		down_write(&root->rwsem);
rmap.c:		up_write(&root->rwsem);
rmap.c: * Returns 0 on success, -ENOMEM on failure.
rmap.c: * If dst->anon_vma is NULL this function tries to find and reuse existing
rmap.c:	list_for_each_entry_reverse(pavc, &src->anon_vma_chain, same_vma) {
rmap.c:		anon_vma = pavc->anon_vma;
rmap.c:		 * it has self-parent reference and at least one child.
rmap.c:		if (!dst->anon_vma && anon_vma != src->anon_vma &&
rmap.c:				anon_vma->degree < 2)
rmap.c:			dst->anon_vma = anon_vma;
rmap.c:	if (dst->anon_vma)
rmap.c:		dst->anon_vma->degree++;
rmap.c:	 * dst->anon_vma is dropped here otherwise its degree can be incorrectly
rmap.c:	 * about dst->anon_vma if anon_vma_clone() failed.
rmap.c:	dst->anon_vma = NULL;
rmap.c:	return -ENOMEM;
rmap.c: * Returns 0 on success, non-zero on failure.
rmap.c:	if (!pvma->anon_vma)
rmap.c:	vma->anon_vma = NULL;
rmap.c:	 * so rmap can find non-COWed pages in child processes.
rmap.c:	if (vma->anon_vma)
rmap.c:	anon_vma->root = pvma->anon_vma->root;
rmap.c:	anon_vma->parent = pvma->anon_vma;
rmap.c:	get_anon_vma(anon_vma->root);
rmap.c:	vma->anon_vma = anon_vma;
rmap.c:	anon_vma->parent->degree++;
rmap.c:	return -ENOMEM;
rmap.c:	list_for_each_entry_safe(avc, next, &vma->anon_vma_chain, same_vma) {
rmap.c:		struct anon_vma *anon_vma = avc->anon_vma;
rmap.c:		anon_vma_interval_tree_remove(avc, &anon_vma->rb_root);
rmap.c:		 * Leave empty anon_vmas on the list - we'll need
rmap.c:		if (RB_EMPTY_ROOT(&anon_vma->rb_root.rb_root)) {
rmap.c:			anon_vma->parent->degree--;
rmap.c:		list_del(&avc->same_vma);
rmap.c:	if (vma->anon_vma)
rmap.c:		vma->anon_vma->degree--;
rmap.c:	 * needing to write-acquire the anon_vma->root->rwsem.
rmap.c:	list_for_each_entry_safe(avc, next, &vma->anon_vma_chain, same_vma) {
rmap.c:		struct anon_vma *anon_vma = avc->anon_vma;
rmap.c:		VM_WARN_ON(anon_vma->degree);
rmap.c:		list_del(&avc->same_vma);
rmap.c:	init_rwsem(&anon_vma->rwsem);
rmap.c:	atomic_set(&anon_vma->refcount, 0);
rmap.c:	anon_vma->rb_root = RB_ROOT_CACHED;
rmap.c: * that the anon_vma pointer from page->mapping is valid if there is a
rmap.c:	anon_mapping = (unsigned long)READ_ONCE(page->mapping);
rmap.c:	anon_vma = (struct anon_vma *) (anon_mapping - PAGE_MAPPING_ANON);
rmap.c:	if (!atomic_inc_not_zero(&anon_vma->refcount)) {
rmap.c:	 * SLAB_TYPESAFE_BY_RCU guarantees that - so the atomic_inc_not_zero()
rmap.c: * atomic op -- the trylock. If we fail the trylock, we fall back to getting a
rmap.c:	anon_mapping = (unsigned long)READ_ONCE(page->mapping);
rmap.c:	anon_vma = (struct anon_vma *) (anon_mapping - PAGE_MAPPING_ANON);
rmap.c:	root_anon_vma = READ_ONCE(anon_vma->root);
rmap.c:	if (down_read_trylock(&root_anon_vma->rwsem)) {
rmap.c:			up_read(&root_anon_vma->rwsem);
rmap.c:	if (!atomic_inc_not_zero(&anon_vma->refcount)) {
rmap.c:	if (atomic_dec_and_test(&anon_vma->refcount)) {
rmap.c:		 * and bail -- can't simply use put_anon_vma() because
rmap.c:	struct tlbflush_unmap_batch *tlb_ubc = &current->tlb_ubc;
rmap.c:	if (!tlb_ubc->flush_required)
rmap.c:	arch_tlbbatch_flush(&tlb_ubc->arch);
rmap.c:	tlb_ubc->flush_required = false;
rmap.c:	tlb_ubc->writable = false;
rmap.c:	struct tlbflush_unmap_batch *tlb_ubc = &current->tlb_ubc;
rmap.c:	if (tlb_ubc->writable)
rmap.c:	struct tlbflush_unmap_batch *tlb_ubc = &current->tlb_ubc;
rmap.c:	arch_tlbbatch_add_mm(&tlb_ubc->arch, mm);
rmap.c:	tlb_ubc->flush_required = true;
rmap.c:	 * Ensure compiler does not re-order the setting of tlb_flush_batched
rmap.c:	mm->tlb_flush_batched = true;
rmap.c:		tlb_ubc->writable = true;
rmap.c:	if (mm->tlb_flush_batched) {
rmap.c:		 * Do not allow the compiler to re-order the clearing of
rmap.c:		mm->tlb_flush_batched = false;
rmap.c:		if (!vma->anon_vma || !page__anon_vma ||
rmap.c:		    vma->anon_vma->root != page__anon_vma->root)
rmap.c:			return -EFAULT;
rmap.c:	} else if (page->mapping) {
rmap.c:		if (!vma->vm_file || vma->vm_file->f_mapping != page->mapping)
rmap.c:			return -EFAULT;
rmap.c:		return -EFAULT;
rmap.c:	if (unlikely(address < vma->vm_start || address >= vma->vm_end))
rmap.c:		return -EFAULT;
rmap.c:		if (vma->vm_flags & VM_LOCKED) {
rmap.c:			pra->vm_flags |= VM_LOCKED;
rmap.c:				if (likely(!(vma->vm_flags & VM_SEQ_READ)))
rmap.c:			/* unexpected pmd-mapped page? */
rmap.c:		pra->mapcount--;
rmap.c:		pra->referenced++;
rmap.c:		pra->vm_flags |= vma->vm_flags;
rmap.c:	if (!pra->mapcount)
rmap.c:	struct mem_cgroup *memcg = pra->memcg;
rmap.c:	if (!mm_match_cgroup(vma->vm_mm, memcg))
rmap.c: * page_referenced - test if the page was referenced
rmap.c: * @vm_flags: collect encountered vma->vm_flags who actually referenced the page
rmap.c:	end = min(vma->vm_end, start + (PAGE_SIZE << compound_order(page)));
rmap.c:	mmu_notifier_invalidate_range_start(vma->vm_mm, start, end);
rmap.c:			set_pte_at(vma->vm_mm, address, pte, entry);
rmap.c:			set_pmd_at(vma->vm_mm, address, pmd, entry);
rmap.c:			/* unexpected pmd-mapped page? */
rmap.c:			mmu_notifier_invalidate_range(vma->vm_mm, cstart, cend);
rmap.c:	mmu_notifier_invalidate_range_end(vma->vm_mm, start, end);
rmap.c:	if (vma->vm_flags & VM_SHARED)
rmap.c: * page_move_anon_rmap - move a page to our anon_vma
rmap.c:	struct anon_vma *anon_vma = vma->anon_vma;
rmap.c:	WRITE_ONCE(page->mapping, (struct address_space *) anon_vma);
rmap.c: * __page_set_anon_rmap - set up new anonymous rmap
rmap.c:	struct anon_vma *anon_vma = vma->anon_vma;
rmap.c:		anon_vma = anon_vma->root;
rmap.c:	page->mapping = (struct address_space *) anon_vma;
rmap.c:	page->index = linear_page_index(vma, address);
rmap.c: * __page_check_anon_rmap - sanity check anonymous rmap addition
rmap.c:	 * The page's anon-rmap details (mapping and index) are guaranteed to
rmap.c:	BUG_ON(page_anon_vma(page)->root != vma->anon_vma->root);
rmap.c: * page_add_anon_rmap - add pte mapping to an anonymous page
rmap.c:		first = atomic_inc_and_test(&page->_mapcount);
rmap.c:		 * We use the irq-unsafe __{inc|mod}_zone_page_stat because
rmap.c: * page_add_new_anon_rmap - add pte mapping to a new anonymous page
rmap.c: * This means the inc-and-test can be bypassed.
rmap.c:	VM_BUG_ON_VMA(address < vma->vm_start || address >= vma->vm_end, vma);
rmap.c:		/* increment count (starts at -1) */
rmap.c:		/* increment count (starts at -1) */
rmap.c:		atomic_set(&page->_mapcount, 0);
rmap.c: * page_add_file_rmap - add pte mapping to a file page
rmap.c:		if (!atomic_inc_and_test(&page->_mapcount))
rmap.c:			if (atomic_add_negative(-1, &page[i]._mapcount))
rmap.c:		if (!atomic_add_negative(-1, compound_mapcount_ptr(page)))
rmap.c:		if (!atomic_add_negative(-1, &page->_mapcount))
rmap.c:	 * We use the irq-unsafe __{inc|mod}_lruvec_page_state because
rmap.c:	__mod_lruvec_page_state(page, NR_FILE_MAPPED, -nr);
rmap.c:	if (!atomic_add_negative(-1, compound_mapcount_ptr(page)))
rmap.c:			if (atomic_add_negative(-1, &page[i]._mapcount))
rmap.c:		__mod_node_page_state(page_pgdat(page), NR_ANON_MAPPED, -nr);
rmap.c: * page_remove_rmap - take down pte mapping from a page
rmap.c:	if (!atomic_add_negative(-1, &page->_mapcount))
rmap.c:	 * We use the irq-unsafe __{inc|mod}_zone_page_stat because
rmap.c:	struct mm_struct *mm = vma->vm_mm;
rmap.c:	/* munlock has nothing to gain from examining un-locked vmas */
rmap.c:	if ((flags & TTU_MUNLOCK) && !(vma->vm_flags & VM_LOCKED))
rmap.c:	end = min(vma->vm_end, start + (PAGE_SIZE << compound_order(page)));
rmap.c:	mmu_notifier_invalidate_range_start(vma->vm_mm, start, end);
rmap.c:		/* PMD-mapped THP migration entry */
rmap.c:			if (vma->vm_flags & VM_LOCKED) {
rmap.c:				/* PTE-mapped THP are never mlocked */
rmap.c:		/* Unexpected PMD-mapped THP? */
rmap.c:		subpage = page - page_to_pfn(page) + pte_pfn(*pvmw.pte);
rmap.c:			 * architecture must guarantee that a clear->dirty
rmap.c:			if (list_empty(&mm->mmlist)) {
rmap.c:				if (list_empty(&mm->mmlist))
rmap.c:					list_add(&mm->mmlist, &init_mm.mmlist);
rmap.c:	mmu_notifier_invalidate_range_end(vma->vm_mm, start, end);
rmap.c:	int maybe_stack = vma->vm_flags & (VM_GROWSDOWN | VM_GROWSUP);
rmap.c:	if ((vma->vm_flags & VM_STACK_INCOMPLETE_SETUP) ==
rmap.c: * try_to_unmap - try to remove all page table mappings to a page
rmap.c: * try_to_munlock - try to munlock a page
rmap.c:	struct anon_vma *root = anon_vma->root;
rmap.c:	if (root != anon_vma && atomic_dec_and_test(&root->refcount))
rmap.c:	if (rwc->anon_lock)
rmap.c:		return rwc->anon_lock(page);
rmap.c: * rmap_walk_anon - do something to anonymous page using the object-based
rmap.c:	pgoff_end = pgoff_start + hpage_nr_pages(page) - 1;
rmap.c:	anon_vma_interval_tree_foreach(avc, &anon_vma->rb_root,
rmap.c:		struct vm_area_struct *vma = avc->vma;
rmap.c:		if (rwc->invalid_vma && rwc->invalid_vma(vma, rwc->arg))
rmap.c:		if (!rwc->rmap_one(page, vma, address, rwc->arg))
rmap.c:		if (rwc->done && rwc->done(page))
rmap.c: * rmap_walk_file - do something to file page using the object-based rmap method
rmap.c:	 * The page lock not only makes sure that page->mapping cannot
rmap.c:	 * so we can safely take mapping->i_mmap_rwsem.
rmap.c:	pgoff_end = pgoff_start + hpage_nr_pages(page) - 1;
rmap.c:	vma_interval_tree_foreach(vma, &mapping->i_mmap,
rmap.c:		if (rwc->invalid_vma && rwc->invalid_vma(vma, rwc->arg))
rmap.c:		if (!rwc->rmap_one(page, vma, address, rwc->arg))
rmap.c:		if (rwc->done && rwc->done(page))
rmap.c:	struct anon_vma *anon_vma = vma->anon_vma;
rmap.c:		anon_vma = anon_vma->root;
rmap.c:	page->mapping = (struct address_space *) anon_vma;
rmap.c:	page->index = linear_page_index(vma, address);
rmap.c:	struct anon_vma *anon_vma = vma->anon_vma;
rmap.c:	BUG_ON(address < vma->vm_start || address >= vma->vm_end);
ksm.c: * Copyright (C) 2008-2009 Red Hat, Inc.
ksm.c: * Therefore KSM uses two data structures - the stable and the unstable tree.
ksm.c: * by their contents.  Because each such page is write-protected, searching on
ksm.c: * by their contents, but since they are not write-protected, KSM cannot rely
ksm.c: * upon the unstable tree to work correctly - the unstable tree is liable to
ksm.c: * 3) The unstable tree is a RedBlack Tree - so its balancing is based on the
ksm.c: * struct mm_slot - ksm information per mm that is being scanned
ksm.c: * @rmap_list: head for this mm_slot's singly-linked list of rmap_items
ksm.c: * struct ksm_scan - cursor for scanning
ksm.c: * struct stable_node - node of the stable rbtree
ksm.c: * @hlist_dup: linked into the stable_node->hlist with a stable_node chain
ksm.c:	 * rmap_hlist_len negative range, but better not -1 to be able
ksm.c:#define STABLE_NODE_CHAIN -1024
ksm.c: * struct rmap_item - reverse mapping item for virtual addresses
ksm.c: * @rmap_list: next rmap_item in mm_slot's singly-linked rmap_list
ksm.c:	return -ENOMEM;
ksm.c:	return chain->rmap_hlist_len == STABLE_NODE_CHAIN;
ksm.c:	return dup->head == STABLE_NODE_DUP_HEAD;
ksm.c:	dup->head = STABLE_NODE_DUP_HEAD;
ksm.c:	hlist_add_head(&dup->hlist_dup, &chain->hlist);
ksm.c:	hlist_del(&dup->hlist_dup);
ksm.c:	ksm_stable_node_dups--;
ksm.c:		rb_erase(&dup->node, root_stable_tree + NUMA(dup->nid));
ksm.c:	dup->head = NULL;
ksm.c:	ksm_rmap_items--;
ksm.c:	rmap_item->mm = NULL;	/* debug safety */
ksm.c:	VM_BUG_ON(stable_node->rmap_hlist_len &&
ksm.c:		if (slot->mm == mm)
ksm.c:	mm_slot->mm = mm;
ksm.c:	hash_add(mm_slots_hash, &mm_slot->link, (unsigned long)mm);
ksm.c: * page tables after it has passed through ksm_exit() - which, if necessary,
ksm.c:	return atomic_read(&mm->mm_users) == 0;
ksm.c:	 * even ksmd can fail in this way - though it's usually breaking ksm
ksm.c:	return (ret & VM_FAULT_OOM) ? -ENOMEM : 0;
ksm.c:	if (!vma || vma->vm_start > addr)
ksm.c:	if (!(vma->vm_flags & VM_MERGEABLE) || !vma->anon_vma)
ksm.c:	struct mm_struct *mm = rmap_item->mm;
ksm.c:	unsigned long addr = rmap_item->address;
ksm.c:	put_anon_vma(rmap_item->anon_vma);
ksm.c:	down_read(&mm->mmap_sem);
ksm.c:	up_read(&mm->mmap_sem);
ksm.c:	struct mm_struct *mm = rmap_item->mm;
ksm.c:	unsigned long addr = rmap_item->address;
ksm.c:	down_read(&mm->mmap_sem);
ksm.c:	up_read(&mm->mmap_sem);
ksm.c: * When merge_across_nodes knob is set to 1, there are only two rb-trees for
ksm.c:		INIT_HLIST_HEAD(&chain->hlist);
ksm.c:		chain->chain_prune_time = jiffies;
ksm.c:		chain->rmap_hlist_len = STABLE_NODE_CHAIN;
ksm.c:		chain->nid = -1; /* debug */
ksm.c:		rb_replace_node(&dup->node, &chain->node, root);
ksm.c:		 * dup stable_nodes in the chain->hlist point to pages
ksm.c:	rb_erase(&chain->node, root);
ksm.c:	ksm_stable_node_chains--;
ksm.c:	BUG_ON(stable_node->rmap_hlist_len < 0);
ksm.c:	hlist_for_each_entry(rmap_item, &stable_node->hlist, hlist) {
ksm.c:		if (rmap_item->hlist.next)
ksm.c:			ksm_pages_sharing--;
ksm.c:			ksm_pages_shared--;
ksm.c:		VM_BUG_ON(stable_node->rmap_hlist_len <= 0);
ksm.c:		stable_node->rmap_hlist_len--;
ksm.c:		put_anon_vma(rmap_item->anon_vma);
ksm.c:		rmap_item->address &= PAGE_MASK;
ksm.c:	if (stable_node->head == &migrate_nodes)
ksm.c:		list_del(&stable_node->list);
ksm.c: * page to reset its page->mapping to NULL, and relies on no other use of
ksm.c: * a page to put something that might look like our key in page->mapping.
ksm.c:	kpfn = READ_ONCE(stable_node->kpfn);
ksm.c:	 * page->mapping is naturally ordered after reading node->kpfn,
ksm.c:	if (READ_ONCE(page->mapping) != expected_mapping)
ksm.c:	 * Usually 0 means free, or tail of a higher-order page: in which
ksm.c:		 * Another check for page->mapping != expected_mapping would
ksm.c:		 * page->mapping reset to NULL later, in free_pages_prepare().
ksm.c:	if (READ_ONCE(page->mapping) != expected_mapping) {
ksm.c:		if (READ_ONCE(page->mapping) != expected_mapping) {
ksm.c:	 * We come here from above when page->mapping or !PageSwapCache
ksm.c:	 * before checking whether node->kpfn has been changed.
ksm.c:	if (READ_ONCE(stable_node->kpfn) != kpfn)
ksm.c:	if (rmap_item->address & STABLE_FLAG) {
ksm.c:		stable_node = rmap_item->head;
ksm.c:		hlist_del(&rmap_item->hlist);
ksm.c:		if (!hlist_empty(&stable_node->hlist))
ksm.c:			ksm_pages_sharing--;
ksm.c:			ksm_pages_shared--;
ksm.c:		VM_BUG_ON(stable_node->rmap_hlist_len <= 0);
ksm.c:		stable_node->rmap_hlist_len--;
ksm.c:		put_anon_vma(rmap_item->anon_vma);
ksm.c:		rmap_item->address &= PAGE_MASK;
ksm.c:	} else if (rmap_item->address & UNSTABLE_FLAG) {
ksm.c:		age = (unsigned char)(ksm_scan.seqnr - rmap_item->address);
ksm.c:			rb_erase(&rmap_item->node,
ksm.c:				 root_unstable_tree + NUMA(rmap_item->nid));
ksm.c:		ksm_pages_unshared--;
ksm.c:		rmap_item->address &= PAGE_MASK;
ksm.c:		*rmap_list = rmap_item->rmap_list;
ksm.c: * that - an rmap_item is assigned to the stable tree after inserting ksm
ksm.c: * to the next pass of ksmd - consider, for example, how ksmd might be
ksm.c:		if (ksm_test_exit(vma->vm_mm))
ksm.c:			err = -ERESTARTSYS;
ksm.c:		 * merge_across_nodes be switched - there is no need to panic.
ksm.c:		err = -EBUSY;
ksm.c:				  &stable_node->hlist, hlist_dup) {
ksm.c:	BUG_ON(!hlist_empty(&stable_node->hlist));
ksm.c:				err = -EBUSY;
ksm.c:			err = -EBUSY;
ksm.c:		mm = mm_slot->mm;
ksm.c:		down_read(&mm->mmap_sem);
ksm.c:		for (vma = mm->mmap; vma; vma = vma->vm_next) {
ksm.c:			if (!(vma->vm_flags & VM_MERGEABLE) || !vma->anon_vma)
ksm.c:						vma->vm_start, vma->vm_end);
ksm.c:		remove_trailing_rmap_items(mm_slot, &mm_slot->rmap_list);
ksm.c:		up_read(&mm->mmap_sem);
ksm.c:		ksm_scan.mm_slot = list_entry(mm_slot->mm_list.next,
ksm.c:			hash_del(&mm_slot->link);
ksm.c:			list_del(&mm_slot->mm_list);
ksm.c:			clear_bit(MMF_VM_MERGEABLE, &mm->flags);
ksm.c:	up_read(&mm->mmap_sem);
ksm.c:	struct mm_struct *mm = vma->vm_mm;
ksm.c:	int err = -EFAULT;
ksm.c:	if (pvmw.address == -EFAULT)
ksm.c: * replace_page - replace page in vma by new ksm page
ksm.c: * Returns 0 on success, -EFAULT on failure.
ksm.c:	struct mm_struct *mm = vma->vm_mm;
ksm.c:	int err = -EFAULT;
ksm.c:	if (addr == -EFAULT)
ksm.c:		newpte = mk_pte(kpage, vma->vm_page_prot);
ksm.c:					       vma->vm_page_prot));
ksm.c: * try_to_merge_one_page - take two pages and merge them into one
ksm.c: * This function returns 0 if the pages were merged, -EFAULT otherwise.
ksm.c:	int err = -EFAULT;
ksm.c:	 * lock_page() because we don't want to wait here - we
ksm.c:	 * to be write-protected.  If it's mapped elsewhere, all of its
ksm.c:	 * ptes are necessarily already write-protected.  But in either
ksm.c:	if ((vma->vm_flags & VM_LOCKED) && kpage && !err) {
ksm.c: * try_to_merge_with_ksm_page - like try_to_merge_two_pages,
ksm.c: * This function returns 0 if the pages were merged, -EFAULT otherwise.
ksm.c:	struct mm_struct *mm = rmap_item->mm;
ksm.c:	int err = -EFAULT;
ksm.c:	down_read(&mm->mmap_sem);
ksm.c:	vma = find_mergeable_vma(mm, rmap_item->address);
ksm.c:	rmap_item->anon_vma = vma->anon_vma;
ksm.c:	get_anon_vma(vma->anon_vma);
ksm.c:	up_read(&mm->mmap_sem);
ksm.c: * try_to_merge_two_pages - take two identical pages and prepare them
ksm.c:	VM_BUG_ON(stable_node->rmap_hlist_len < 0);
ksm.c:	return stable_node->rmap_hlist_len &&
ksm.c:		stable_node->rmap_hlist_len + offset < ksm_max_page_sharing;
ksm.c:	    time_before(jiffies, stable_node->chain_prune_time +
ksm.c:		stable_node->chain_prune_time = jiffies;
ksm.c:				  &stable_node->hlist, hlist_dup) {
ksm.c:		 * stable_node->hlist if they point to freed pages
ksm.c:			    dup->rmap_hlist_len > found_rmap_hlist_len) {
ksm.c:				found_rmap_hlist_len = found->rmap_hlist_len;
ksm.c:			BUG_ON(stable_node->hlist.first->next);
ksm.c:			rb_replace_node(&stable_node->node, &found->node,
ksm.c:			ksm_stable_node_chains--;
ksm.c:			ksm_stable_node_dups--;
ksm.c:		} else if (stable_node->hlist.first != &found->hlist_dup &&
ksm.c:			hlist_del(&found->hlist_dup);
ksm.c:			hlist_add_head(&found->hlist_dup,
ksm.c:				       &stable_node->hlist);
ksm.c:	if (hlist_empty(&stable_node->hlist)) {
ksm.c:	return hlist_entry(stable_node->hlist.first,
ksm.c: * stable_tree_search - search for page inside the stable tree
ksm.c:	if (page_node && page_node->head != &migrate_nodes) {
ksm.c:	new = &root->rb_node;
ksm.c:			new = &parent->rb_left;
ksm.c:			new = &parent->rb_right;
ksm.c:				VM_BUG_ON(page_node->head != &migrate_nodes);
ksm.c:				 * so re-evaluate parent and new.
ksm.c:			if (get_kpfn_nid(stable_node_dup->kpfn) !=
ksm.c:			    NUMA(stable_node_dup->nid)) {
ksm.c:	list_del(&page_node->list);
ksm.c:	DO_NUMA(page_node->nid = nid);
ksm.c:	rb_link_node(&page_node->node, parent, new);
ksm.c:	rb_insert_color(&page_node->node, root);
ksm.c:			VM_BUG_ON(page_node->head != &migrate_nodes);
ksm.c:			list_del(&page_node->list);
ksm.c:			DO_NUMA(page_node->nid = nid);
ksm.c:			rb_replace_node(&stable_node_dup->node,
ksm.c:					&page_node->node,
ksm.c:			rb_erase(&stable_node_dup->node, root);
ksm.c:			VM_BUG_ON(page_node->head != &migrate_nodes);
ksm.c:			list_del(&page_node->list);
ksm.c:			DO_NUMA(page_node->nid = nid);
ksm.c:	stable_node_dup->head = &migrate_nodes;
ksm.c:	list_add(&stable_node_dup->list, stable_node_dup->head);
ksm.c:	VM_BUG_ON(page_node->head != &migrate_nodes);
ksm.c:	list_del(&page_node->list);
ksm.c:	DO_NUMA(page_node->nid = nid);
ksm.c: * stable_tree_insert - insert stable tree node pointing to new ksm page
ksm.c:	new = &root->rb_node;
ksm.c:			new = &parent->rb_left;
ksm.c:			new = &parent->rb_right;
ksm.c:	INIT_HLIST_HEAD(&stable_node_dup->hlist);
ksm.c:	stable_node_dup->kpfn = kpfn;
ksm.c:	stable_node_dup->rmap_hlist_len = 0;
ksm.c:	DO_NUMA(stable_node_dup->nid = nid);
ksm.c:		rb_link_node(&stable_node_dup->node, parent, new);
ksm.c:		rb_insert_color(&stable_node_dup->node, root);
ksm.c: * unstable_tree_search_insert - search for identical page,
ksm.c:	new = &root->rb_node;
ksm.c:			new = &parent->rb_left;
ksm.c:			new = &parent->rb_right;
ksm.c:	rmap_item->address |= UNSTABLE_FLAG;
ksm.c:	rmap_item->address |= (ksm_scan.seqnr & SEQNR_MASK);
ksm.c:	DO_NUMA(rmap_item->nid = nid);
ksm.c:	rb_link_node(&rmap_item->node, parent, new);
ksm.c:	rb_insert_color(&rmap_item->node, root);
ksm.c: * stable_tree_append - add another rmap_item to the linked list of
ksm.c:	BUG_ON(stable_node->rmap_hlist_len < 0);
ksm.c:	stable_node->rmap_hlist_len++;
ksm.c:		WARN_ON_ONCE(stable_node->rmap_hlist_len >
ksm.c:	rmap_item->head = stable_node;
ksm.c:	rmap_item->address |= STABLE_FLAG;
ksm.c:	hlist_add_head(&rmap_item->hlist, &stable_node->hlist);
ksm.c:	if (rmap_item->hlist.next)
ksm.c: * cmp_and_merge_page - first see if page can be merged into the stable tree;
ksm.c:	struct mm_struct *mm = rmap_item->mm;
ksm.c:		if (stable_node->head != &migrate_nodes &&
ksm.c:		    get_kpfn_nid(READ_ONCE(stable_node->kpfn)) !=
ksm.c:		    NUMA(stable_node->nid)) {
ksm.c:			stable_node->head = &migrate_nodes;
ksm.c:			list_add(&stable_node->list, stable_node->head);
ksm.c:		if (stable_node->head != &migrate_nodes &&
ksm.c:		    rmap_item->head == stable_node)
ksm.c:	if (kpage == page && rmap_item->head == stable_node) {
ksm.c:	if (rmap_item->oldchecksum != checksum) {
ksm.c:		rmap_item->oldchecksum = checksum;
ksm.c:		down_read(&mm->mmap_sem);
ksm.c:		vma = find_mergeable_vma(mm, rmap_item->address);
ksm.c:					    ZERO_PAGE(rmap_item->address));
ksm.c:		up_read(&mm->mmap_sem);
ksm.c:		if ((rmap_item->address & PAGE_MASK) == addr)
ksm.c:		if (rmap_item->address > addr)
ksm.c:		*rmap_list = rmap_item->rmap_list;
ksm.c:		rmap_item->mm = mm_slot->mm;
ksm.c:		rmap_item->address = addr;
ksm.c:		rmap_item->rmap_list = *rmap_list;
ksm.c:		 * A number of pages can hang around indefinitely on per-cpu
ksm.c:		slot = list_entry(slot->mm_list.next, struct mm_slot, mm_list);
ksm.c:		ksm_scan.rmap_list = &slot->rmap_list;
ksm.c:	mm = slot->mm;
ksm.c:	down_read(&mm->mmap_sem);
ksm.c:	for (; vma; vma = vma->vm_next) {
ksm.c:		if (!(vma->vm_flags & VM_MERGEABLE))
ksm.c:		if (ksm_scan.address < vma->vm_start)
ksm.c:			ksm_scan.address = vma->vm_start;
ksm.c:		if (!vma->anon_vma)
ksm.c:			ksm_scan.address = vma->vm_end;
ksm.c:		while (ksm_scan.address < vma->vm_end) {
ksm.c:							&rmap_item->rmap_list;
ksm.c:				up_read(&mm->mmap_sem);
ksm.c:		ksm_scan.rmap_list = &slot->rmap_list;
ksm.c:	ksm_scan.mm_slot = list_entry(slot->mm_list.next,
ksm.c:		hash_del(&slot->link);
ksm.c:		list_del(&slot->mm_list);
ksm.c:		clear_bit(MMF_VM_MERGEABLE, &mm->flags);
ksm.c:		up_read(&mm->mmap_sem);
ksm.c:		up_read(&mm->mmap_sem);
ksm.c:		 * up_read(&mm->mmap_sem) first because after
ksm.c: * ksm_do_scan  - the ksm scanner main worker function.
ksm.c: * @scan_npages - number of pages we want to scan before we return.
ksm.c:	while (scan_npages-- && likely(!freezing(current))) {
ksm.c:	struct mm_struct *mm = vma->vm_mm;
ksm.c:		 * Be somewhat over-protective for now!
ksm.c:		if (!test_bit(MMF_VM_MERGEABLE, &mm->flags)) {
ksm.c:		if (vma->anon_vma) {
ksm.c:		return -ENOMEM;
ksm.c:		list_add_tail(&mm_slot->mm_list, &ksm_mm_head.mm_list);
ksm.c:		list_add_tail(&mm_slot->mm_list, &ksm_scan.mm_slot->mm_list);
ksm.c:	set_bit(MMF_VM_MERGEABLE, &mm->flags);
ksm.c:		if (!mm_slot->rmap_list) {
ksm.c:			hash_del(&mm_slot->link);
ksm.c:			list_del(&mm_slot->mm_list);
ksm.c:			list_move(&mm_slot->mm_list,
ksm.c:				  &ksm_scan.mm_slot->mm_list);
ksm.c:		clear_bit(MMF_VM_MERGEABLE, &mm->flags);
ksm.c:		down_write(&mm->mmap_sem);
ksm.c:		up_write(&mm->mmap_sem);
ksm.c:	} else if (anon_vma->root == vma->anon_vma->root &&
ksm.c:		 page->index == linear_page_index(vma, address)) {
ksm.c:	hlist_for_each_entry(rmap_item, &stable_node->hlist, hlist) {
ksm.c:		struct anon_vma *anon_vma = rmap_item->anon_vma;
ksm.c:		anon_vma_interval_tree_foreach(vmac, &anon_vma->rb_root,
ksm.c:			vma = vmac->vma;
ksm.c:			addr = rmap_item->address & ~KSM_FLAG_MASK;
ksm.c:			if (addr < vma->vm_start || addr >= vma->vm_end)
ksm.c:			if ((rmap_item->mm == vma->vm_mm) == search_new_forks)
ksm.c:			if (rwc->invalid_vma && rwc->invalid_vma(vma, rwc->arg))
ksm.c:			if (!rwc->rmap_one(page, vma, addr, rwc->arg)) {
ksm.c:			if (rwc->done && rwc->done(page)) {
ksm.c:	VM_BUG_ON_PAGE(newpage->mapping != oldpage->mapping, newpage);
ksm.c:		VM_BUG_ON_PAGE(stable_node->kpfn != page_to_pfn(oldpage), oldpage);
ksm.c:		stable_node->kpfn = page_to_pfn(newpage);
ksm.c:		 * newpage->mapping was set in advance; now we need smp_wmb()
ksm.c:		 * to make sure that the new stable_node->kpfn is visible
ksm.c:		 * to get_ksm_page() before it can see that oldpage->mapping
ksm.c:	if (stable_node->kpfn >= start_pfn &&
ksm.c:	    stable_node->kpfn < end_pfn) {
ksm.c:				  &stable_node->hlist, hlist_dup) {
ksm.c:	if (hlist_empty(&stable_node->hlist)) {
ksm.c:		if (stable_node->kpfn >= start_pfn &&
ksm.c:		    stable_node->kpfn < end_pfn)
ksm.c:		 * non-existent struct page.
ksm.c:		ksm_check_stable_tree(mn->start_pfn,
ksm.c:				      mn->start_pfn + mn->nr_pages);
ksm.c:		return -EINVAL;
ksm.c:		return -EINVAL;
ksm.c:		return -EINVAL;
ksm.c:		return -EINVAL;
ksm.c:		return -EINVAL;
ksm.c:			err = -EBUSY;
ksm.c:				err = -ENOMEM;
ksm.c:		return -EINVAL;
ksm.c:		return -EINVAL;
ksm.c:			err = -EBUSY;
ksm.c:	ksm_pages_volatile = ksm_rmap_items - ksm_pages_shared
ksm.c:				- ksm_pages_sharing - ksm_pages_unshared;
ksm.c:		return -EINVAL;
percpu-km.c: * mm/percpu-km.c - kernel memory based chunk allocation
percpu-km.c: * To use percpu-km,
percpu-km.c: * - define CONFIG_NEED_PER_CPU_KM from the arch Kconfig.
percpu-km.c: * - CONFIG_NEED_PER_CPU_PAGE_FIRST_CHUNK must not be defined.  It's
percpu-km.c: * - NUMA is not supported.  When setting up the first chunk,
percpu-km.c: * - It's best if the chunk size is power of two multiple of
percpu-km.c: *   chunk size is not aligned.  percpu-km code will whine about it.
percpu-km.c:	chunk->data = pages;
percpu-km.c:	chunk->base_addr = page_address(pages) - pcpu_group_offsets[0];
percpu-km.c:	trace_percpu_create_chunk(chunk->base_addr);
percpu-km.c:	trace_percpu_destroy_chunk(chunk->base_addr);
percpu-km.c:	if (chunk->data)
percpu-km.c:		__free_pages(chunk->data, order_base_2(nr_pages));
percpu-km.c:	if (ai->nr_groups != 1) {
percpu-km.c:		return -EINVAL;
percpu-km.c:	nr_pages = (ai->groups[0].nr_units * ai->unit_size) >> PAGE_SHIFT;
percpu-km.c:			alloc_pages - nr_pages);
slab_common.c:// SPDX-License-Identifier: GPL-2.0
slab_common.c:	return s->object_size;
slab_common.c:		return -EINVAL;
slab_common.c:		 * destroy its slab cache and no-one else reuses the vmalloc
slab_common.c:		res = probe_kernel_address(s->name, tmp);
slab_common.c:			       s->object_size);
slab_common.c:	s->memcg_params.root_cache = NULL;
slab_common.c:	RCU_INIT_POINTER(s->memcg_params.memcg_caches, NULL);
slab_common.c:	INIT_LIST_HEAD(&s->memcg_params.children);
slab_common.c:		s->memcg_params.root_cache = root_cache;
slab_common.c:		s->memcg_params.memcg = memcg;
slab_common.c:		INIT_LIST_HEAD(&s->memcg_params.children_node);
slab_common.c:		INIT_LIST_HEAD(&s->memcg_params.kmem_caches_node);
slab_common.c:		return -ENOMEM;
slab_common.c:	RCU_INIT_POINTER(s->memcg_params.memcg_caches, arr);
slab_common.c:		kvfree(rcu_access_pointer(s->memcg_params.memcg_caches));
slab_common.c:		return -ENOMEM;
slab_common.c:	old = rcu_dereference_protected(s->memcg_params.memcg_caches,
slab_common.c:		memcpy(new->entries, old->entries,
slab_common.c:	rcu_assign_pointer(s->memcg_params.memcg_caches, new);
slab_common.c:		call_rcu(&old->rcu, free_memcg_params);
slab_common.c:		list_add(&s->root_caches_node, &slab_root_caches);
slab_common.c:		list_add(&s->memcg_params.children_node,
slab_common.c:			 &s->memcg_params.root_cache->memcg_params.children);
slab_common.c:		list_add(&s->memcg_params.kmem_caches_node,
slab_common.c:			 &s->memcg_params.memcg->kmem_caches);
slab_common.c:		list_del(&s->root_caches_node);
slab_common.c:		list_del(&s->memcg_params.children_node);
slab_common.c:		list_del(&s->memcg_params.kmem_caches_node);
slab_common.c:	if (slab_nomerge || (s->flags & SLAB_NEVER_MERGE))
slab_common.c:	if (s->ctor)
slab_common.c:	if (s->refcount < 0)
slab_common.c:		if (size > s->size)
slab_common.c:		if ((flags & SLAB_MERGE_SAME) != (s->flags & SLAB_MERGE_SAME))
slab_common.c:		if ((s->size & ~(align - 1)) != s->size)
slab_common.c:		if (s->size - size >= sizeof(void *))
slab_common.c:			(align > s->align || s->align % align))
slab_common.c:	err = -ENOMEM;
slab_common.c:	s->name = name;
slab_common.c:	s->object_size = object_size;
slab_common.c:	s->size = size;
slab_common.c:	s->align = align;
slab_common.c:	s->ctor = ctor;
slab_common.c:	s->refcount = 1;
slab_common.c:	list_add(&s->list, &slab_caches);
slab_common.c: * kmem_cache_create - Create a cache.
slab_common.c: * %SLAB_POISON - Poison the slab with a known test pattern (a5a5a5a5)
slab_common.c: * %SLAB_RED_ZONE - Insert `Red' zones around the allocated memory to check
slab_common.c: * %SLAB_HWCACHE_ALIGN - Align the objects in this cache to a hardware
slab_common.c:		err = -EINVAL;
slab_common.c:		err = -ENOMEM;
slab_common.c:		return -EBUSY;
slab_common.c:	list_del(&s->list);
slab_common.c:	if (s->flags & SLAB_TYPESAFE_BY_RCU) {
slab_common.c:		list_add_tail(&s->list, &slab_caches_to_rcu_destroy);
slab_common.c: * memcg_create_kmem_cache - Create a cache for a memory cgroup.
slab_common.c:	struct cgroup_subsys_state *css = &memcg->css;
slab_common.c:	if (memcg->kmem_state != KMEM_ONLINE)
slab_common.c:	arr = rcu_dereference_protected(root_cache->memcg_params.memcg_caches,
slab_common.c:	 * Since per-memcg caches are created asynchronously on first
slab_common.c:	if (arr->entries[idx])
slab_common.c:	cgroup_name(css->cgroup, memcg_name_buf, sizeof(memcg_name_buf));
slab_common.c:	cache_name = kasprintf(GFP_KERNEL, "%s(%llu:%s)", root_cache->name,
slab_common.c:			       css->serial_nr, memcg_name_buf);
slab_common.c:	s = create_cache(cache_name, root_cache->object_size,
slab_common.c:			 root_cache->size, root_cache->align,
slab_common.c:			 root_cache->flags & CACHE_CREATE_MASK,
slab_common.c:			 root_cache->ctor, memcg, root_cache);
slab_common.c:	arr->entries[idx] = s;
slab_common.c:	s->memcg_params.deact_fn(s);
slab_common.c:	css_put(&s->memcg_params.memcg->css);
slab_common.c:	 * We need to grab blocking locks.  Bounce to ->deact_work.  The
slab_common.c:	INIT_WORK(&s->memcg_params.deact_work, kmemcg_deactivate_workfn);
slab_common.c:	queue_work(memcg_kmem_cache_wq, &s->memcg_params.deact_work);
slab_common.c: * slab_deactivate_memcg_cache_rcu_sched - schedule deactivation after a
slab_common.c:	    WARN_ON_ONCE(s->memcg_params.deact_fn))
slab_common.c:	css_get(&s->memcg_params.memcg->css);
slab_common.c:	s->memcg_params.deact_fn = deact_fn;
slab_common.c:	call_rcu_sched(&s->memcg_params.deact_rcu_head, kmemcg_deactivate_rcufn);
slab_common.c:		arr = rcu_dereference_protected(s->memcg_params.memcg_caches,
slab_common.c:		c = arr->entries[idx];
slab_common.c:		arr->entries[idx] = NULL;
slab_common.c:	list_for_each_entry_safe(s, s2, &memcg->kmem_caches,
slab_common.c:	arr = rcu_dereference_protected(s->memcg_params.memcg_caches,
slab_common.c:		c = arr->entries[i];
slab_common.c:			list_move(&c->memcg_params.children_node, &busy);
slab_common.c:			arr->entries[i] = NULL;
slab_common.c:	list_for_each_entry_safe(c, c2, &s->memcg_params.children,
slab_common.c:	list_splice(&busy, &s->memcg_params.children);
slab_common.c:	if (!list_empty(&s->memcg_params.children))
slab_common.c:		return -EBUSY;
slab_common.c:	kfree_const(s->name);
slab_common.c:	s->refcount--;
slab_common.c:	if (s->refcount)
slab_common.c:		       s->name);
slab_common.c: * kmem_cache_shrink - Shrink a cache.
slab_common.c:	s->name = name;
slab_common.c:	s->size = s->object_size = size;
slab_common.c:	s->align = calculate_alignment(flags, ARCH_KMALLOC_MINALIGN, size);
slab_common.c:	s->refcount = -1;	/* Exempt from merging for now */
slab_common.c:	list_add(&s->list, &slab_caches);
slab_common.c:	s->refcount = 1;
slab_common.c:	return (bytes - 1) / 8;
slab_common.c:		index = fls(size - 1);
slab_common.c: * kmalloc_info[] is to make slub_debug=,kmalloc-xx option work at boot time.
slab_common.c: * kmalloc-67108864.
slab_common.c:	{NULL,                      0},		{"kmalloc-96",             96},
slab_common.c:	{"kmalloc-192",           192},		{"kmalloc-8",               8},
slab_common.c:	{"kmalloc-16",             16},		{"kmalloc-32",             32},
slab_common.c:	{"kmalloc-64",             64},		{"kmalloc-128",           128},
slab_common.c:	{"kmalloc-256",           256},		{"kmalloc-512",           512},
slab_common.c:	{"kmalloc-1024",         1024},		{"kmalloc-2048",         2048},
slab_common.c:	{"kmalloc-4096",         4096},		{"kmalloc-8192",         8192},
slab_common.c:	{"kmalloc-16384",       16384},		{"kmalloc-32768",       32768},
slab_common.c:	{"kmalloc-65536",       65536},		{"kmalloc-131072",     131072},
slab_common.c:	{"kmalloc-262144",     262144},		{"kmalloc-524288",     524288},
slab_common.c:	{"kmalloc-1048576",   1048576},		{"kmalloc-2097152",   2097152},
slab_common.c:	{"kmalloc-4194304",   4194304},		{"kmalloc-8388608",   8388608},
slab_common.c:	{"kmalloc-16777216", 16777216},		{"kmalloc-33554432", 33554432},
slab_common.c:	{"kmalloc-67108864", 67108864}
slab_common.c:		(KMALLOC_MIN_SIZE & (KMALLOC_MIN_SIZE - 1)));
slab_common.c:		 * Caches that are not of the two-to-the-power-of size.
slab_common.c:				 "dma-kmalloc-%d", size);
slab_common.c:	/* Fisher-Yates shuffle */
slab_common.c:	for (i = count - 1; i > 0; i--) {
slab_common.c:	if (count < 2 || cachep->random_seq)
slab_common.c:	cachep->random_seq = kcalloc(count, sizeof(unsigned int), gfp);
slab_common.c:	if (!cachep->random_seq)
slab_common.c:		return -ENOMEM;
slab_common.c:	freelist_randomize(&state, cachep->random_seq, count);
slab_common.c:/* Destroy the per-cache random freelist sequence */
slab_common.c:	kfree(cachep->random_seq);
slab_common.c:	cachep->random_seq = NULL;
slab_common.c:	seq_puts(m, "slabinfo - version: 2.1 (statistics)\n");
slab_common.c:	seq_puts(m, "slabinfo - version: 2.1\n");
slab_common.c:		info->active_slabs += sinfo.active_slabs;
slab_common.c:		info->num_slabs += sinfo.num_slabs;
slab_common.c:		info->shared_avail += sinfo.shared_avail;
slab_common.c:		info->active_objs += sinfo.active_objs;
slab_common.c:		info->num_objs += sinfo.num_objs;
slab_common.c:	seq_printf(m, "%-17s %6lu %6lu %6u %4u %4d",
slab_common.c:		   cache_name(s), sinfo.active_objs, sinfo.num_objs, s->size,
slab_common.c:	return seq_list_start(&memcg->kmem_caches, *pos);
slab_common.c:	return seq_list_next(p, &memcg->kmem_caches, pos);
slab_common.c:	if (p == memcg->kmem_caches.next)
slab_common.c: * slabinfo_op - iterator that generates /proc/slabinfo
slab_common.c: * cache-name
slab_common.c: * num-active-objs
slab_common.c: * total-objs
slab_common.c: * num-active-slabs
slab_common.c: * total-slabs
slab_common.c: * num-pages-per-slab
slab_common.c: * __krealloc - like krealloc() but don't free @p.
slab_common.c: * krealloc - reallocate memory. The contents will remain unchanged.
slab_common.c: * kzfree - like kfree but zero memory
kmemleak.c: * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
kmemleak.c: * Documentation/dev-tools/kmemleak.rst.
kmemleak.c: * ----------------
kmemleak.c: * - kmemleak_lock (rwlock): protects the object_list modifications and
kmemleak.c: *   blocks. The object_tree_root is a red black tree used to look-up
kmemleak.c: * - kmemleak_object.lock (spinlock): protects a kmemleak_object. Accesses to
kmemleak.c: * - scan_mutex (mutex): ensures that only one thread may scan the memory for
kmemleak.c: *   scan_mutex [-> object->lock] -> kmemleak_lock -> other_object->lock (SINGLE_DEPTH_NESTING)
kmemleak.c: * No kmemleak_lock and object->lock nesting is allowed outside scan_mutex
kmemleak.c:#define KMEMLEAK_BLACK	-1
kmemleak.c: * object->lock. Insertions or deletions from object_list, gray_list or
kmemleak.c: * the notes on locking above). These objects are reference-counted
kmemleak.c:/* the list of gray-colored objects (see color_gray comment below) */
kmemleak.c: * with the object->lock held.
kmemleak.c:	const u8 *ptr = (const u8 *)object->pointer;
kmemleak.c:	len = min_t(size_t, object->size, HEX_MAX_LINES * HEX_ROW_SIZE);
kmemleak.c: * - white - orphan object, not enough references to it (count < min_count)
kmemleak.c: * - gray  - not orphan, not marked as false positive (min_count == 0) or
kmemleak.c: * - black - ignore, it doesn't contain references (e.g. text section)
kmemleak.c: *		(min_count == -1). No function defined for this color.
kmemleak.c: * Newly created objects don't have any color assigned (object->count == -1)
kmemleak.c:	return object->count != KMEMLEAK_BLACK &&
kmemleak.c:		object->count < object->min_count;
kmemleak.c:	return object->min_count != KMEMLEAK_BLACK &&
kmemleak.c:		object->count >= object->min_count;
kmemleak.c:	return (color_white(object) && object->flags & OBJECT_ALLOCATED) &&
kmemleak.c:		time_before_eq(object->jiffies + jiffies_min_age,
kmemleak.c: * print_unreferenced function must be called with the object->lock held.
kmemleak.c:	unsigned int msecs_age = jiffies_to_msecs(jiffies - object->jiffies);
kmemleak.c:		   object->pointer, object->size);
kmemleak.c:		   object->comm, object->pid, object->jiffies,
kmemleak.c:	for (i = 0; i < object->trace_len; i++) {
kmemleak.c:		void *ptr = (void *)object->trace[i];
kmemleak.c: * the object->lock held.
kmemleak.c:	trace.nr_entries = object->trace_len;
kmemleak.c:	trace.entries = object->trace;
kmemleak.c:		  object->pointer, object->size);
kmemleak.c:		  object->comm, object->pid, object->jiffies);
kmemleak.c:	pr_notice("  min_count = %d\n", object->min_count);
kmemleak.c:	pr_notice("  count = %d\n", object->count);
kmemleak.c:	pr_notice("  flags = 0x%x\n", object->flags);
kmemleak.c:	pr_notice("  checksum = %u\n", object->checksum);
kmemleak.c: * Look-up a memory block metadata (kmemleak_object) in the object search
kmemleak.c:		if (ptr < object->pointer)
kmemleak.c:			rb = object->rb_node.rb_left;
kmemleak.c:		else if (object->pointer + object->size <= ptr)
kmemleak.c:			rb = object->rb_node.rb_right;
kmemleak.c:		else if (object->pointer == ptr || alias)
kmemleak.c:	return atomic_inc_not_zero(&object->use_count);
kmemleak.c:	hlist_for_each_entry_safe(area, tmp, &object->area_list, node) {
kmemleak.c:		hlist_del(&area->node);
kmemleak.c: * an RCU callback. Since put_object() may be called via the kmemleak_free() ->
kmemleak.c: * recursive call to the kernel allocator. Lock-less RCU object_list traversal
kmemleak.c:	if (!atomic_dec_and_test(&object->use_count))
kmemleak.c:	WARN_ON(object->flags & OBJECT_ALLOCATED);
kmemleak.c:	call_rcu(&object->rcu, free_object_rcu);
kmemleak.c:		rb_erase(&object->rb_node, &object_tree_root);
kmemleak.c:		list_del_rcu(&object->object_list);
kmemleak.c:	INIT_LIST_HEAD(&object->object_list);
kmemleak.c:	INIT_LIST_HEAD(&object->gray_list);
kmemleak.c:	INIT_HLIST_HEAD(&object->area_list);
kmemleak.c:	spin_lock_init(&object->lock);
kmemleak.c:	atomic_set(&object->use_count, 1);
kmemleak.c:	object->flags = OBJECT_ALLOCATED;
kmemleak.c:	object->pointer = ptr;
kmemleak.c:	object->size = size;
kmemleak.c:	object->excess_ref = 0;
kmemleak.c:	object->min_count = min_count;
kmemleak.c:	object->count = 0;			/* white color initially */
kmemleak.c:	object->jiffies = jiffies;
kmemleak.c:	object->checksum = 0;
kmemleak.c:		object->pid = 0;
kmemleak.c:		strncpy(object->comm, "hardirq", sizeof(object->comm));
kmemleak.c:		object->pid = 0;
kmemleak.c:		strncpy(object->comm, "softirq", sizeof(object->comm));
kmemleak.c:		object->pid = current->pid;
kmemleak.c:		 * dependency issues with current->alloc_lock. In the worst
kmemleak.c:		strncpy(object->comm, current->comm, sizeof(object->comm));
kmemleak.c:	object->trace_len = __save_stack_trace(object->trace);
kmemleak.c:		if (ptr + size <= parent->pointer)
kmemleak.c:			link = &parent->rb_node.rb_left;
kmemleak.c:		else if (parent->pointer + parent->size <= ptr)
kmemleak.c:			link = &parent->rb_node.rb_right;
kmemleak.c:			 * No need for parent->lock here since "parent" cannot
kmemleak.c:	rb_link_node(&object->rb_node, rb_parent, link);
kmemleak.c:	rb_insert_color(&object->rb_node, &object_tree_root);
kmemleak.c:	list_add_tail_rcu(&object->object_list, &object_list);
kmemleak.c:	WARN_ON(!(object->flags & OBJECT_ALLOCATED));
kmemleak.c:	WARN_ON(atomic_read(&object->use_count) < 1);
kmemleak.c:	spin_lock_irqsave(&object->lock, flags);
kmemleak.c:	object->flags &= ~OBJECT_ALLOCATED;
kmemleak.c:	spin_unlock_irqrestore(&object->lock, flags);
kmemleak.c:	start = object->pointer;
kmemleak.c:	end = object->pointer + object->size;
kmemleak.c:		create_object(start, ptr - start, object->min_count,
kmemleak.c:		create_object(ptr + size, end - ptr - size, object->min_count,
kmemleak.c:	object->min_count = color;
kmemleak.c:		object->flags |= OBJECT_NO_SCAN;
kmemleak.c:	spin_lock_irqsave(&object->lock, flags);
kmemleak.c:	spin_unlock_irqrestore(&object->lock, flags);
kmemleak.c: * Mark an object permanently as gray-colored so that it can no longer be
kmemleak.c: * Mark the object as black-colored so that it is ignored from scans and
kmemleak.c:	spin_lock_irqsave(&object->lock, flags);
kmemleak.c:		size = object->pointer + object->size - ptr;
kmemleak.c:	} else if (ptr + size > object->pointer + object->size) {
kmemleak.c:	INIT_HLIST_NODE(&area->node);
kmemleak.c:	area->start = ptr;
kmemleak.c:	area->size = size;
kmemleak.c:	hlist_add_head(&area->node, &object->area_list);
kmemleak.c:	spin_unlock_irqrestore(&object->lock, flags);
kmemleak.c:	spin_lock_irqsave(&object->lock, flags);
kmemleak.c:	object->excess_ref = excess_ref;
kmemleak.c:	spin_unlock_irqrestore(&object->lock, flags);
kmemleak.c:	spin_lock_irqsave(&object->lock, flags);
kmemleak.c:	object->flags |= OBJECT_NO_SCAN;
kmemleak.c:	spin_unlock_irqrestore(&object->lock, flags);
kmemleak.c:	log->op_type = op_type;
kmemleak.c:	log->ptr = ptr;
kmemleak.c:	log->size = size;
kmemleak.c:	log->min_count = min_count;
kmemleak.c:	log->trace_len = __save_stack_trace(log->trace);
kmemleak.c:	if (!kmemleak_enabled || !log->ptr || IS_ERR(log->ptr))
kmemleak.c:	object = create_object((unsigned long)log->ptr, log->size,
kmemleak.c:			       log->min_count, GFP_ATOMIC);
kmemleak.c:	spin_lock_irqsave(&object->lock, flags);
kmemleak.c:	for (i = 0; i < log->trace_len; i++)
kmemleak.c:		object->trace[i] = log->trace[i];
kmemleak.c:	object->trace_len = log->trace_len;
kmemleak.c:	spin_unlock_irqrestore(&object->lock, flags);
kmemleak.c:	const void __percpu *ptr = log->ptr;
kmemleak.c:		log->ptr = per_cpu_ptr(ptr, cpu);
kmemleak.c: * kmemleak_alloc - register a newly allocated object
kmemleak.c: *		the object is never reported as a leak. If @min_count is -1,
kmemleak.c: * kmemleak_alloc_percpu - register a newly allocated __percpu object
kmemleak.c: * kmemleak_vmalloc - register a newly vmalloc'ed object
kmemleak.c:		create_object((unsigned long)area->addr, size, 2, gfp);
kmemleak.c:				      (unsigned long)area->addr);
kmemleak.c:		log_early(KMEMLEAK_ALLOC, area->addr, size, 2);
kmemleak.c:		/* reusing early_log.size for storing area->addr */
kmemleak.c:			  area, (unsigned long)area->addr, 0);
kmemleak.c: * kmemleak_free - unregister a previously registered object
kmemleak.c: * kmemleak_free_part - partially unregister a previously registered object
kmemleak.c: * kmemleak_free_percpu - unregister a previously registered __percpu object
kmemleak.c: * kmemleak_update_trace - update object allocation stack trace
kmemleak.c:	spin_lock_irqsave(&object->lock, flags);
kmemleak.c:	object->trace_len = __save_stack_trace(object->trace);
kmemleak.c:	spin_unlock_irqrestore(&object->lock, flags);
kmemleak.c: * kmemleak_not_leak - mark an allocated object as false positive
kmemleak.c: * kmemleak_ignore - ignore an allocated object
kmemleak.c: * kmemleak_scan_area - limit the range to be scanned in an allocated object
kmemleak.c: * kmemleak_no_scan - do not scan an allocated object
kmemleak.c: * kmemleak_alloc_phys - similar to kmemleak_alloc but taking a physical
kmemleak.c: * kmemleak_free_part_phys - similar to kmemleak_free_part but taking a
kmemleak.c: * kmemleak_not_leak_phys - similar to kmemleak_not_leak but taking a physical
kmemleak.c: * kmemleak_ignore_phys - similar to kmemleak_ignore but taking a physical
kmemleak.c:	u32 old_csum = object->checksum;
kmemleak.c:	object->checksum = crc32(0, (void *)object->pointer, object->size);
kmemleak.c:	return object->checksum != old_csum;
kmemleak.c: * Update an object's references. object->lock must be held by the caller.
kmemleak.c:		/* non-orphan, ignored or new */
kmemleak.c:	object->count++;
kmemleak.c:		list_add_tail(&object->gray_list, &gray_list);
kmemleak.c:	if (current->mm)
kmemleak.c:	unsigned long *end = _end - (BYTES_PER_POINTER - 1);
kmemleak.c:		 * object->use_count cannot be dropped to 0 while the object
kmemleak.c:		 * Avoid the lockdep recursive warning on object->lock being
kmemleak.c:		spin_lock_nested(&object->lock, SINGLE_DEPTH_NESTING);
kmemleak.c:			excess_ref = object->excess_ref;
kmemleak.c:		spin_unlock(&object->lock);
kmemleak.c:			spin_lock_nested(&object->lock, SINGLE_DEPTH_NESTING);
kmemleak.c:			spin_unlock(&object->lock);
kmemleak.c: * that object->use_count >= 1.
kmemleak.c:	 * Once the object->lock is acquired, the corresponding memory block
kmemleak.c:	spin_lock_irqsave(&object->lock, flags);
kmemleak.c:	if (object->flags & OBJECT_NO_SCAN)
kmemleak.c:	if (!(object->flags & OBJECT_ALLOCATED))
kmemleak.c:	if (hlist_empty(&object->area_list)) {
kmemleak.c:		void *start = (void *)object->pointer;
kmemleak.c:		void *end = (void *)(object->pointer + object->size);
kmemleak.c:			spin_unlock_irqrestore(&object->lock, flags);
kmemleak.c:			spin_lock_irqsave(&object->lock, flags);
kmemleak.c:		} while (object->flags & OBJECT_ALLOCATED);
kmemleak.c:		hlist_for_each_entry(area, &object->area_list, node)
kmemleak.c:			scan_block((void *)area->start,
kmemleak.c:				   (void *)(area->start + area->size),
kmemleak.c:	spin_unlock_irqrestore(&object->lock, flags);
kmemleak.c:	while (&object->gray_list != &gray_list) {
kmemleak.c:		tmp = list_entry(object->gray_list.next, typeof(*object),
kmemleak.c:		list_del(&object->gray_list);
kmemleak.c:		spin_lock_irqsave(&object->lock, flags);
kmemleak.c:		if (atomic_read(&object->use_count) > 1) {
kmemleak.c:			pr_debug("object->use_count = %d\n",
kmemleak.c:				 atomic_read(&object->use_count));
kmemleak.c:		object->count = 0;
kmemleak.c:			list_add_tail(&object->gray_list, &gray_list);
kmemleak.c:		spin_unlock_irqrestore(&object->lock, flags);
kmemleak.c:	/* per-cpu sections scanning */
kmemleak.c:		spin_lock_irqsave(&object->lock, flags);
kmemleak.c:		if (color_white(object) && (object->flags & OBJECT_ALLOCATED)
kmemleak.c:			object->count = object->min_count;
kmemleak.c:			list_add_tail(&object->gray_list, &gray_list);
kmemleak.c:		spin_unlock_irqrestore(&object->lock, flags);
kmemleak.c:	 * Re-scan the gray list for modified unreferenced objects.
kmemleak.c:		spin_lock_irqsave(&object->lock, flags);
kmemleak.c:		    !(object->flags & OBJECT_REPORTED)) {
kmemleak.c:			object->flags |= OBJECT_REPORTED;
kmemleak.c:		spin_unlock_irqrestore(&object->lock, flags);
kmemleak.c:		if (n-- > 0)
kmemleak.c:	spin_lock_irqsave(&object->lock, flags);
kmemleak.c:	if ((object->flags & OBJECT_REPORTED) && unreferenced_object(object))
kmemleak.c:	spin_unlock_irqrestore(&object->lock, flags);
kmemleak.c:		return -EINVAL;
kmemleak.c:		return -EINVAL;
kmemleak.c:	spin_lock_irqsave(&object->lock, flags);
kmemleak.c:	spin_unlock_irqrestore(&object->lock, flags);
kmemleak.c:		spin_lock_irqsave(&object->lock, flags);
kmemleak.c:		if ((object->flags & OBJECT_REPORTED) &&
kmemleak.c:		spin_unlock_irqrestore(&object->lock, flags);
kmemleak.c: * File write operation to configure kmemleak at run-time. The following
kmemleak.c: *   off	- disable kmemleak (irreversible)
kmemleak.c: *   stack=on	- enable the task stacks scanning
kmemleak.c: *   stack=off	- disable the tasks stacks scanning
kmemleak.c: *   scan=on	- start the automatic memory scanning thread
kmemleak.c: *   scan=off	- stop the automatic memory scanning thread
kmemleak.c: *   scan=...	- set the automatic memory scanning period in seconds (0 to
kmemleak.c: *   scan	- trigger a memory scan
kmemleak.c: *   clear	- mark all current reported unreferenced kmemleak objects as
kmemleak.c: *   dump=...	- dump information about the object found at the given address
kmemleak.c:	buf_size = min(size, (sizeof(buf) - 1));
kmemleak.c:		return -EFAULT;
kmemleak.c:		ret = -EBUSY;
kmemleak.c:		ret = -EINVAL;
kmemleak.c:		delete_object_full(object->pointer);
kmemleak.c: * Allow boot-time kmemleak disabling (enabled by default).
kmemleak.c:		return -EINVAL;
kmemleak.c:		return -EINVAL;
kmemleak.c:	trace.nr_entries = log->trace_len;
kmemleak.c:	trace.entries = log->trace;
kmemleak.c:		switch (log->op_type) {
kmemleak.c:			kmemleak_free(log->ptr);
kmemleak.c:			kmemleak_free_part(log->ptr, log->size);
kmemleak.c:			kmemleak_free_percpu(log->ptr);
kmemleak.c:			kmemleak_not_leak(log->ptr);
kmemleak.c:			kmemleak_ignore(log->ptr);
kmemleak.c:			kmemleak_scan_area(log->ptr, log->size, GFP_KERNEL);
kmemleak.c:			kmemleak_no_scan(log->ptr);
kmemleak.c:			object_set_excess_ref((unsigned long)log->ptr,
kmemleak.c:					      log->excess_ref);
kmemleak.c:				      log->op_type);
kmemleak.c:		 * two clean-up threads but serialized by scan_mutex.
kmemleak.c:		return -ENOMEM;
hmm.c: * struct hmm - HMM per mm struct
hmm.c: * hmm_register - register HMM against an mm (HMM internal)
hmm.c:	struct hmm *hmm = READ_ONCE(mm->hmm);
hmm.c:	 * hence we should always have pre-allocated an new hmm struct
hmm.c:	INIT_LIST_HEAD(&hmm->mirrors);
hmm.c:	init_rwsem(&hmm->mirrors_sem);
hmm.c:	atomic_set(&hmm->sequence, 0);
hmm.c:	hmm->mmu_notifier.ops = NULL;
hmm.c:	INIT_LIST_HEAD(&hmm->ranges);
hmm.c:	spin_lock_init(&hmm->lock);
hmm.c:	hmm->mm = mm;
hmm.c:	hmm->mmu_notifier.ops = &hmm_mmu_notifier_ops;
hmm.c:	if (__mmu_notifier_register(&hmm->mmu_notifier, mm)) {
hmm.c:	spin_lock(&mm->page_table_lock);
hmm.c:	if (!mm->hmm)
hmm.c:		mm->hmm = hmm;
hmm.c:	spin_unlock(&mm->page_table_lock);
hmm.c:		mmu_notifier_unregister(&hmm->mmu_notifier, mm);
hmm.c:	return mm->hmm;
hmm.c:	kfree(mm->hmm);
hmm.c:	spin_lock(&hmm->lock);
hmm.c:	list_for_each_entry(range, &hmm->ranges, list) {
hmm.c:		if (end < range->start || start >= range->end)
hmm.c:		range->valid = false;
hmm.c:		addr = max(start, range->start);
hmm.c:		idx = (addr - range->start) >> PAGE_SHIFT;
hmm.c:		npages = (min(range->end, end) - addr) >> PAGE_SHIFT;
hmm.c:		memset(&range->pfns[idx], 0, sizeof(*range->pfns) * npages);
hmm.c:	spin_unlock(&hmm->lock);
hmm.c:	down_read(&hmm->mirrors_sem);
hmm.c:	list_for_each_entry(mirror, &hmm->mirrors, list)
hmm.c:		mirror->ops->sync_cpu_device_pagetables(mirror, action,
hmm.c:	up_read(&hmm->mirrors_sem);
hmm.c:	struct hmm *hmm = mm->hmm;
hmm.c:	atomic_inc(&hmm->sequence);
hmm.c:	struct hmm *hmm = mm->hmm;
hmm.c:	hmm_invalidate_range(mm->hmm, HMM_UPDATE_INVALIDATE, start, end);
hmm.c: * hmm_mirror_register() - register a mirror against an mm
hmm.c: * THE mm->mmap_sem MUST BE HELD IN WRITE MODE !
hmm.c:	if (!mm || !mirror || !mirror->ops)
hmm.c:		return -EINVAL;
hmm.c:	mirror->hmm = hmm_register(mm);
hmm.c:	if (!mirror->hmm)
hmm.c:		return -ENOMEM;
hmm.c:	down_write(&mirror->hmm->mirrors_sem);
hmm.c:	list_add(&mirror->list, &mirror->hmm->mirrors);
hmm.c:	up_write(&mirror->hmm->mirrors_sem);
hmm.c: * hmm_mirror_unregister() - unregister a mirror
hmm.c:	struct hmm *hmm = mirror->hmm;
hmm.c:	down_write(&hmm->mirrors_sem);
hmm.c:	list_del(&mirror->list);
hmm.c:	up_write(&hmm->mirrors_sem);
hmm.c:	struct hmm_vma_walk *hmm_vma_walk = walk->private;
hmm.c:	struct vm_area_struct *vma = walk->vma;
hmm.c:	flags |= hmm_vma_walk->block ? 0 : FAULT_FLAG_ALLOW_RETRY;
hmm.c:	flags |= hmm_vma_walk->write ? FAULT_FLAG_WRITE : 0;
hmm.c:		return -EBUSY;
hmm.c:		return -EFAULT;
hmm.c:	return -EAGAIN;
hmm.c:	struct hmm_vma_walk *hmm_vma_walk = walk->private;
hmm.c:	struct hmm_range *range = hmm_vma_walk->range;
hmm.c:	hmm_pfn_t *pfns = range->pfns;
hmm.c:	i = (addr - range->start) >> PAGE_SHIFT;
hmm.c:	struct hmm_vma_walk *hmm_vma_walk = walk->private;
hmm.c:	struct hmm_range *range = hmm_vma_walk->range;
hmm.c:	hmm_pfn_t *pfns = range->pfns;
hmm.c:	hmm_vma_walk->last = addr;
hmm.c:	i = (addr - range->start) >> PAGE_SHIFT;
hmm.c:		if (hmm_vma_walk->fault) {
hmm.c:			if (ret != -EAGAIN)
hmm.c:	return hmm_vma_walk->fault ? -EAGAIN : 0;
hmm.c:	struct hmm_vma_walk *hmm_vma_walk = walk->private;
hmm.c:	struct hmm_range *range = hmm_vma_walk->range;
hmm.c:	hmm_pfn_t *pfns = range->pfns;
hmm.c:	hmm_vma_walk->last = addr;
hmm.c:	i = (addr - range->start) >> PAGE_SHIFT;
hmm.c:		if (hmm_vma_walk->fault) {
hmm.c:			if (ret != -EAGAIN)
hmm.c:	return hmm_vma_walk->fault ? -EAGAIN : 0;
hmm.c:	struct hmm_vma_walk *hmm_vma_walk = walk->private;
hmm.c:	struct hmm_range *range = hmm_vma_walk->range;
hmm.c:	struct vm_area_struct *vma = walk->vma;
hmm.c:	hmm_pfn_t *pfns = range->pfns;
hmm.c:	i = (addr - range->start) >> PAGE_SHIFT;
hmm.c:	flag = vma->vm_flags & VM_READ ? HMM_PFN_READ : 0;
hmm.c:	write_fault = hmm_vma_walk->fault & hmm_vma_walk->write;
hmm.c:	if (pmd_huge(*pmdp) && vma->vm_flags & VM_HUGETLB)
hmm.c:			if (hmm_vma_walk->fault)
hmm.c:				if (hmm_vma_walk->fault)
hmm.c:				if (hmm_vma_walk->fault) {
hmm.c:					hmm_vma_walk->last = addr;
hmm.c:					migration_entry_wait(vma->vm_mm,
hmm.c:					return -EAGAIN;
hmm.c:	pte_unmap(ptep - 1);
hmm.c: * hmm_vma_get_pfns() - snapshot CPU page table for a range of virtual addresses
hmm.c: * Returns: -EINVAL if invalid argument, -ENOMEM out of memory, 0 success
hmm.c:	if (is_vm_hugetlb_page(vma) || (vma->vm_flags & VM_SPECIAL)) {
hmm.c:		return -EINVAL;
hmm.c:	if (start < vma->vm_start || start >= vma->vm_end)
hmm.c:		return -EINVAL;
hmm.c:	if (end < vma->vm_start || end > vma->vm_end)
hmm.c:		return -EINVAL;
hmm.c:	hmm = hmm_register(vma->vm_mm);
hmm.c:		return -ENOMEM;
hmm.c:	if (!hmm->mmu_notifier.ops)
hmm.c:		return -EINVAL;
hmm.c:	range->start = start;
hmm.c:	range->pfns = pfns;
hmm.c:	range->end = end;
hmm.c:	spin_lock(&hmm->lock);
hmm.c:	range->valid = true;
hmm.c:	list_add_rcu(&range->list, &hmm->ranges);
hmm.c:	spin_unlock(&hmm->lock);
hmm.c:	mm_walk.mm = vma->vm_mm;
hmm.c: * hmm_vma_range_done() - stop tracking change to CPU page table over a range
hmm.c:	unsigned long npages = (range->end - range->start) >> PAGE_SHIFT;
hmm.c:	if (range->end <= range->start) {
hmm.c:	hmm = hmm_register(vma->vm_mm);
hmm.c:		memset(range->pfns, 0, sizeof(*range->pfns) * npages);
hmm.c:	spin_lock(&hmm->lock);
hmm.c:	list_del_rcu(&range->list);
hmm.c:	spin_unlock(&hmm->lock);
hmm.c:	return range->valid;
hmm.c: * hmm_vma_fault() - try to fault some address in a virtual address range
hmm.c: * Returns: 0 success, error otherwise (-EAGAIN means mmap_sem have been drop)
hmm.c: *   down_read(&mm->mmap_sem);
hmm.c: *   case -EAGAIN:
hmm.c: *     up_read(&mm->mmap_sem)
hmm.c: *   up_read(&mm->mmap_sem)
hmm.c:	if (start < vma->vm_start || start >= vma->vm_end)
hmm.c:		return -EINVAL;
hmm.c:	if (end < vma->vm_start || end > vma->vm_end)
hmm.c:		return -EINVAL;
hmm.c:	hmm = hmm_register(vma->vm_mm);
hmm.c:		return -ENOMEM;
hmm.c:	if (!hmm->mmu_notifier.ops)
hmm.c:		return -EINVAL;
hmm.c:	range->start = start;
hmm.c:	range->pfns = pfns;
hmm.c:	range->end = end;
hmm.c:	spin_lock(&hmm->lock);
hmm.c:	range->valid = true;
hmm.c:	list_add_rcu(&range->list, &hmm->ranges);
hmm.c:	spin_unlock(&hmm->lock);
hmm.c:	if (is_vm_hugetlb_page(vma) || (vma->vm_flags & VM_SPECIAL)) {
hmm.c:	hmm_vma_walk.last = range->start;
hmm.c:	mm_walk.mm = vma->vm_mm;
hmm.c:	} while (ret == -EAGAIN);
hmm.c:		i = (hmm_vma_walk.last - range->start) >> PAGE_SHIFT;
hmm.c:	complete(&devmem->completion);
hmm.c:	devm_remove_action(devmem->device, &hmm_devmem_ref_exit, data);
hmm.c:	wait_for_completion(&devmem->completion);
hmm.c:	devm_remove_action(devmem->device, &hmm_devmem_ref_kill, data);
hmm.c:	struct hmm_devmem *devmem = page->pgmap->data;
hmm.c:	return devmem->ops->fault(devmem, vma, addr, page, flags, pmdp);
hmm.c:	devmem->ops->free(devmem, page);
hmm.c:	align_start = resource->start & ~(PA_SECTION_SIZE - 1);
hmm.c:	align_end = align_start + align_size - 1;
hmm.c:	for (key = resource->start;
hmm.c:	     key <= resource->end;
hmm.c:	struct resource *resource = devmem->resource;
hmm.c:	if (percpu_ref_tryget_live(&devmem->ref)) {
hmm.c:		percpu_ref_put(&devmem->ref);
hmm.c:	start_pfn = (resource->start & ~(PA_SECTION_SIZE - 1)) >> PAGE_SHIFT;
hmm.c:	if (resource->desc == IORES_DESC_DEVICE_PRIVATE_MEMORY)
hmm.c:	struct device *device = devmem->device;
hmm.c:	align_start = devmem->resource->start & ~(PA_SECTION_SIZE - 1);
hmm.c:	align_size = ALIGN(devmem->resource->start +
hmm.c:			   resource_size(devmem->resource),
hmm.c:			   PA_SECTION_SIZE) - align_start;
hmm.c:				__func__, devmem->resource);
hmm.c:		return -ENXIO;
hmm.c:		return -ENXIO;
hmm.c:	if (devmem->resource->desc == IORES_DESC_DEVICE_PUBLIC_MEMORY)
hmm.c:		devmem->pagemap.type = MEMORY_DEVICE_PUBLIC;
hmm.c:		devmem->pagemap.type = MEMORY_DEVICE_PRIVATE;
hmm.c:	devmem->pagemap.res = devmem->resource;
hmm.c:	devmem->pagemap.page_fault = hmm_devmem_fault;
hmm.c:	devmem->pagemap.page_free = hmm_devmem_free;
hmm.c:	devmem->pagemap.dev = devmem->device;
hmm.c:	devmem->pagemap.ref = &devmem->ref;
hmm.c:	devmem->pagemap.data = devmem;
hmm.c:	align_end = align_start + align_size - 1;
hmm.c:				__func__, dev_name(dup->device));
hmm.c:			ret = -EBUSY;
hmm.c:	 * allocate and initialize struct page for the device memory. More-
hmm.c:	 * over the device memory is un-accessible thus we do not want to
hmm.c:	if (devmem->pagemap.type == MEMORY_DEVICE_PUBLIC)
hmm.c:	move_pfn_range_to_zone(&NODE_DATA(nid)->node_zones[ZONE_DEVICE],
hmm.c:	for (pfn = devmem->pfn_first; pfn < devmem->pfn_last; pfn++) {
hmm.c:		page->pgmap = &devmem->pagemap;
hmm.c:	hmm_devmem_radix_release(devmem->resource);
hmm.c:	return devmem->resource == match_data;
hmm.c:	devres_release(devmem->device, &hmm_devmem_release,
hmm.c:		       &hmm_devmem_match, devmem->resource);
hmm.c: * hmm_devmem_add() - hotplug ZONE_DEVICE memory for device memory
hmm.c:		return ERR_PTR(-ENOMEM);
hmm.c:	init_completion(&devmem->completion);
hmm.c:	devmem->pfn_first = -1UL;
hmm.c:	devmem->pfn_last = -1UL;
hmm.c:	devmem->resource = NULL;
hmm.c:	devmem->device = device;
hmm.c:	devmem->ops = ops;
hmm.c:	ret = percpu_ref_init(&devmem->ref, &hmm_devmem_ref_release,
hmm.c:	ret = devm_add_action(device, hmm_devmem_ref_exit, &devmem->ref);
hmm.c:		   (1UL << MAX_PHYSMEM_BITS) - 1);
hmm.c:	addr = addr - size + 1UL;
hmm.c:	for (; addr > size && addr >= iomem_resource.start; addr -= size) {
hmm.c:		devmem->resource = devm_request_mem_region(device, addr, size,
hmm.c:		if (!devmem->resource) {
hmm.c:			ret = -ENOMEM;
hmm.c:	if (!devmem->resource) {
hmm.c:		ret = -ERANGE;
hmm.c:	devmem->resource->desc = IORES_DESC_DEVICE_PRIVATE_MEMORY;
hmm.c:	devmem->pfn_first = devmem->resource->start >> PAGE_SHIFT;
hmm.c:	devmem->pfn_last = devmem->pfn_first +
hmm.c:			   (resource_size(devmem->resource) >> PAGE_SHIFT);
hmm.c:	ret = devm_add_action(device, hmm_devmem_ref_kill, &devmem->ref);
hmm.c:	devm_release_mem_region(device, devmem->resource->start,
hmm.c:				resource_size(devmem->resource));
hmm.c:	hmm_devmem_ref_kill(&devmem->ref);
hmm.c:	hmm_devmem_ref_exit(&devmem->ref);
hmm.c:	if (res->desc != IORES_DESC_DEVICE_PUBLIC_MEMORY)
hmm.c:		return ERR_PTR(-EINVAL);
hmm.c:		return ERR_PTR(-ENOMEM);
hmm.c:	init_completion(&devmem->completion);
hmm.c:	devmem->pfn_first = -1UL;
hmm.c:	devmem->pfn_last = -1UL;
hmm.c:	devmem->resource = res;
hmm.c:	devmem->device = device;
hmm.c:	devmem->ops = ops;
hmm.c:	ret = percpu_ref_init(&devmem->ref, &hmm_devmem_ref_release,
hmm.c:	ret = devm_add_action(device, hmm_devmem_ref_exit, &devmem->ref);
hmm.c:	devmem->pfn_first = devmem->resource->start >> PAGE_SHIFT;
hmm.c:	devmem->pfn_last = devmem->pfn_first +
hmm.c:			   (resource_size(devmem->resource) >> PAGE_SHIFT);
hmm.c:	ret = devm_add_action(device, hmm_devmem_ref_kill, &devmem->ref);
hmm.c:	hmm_devmem_ref_kill(&devmem->ref);
hmm.c:	hmm_devmem_ref_exit(&devmem->ref);
hmm.c: * hmm_devmem_remove() - remove device memory (kill and free ZONE_DEVICE)
hmm.c: * This will hot-unplug memory that was hotplugged by hmm_devmem_add on behalf
hmm.c:	device = devmem->device;
hmm.c:	start = devmem->resource->start;
hmm.c:	size = resource_size(devmem->resource);
hmm.c:	cdm = devmem->resource->desc == IORES_DESC_DEVICE_PUBLIC_MEMORY;
hmm.c:	hmm_devmem_ref_kill(&devmem->ref);
hmm.c:	hmm_devmem_ref_exit(&devmem->ref);
hmm.c:	clear_bit(hmm_device->minor, hmm_device_mask);
hmm.c:		return ERR_PTR(-ENOMEM);
hmm.c:	hmm_device->minor = find_first_zero_bit(hmm_device_mask, HMM_DEVICE_MAX);
hmm.c:	if (hmm_device->minor >= HMM_DEVICE_MAX) {
hmm.c:		return ERR_PTR(-EBUSY);
hmm.c:	set_bit(hmm_device->minor, hmm_device_mask);
hmm.c:	dev_set_name(&hmm_device->device, "hmm_device%d", hmm_device->minor);
hmm.c:	hmm_device->device.devt = MKDEV(MAJOR(hmm_device_devt),
hmm.c:					hmm_device->minor);
hmm.c:	hmm_device->device.release = hmm_device_release;
hmm.c:	dev_set_drvdata(&hmm_device->device, drvdata);
hmm.c:	hmm_device->device.class = hmm_device_class;
hmm.c:	device_initialize(&hmm_device->device);
hmm.c:	put_device(&hmm_device->device);
mincore.c:// SPDX-License-Identifier: GPL-2.0
mincore.c: * Copyright (C) 1994-2006  Linus Torvalds
mincore.c:	unsigned char *vec = walk->private;
mincore.c:	walk->private = vec;
mincore.c: * and is up to date; i.e. that no page-in operation would be required
mincore.c:	unsigned long nr = (end - addr) >> PAGE_SHIFT;
mincore.c:	if (vma->vm_file) {
mincore.c:			vec[i] = mincore_page(vma->vm_file->f_mapping, pgoff);
mincore.c:	walk->private += __mincore_unmapped_range(addr, end,
mincore.c:						  walk->vma, walk->private);
mincore.c:	struct vm_area_struct *vma = walk->vma;
mincore.c:	unsigned char *vec = walk->private;
mincore.c:	int nr = (end - addr) >> PAGE_SHIFT;
mincore.c:	ptep = pte_offset_map_lock(walk->mm, pmd, addr, &ptl);
mincore.c:	pte_unmap_unlock(ptep - 1, ptl);
mincore.c:	walk->private += nr;
mincore.c:	vma = find_vma(current->mm, addr);
mincore.c:	if (!vma || addr < vma->vm_start)
mincore.c:		return -ENOMEM;
mincore.c:	mincore_walk.mm = vma->vm_mm;
mincore.c:	end = min(vma->vm_end, addr + (pages << PAGE_SHIFT));
mincore.c:	return (end - addr) >> PAGE_SHIFT;
mincore.c: *  zero    - success
mincore.c: *  -EFAULT - vec points to an illegal address
mincore.c: *  -EINVAL - addr is not a multiple of PAGE_SIZE
mincore.c: *  -ENOMEM - Addresses in the range [addr, addr + len] are
mincore.c: *  -EAGAIN - A kernel resource was temporarily unavailable.
mincore.c:	/* Check the start address: needs to be page-aligned.. */
mincore.c:		return -EINVAL;
mincore.c:	/* ..and we need to be passed a valid user-space range */
mincore.c:		return -ENOMEM;
mincore.c:		return -EFAULT;
mincore.c:		return -EAGAIN;
mincore.c:		down_read(&current->mm->mmap_sem);
mincore.c:		up_read(&current->mm->mmap_sem);
mincore.c:			retval = -EFAULT;
mincore.c:		pages -= retval;
khugepaged.c:// SPDX-License-Identifier: GPL-2.0
khugepaged.c: * struct mm_slot - hash lookup from mm to mm_slot
khugepaged.c: * struct khugepaged_scan - cursor for scanning
khugepaged.c:		return -EINVAL;
khugepaged.c:		return -EINVAL;
khugepaged.c:		return -EINVAL;
khugepaged.c:	if (err || max_ptes_none > HPAGE_PMD_NR-1)
khugepaged.c:		return -EINVAL;
khugepaged.c:	if (err || max_ptes_swap > HPAGE_PMD_NR-1)
khugepaged.c:		return -EINVAL;
khugepaged.c:		if (mm_has_pgste(vma->vm_mm))
khugepaged.c:			return -ENOMEM;
khugepaged.c:		return -ENOMEM;
khugepaged.c:	khugepaged_max_ptes_none = HPAGE_PMD_NR - 1;
khugepaged.c:		if (mm == mm_slot->mm)
khugepaged.c:	mm_slot->mm = mm;
khugepaged.c:	hash_add(mm_slots_hash, &mm_slot->hash, (long)mm);
khugepaged.c:	return atomic_read(&mm->mm_users) == 0;
khugepaged.c:		return -ENOMEM;
khugepaged.c:	if (unlikely(test_and_set_bit(MMF_VM_HUGEPAGE, &mm->flags))) {
khugepaged.c:	list_add_tail(&mm_slot->mm_node, &khugepaged_scan.mm_head);
khugepaged.c:	if (!vma->anon_vma)
khugepaged.c:	if (vma->vm_ops || (vm_flags & VM_NO_KHUGEPAGED))
khugepaged.c:	hstart = (vma->vm_start + ~HPAGE_PMD_MASK) & HPAGE_PMD_MASK;
khugepaged.c:	hend = vma->vm_end & HPAGE_PMD_MASK;
khugepaged.c:		hash_del(&mm_slot->hash);
khugepaged.c:		list_del(&mm_slot->mm_node);
khugepaged.c:		clear_bit(MMF_VM_HUGEPAGE, &mm->flags);
khugepaged.c:		down_write(&mm->mmap_sem);
khugepaged.c:		up_write(&mm->mmap_sem);
khugepaged.c:	while (--_pte >= pte) {
khugepaged.c:		    mmu_notifier_test_young(vma->vm_mm, address))
khugepaged.c:			add_mm_counter(vma->vm_mm, MM_ANONPAGES, 1);
khugepaged.c:				pte_clear(vma->vm_mm, address, _pte);
khugepaged.c:			 * be disabled to update the per-cpu stats
khugepaged.c:			pte_clear(vma->vm_mm, address, _pte);
khugepaged.c:		*hpage = ERR_PTR(-ENOMEM);
khugepaged.c:	if ((!(vma->vm_flags & VM_HUGEPAGE) && !khugepaged_always()) ||
khugepaged.c:	    (vma->vm_flags & VM_NOHUGEPAGE) ||
khugepaged.c:	    test_bit(MMF_DISABLE_THP, &vma->vm_mm->flags))
khugepaged.c:	if (shmem_file(vma->vm_file)) {
khugepaged.c:		return IS_ALIGNED((vma->vm_start >> PAGE_SHIFT) - vma->vm_pgoff,
khugepaged.c:	if (!vma->anon_vma || vma->vm_ops)
khugepaged.c:	return !(vma->vm_flags & VM_NO_KHUGEPAGED);
khugepaged.c: * Return 0 if succeeds, otherwise return none-zero
khugepaged.c:	hstart = (vma->vm_start + ~HPAGE_PMD_MASK) & HPAGE_PMD_MASK;
khugepaged.c:	hend = vma->vm_end & HPAGE_PMD_MASK;
khugepaged.c:			down_read(&mm->mmap_sem);
khugepaged.c:	vmf.pte--;
khugepaged.c:	up_read(&mm->mmap_sem);
khugepaged.c:	down_read(&mm->mmap_sem);
khugepaged.c:		up_read(&mm->mmap_sem);
khugepaged.c:		up_read(&mm->mmap_sem);
khugepaged.c:		up_read(&mm->mmap_sem);
khugepaged.c:	up_read(&mm->mmap_sem);
khugepaged.c:	down_write(&mm->mmap_sem);
khugepaged.c:	anon_vma_lock_write(vma->anon_vma);
khugepaged.c:		anon_vma_unlock_write(vma->anon_vma);
khugepaged.c:	anon_vma_unlock_write(vma->anon_vma);
khugepaged.c:	_pmd = mk_huge_pmd(new_page, vma->vm_page_prot);
khugepaged.c:	up_write(&mm->mmap_sem);
khugepaged.c:		    mmu_notifier_test_young(vma->vm_mm, address))
khugepaged.c:	struct mm_struct *mm = mm_slot->mm;
khugepaged.c:		hash_del(&mm_slot->hash);
khugepaged.c:		list_del(&mm_slot->mm_node);
khugepaged.c:		 * clear_bit(MMF_VM_HUGEPAGE, &mm->flags);
khugepaged.c:	vma_interval_tree_foreach(vma, &mapping->i_mmap, pgoff, pgoff) {
khugepaged.c:		if (vma->anon_vma)
khugepaged.c:		addr = vma->vm_start + ((pgoff - vma->vm_pgoff) << PAGE_SHIFT);
khugepaged.c:		if (vma->vm_end < addr + HPAGE_PMD_SIZE)
khugepaged.c:		pmd = mm_find_pmd(vma->vm_mm, addr);
khugepaged.c:		 * If trylock fails we would end up with pte-mapped THP after
khugepaged.c:		 * re-fault. Not ideal, but it's more important to not disturb
khugepaged.c:		if (down_write_trylock(&vma->vm_mm->mmap_sem)) {
khugepaged.c:			spinlock_t *ptl = pmd_lock(vma->vm_mm, pmd);
khugepaged.c:			up_write(&vma->vm_mm->mmap_sem);
khugepaged.c:			atomic_long_dec(&vma->vm_mm->nr_ptes);
khugepaged.c:			pte_free(vma->vm_mm, pmd_pgtable(_pmd));
khugepaged.c: * collapse_shmem - collapse small tmpfs/shmem pages into huge one.
khugepaged.c: *  - allocate and freeze a new huge page;
khugepaged.c: *  - scan over radix tree replacing old pages the new one
khugepaged.c: *  - if replacing succeed:
khugepaged.c: *  - if replacing failed;
khugepaged.c: *    + restore gaps in the radix-tree;
khugepaged.c:	VM_BUG_ON(start & (HPAGE_PMD_NR - 1));
khugepaged.c:	new_page->index = start;
khugepaged.c:	new_page->mapping = mapping;
khugepaged.c:	 * and not up-to-date. It's safe to insert it into radix tree, because
khugepaged.c:	spin_lock_irq(&mapping->tree_lock);
khugepaged.c:	radix_tree_for_each_slot(slot, &mapping->page_tree, &iter, start) {
khugepaged.c:		int n = min(iter.index, end) - index;
khugepaged.c:		 * insert relevant subpage of new_page into the radix-tree.
khugepaged.c:		if (n && !shmem_charge(mapping->host, n)) {
khugepaged.c:			radix_tree_insert(&mapping->page_tree, index,
khugepaged.c:				&mapping->tree_lock);
khugepaged.c:			spin_unlock_irq(&mapping->tree_lock);
khugepaged.c:			if (shmem_getpage(mapping->host, index, &page,
khugepaged.c:			spin_lock_irq(&mapping->tree_lock);
khugepaged.c:		spin_unlock_irq(&mapping->tree_lock);
khugepaged.c:		spin_lock_irq(&mapping->tree_lock);
khugepaged.c:		slot = radix_tree_lookup_slot(&mapping->page_tree, index);
khugepaged.c:					&mapping->tree_lock), page);
khugepaged.c:		 *  - we hold a pin on it;
khugepaged.c:		 *  - one reference from radix tree;
khugepaged.c:		 *  - one from isolate_lru_page;
khugepaged.c:		list_add_tail(&page->lru, &pagelist);
khugepaged.c:		radix_tree_replace_slot(&mapping->page_tree, slot,
khugepaged.c:		spin_unlock_irq(&mapping->tree_lock);
khugepaged.c:		int n = end - index;
khugepaged.c:		if (!shmem_charge(mapping->host, n)) {
khugepaged.c:			radix_tree_insert(&mapping->page_tree, index,
khugepaged.c:	spin_unlock_irq(&mapping->tree_lock);
khugepaged.c:			copy_highpage(new_page + (page->index % HPAGE_PMD_NR),
khugepaged.c:			list_del(&page->lru);
khugepaged.c:			page->mapping = NULL;
khugepaged.c:			__mod_node_page_state(zone->zone_pgdat, NR_FILE_PAGES, nr_none);
khugepaged.c:			__mod_node_page_state(zone->zone_pgdat, NR_SHMEM, nr_none);
khugepaged.c:		 * Remove pte page tables, so we can re-faulti
khugepaged.c:		/* Something went wrong: rollback changes to the radix-tree */
khugepaged.c:		shmem_uncharge(mapping->host, nr_none);
khugepaged.c:		spin_lock_irq(&mapping->tree_lock);
khugepaged.c:		radix_tree_for_each_slot(slot, &mapping->page_tree, &iter,
khugepaged.c:			if (!page || iter.index < page->index) {
khugepaged.c:				nr_none--;
khugepaged.c:				radix_tree_delete(&mapping->page_tree,
khugepaged.c:			VM_BUG_ON_PAGE(page->index != iter.index, page);
khugepaged.c:			list_del(&page->lru);
khugepaged.c:			radix_tree_replace_slot(&mapping->page_tree,
khugepaged.c:			spin_unlock_irq(&mapping->tree_lock);
khugepaged.c:			spin_lock_irq(&mapping->tree_lock);
khugepaged.c:		spin_unlock_irq(&mapping->tree_lock);
khugepaged.c:		new_page->mapping = NULL;
khugepaged.c:	radix_tree_for_each_slot(slot, &mapping->page_tree, &iter, start) {
khugepaged.c:		if (present < HPAGE_PMD_NR - khugepaged_max_ptes_none) {
khugepaged.c:	mm = mm_slot->mm;
khugepaged.c:	if (unlikely(!down_read_trylock(&mm->mmap_sem)))
khugepaged.c:	for (; vma; vma = vma->vm_next) {
khugepaged.c:		hstart = (vma->vm_start + ~HPAGE_PMD_MASK) & HPAGE_PMD_MASK;
khugepaged.c:		hend = vma->vm_end & HPAGE_PMD_MASK;
khugepaged.c:			if (shmem_file(vma->vm_file)) {
khugepaged.c:				file = get_file(vma->vm_file);
khugepaged.c:				up_read(&mm->mmap_sem);
khugepaged.c:				khugepaged_scan_shmem(mm, file->f_mapping,
khugepaged.c:	up_read(&mm->mmap_sem); /* exit_mmap will destroy ptes after this */
khugepaged.c:		if (mm_slot->mm_node.next != &khugepaged_scan.mm_head) {
khugepaged.c:				mm_slot->mm_node.next,
khugepaged.c:			progress += khugepaged_scan_mm_slot(pages - progress,
khugepaged.c:	recommended_min <<= (PAGE_SHIFT-10);
memory-failure.c: * hardware as being corrupted usually due to a multi-bit ECC memory or cache
memory-failure.c: * not-yet-corrupted-by-suspicious pages without killing anything.
memory-failure.c: * - You know how to test it.
memory-failure.c: * - You have a test that can be added to mce-test
memory-failure.c: *   https://git.kernel.org/cgit/utils/cpu/mce/mce-test.git/
memory-failure.c: * - The case actually shows up as a frequent (top 10) page state in
memory-failure.c: *   tools/vm/page-types when running a real workload.
memory-failure.c:#include <linux/page-flags.h>
memory-failure.c:#include <linux/kernel-page-flags.h>
memory-failure.c:#include <linux/backing-dev.h>
memory-failure.c:		return -EINVAL;
memory-failure.c:	if (mapping == NULL || mapping->host == NULL)
memory-failure.c:		return -EINVAL;
memory-failure.c:	dev = mapping->host->i_sb->s_dev;
memory-failure.c:		return -EINVAL;
memory-failure.c:		return -EINVAL;
memory-failure.c:		return -EINVAL;
memory-failure.c:		return -EINVAL;
memory-failure.c:		return -EINVAL;
memory-failure.c:		return -EINVAL;
memory-failure.c:		return -EINVAL;
memory-failure.c:		pfn, t->comm, t->pid);
memory-failure.c:	if ((flags & MF_ACTION_REQUIRED) && t->mm == current->mm) {
memory-failure.c:			t->comm, t->pid, ret);
memory-failure.c: * from the VMAs. So do a brute-force search over all
memory-failure.c:	tk->addr = page_address_in_vma(p, vma);
memory-failure.c:	tk->addr_valid = 1;
memory-failure.c:	if (tk->addr == -EFAULT) {
memory-failure.c:			page_to_pfn(p), tsk->comm);
memory-failure.c:		tk->addr_valid = 0;
memory-failure.c:	tk->tsk = tsk;
memory-failure.c:	list_add_tail(&tk->nd, to_kill);
memory-failure.c:			if (fail || tk->addr_valid == 0) {
memory-failure.c:				       pfn, tk->tsk->comm, tk->tsk->pid);
memory-failure.c:				force_sig(SIGKILL, tk->tsk);
memory-failure.c:			 * something else on the address in-between. We could
memory-failure.c:			else if (kill_proc(tk->tsk, tk->addr, trapno,
memory-failure.c:				       pfn, tk->tsk->comm, tk->tsk->pid);
memory-failure.c:		put_task_struct(tk->tsk);
memory-failure.c:		if ((t->flags & PF_MCE_PROCESS) && (t->flags & PF_MCE_EARLY))
memory-failure.c:	if (!tsk->mm)
memory-failure.c:		anon_vma_interval_tree_foreach(vmac, &av->rb_root,
memory-failure.c:			vma = vmac->vma;
memory-failure.c:			if (vma->vm_mm == t->mm)
memory-failure.c:	struct address_space *mapping = page->mapping;
memory-failure.c:		vma_interval_tree_foreach(vma, &mapping->i_mmap, pgoff,
memory-failure.c:			if (vma->vm_mm == t->mm)
memory-failure.c:	if (!page->mapping)
memory-failure.c:	[MF_MSG_KERNEL_HIGH_ORDER]	= "high-order kernel page",
memory-failure.c:	[MF_MSG_NON_PMD_HUGE]		= "non-pmd-sized huge page",
memory-failure.c:		 * complain when the page is unpoison-and-freed.
memory-failure.c:	return -EIO;
memory-failure.c:	if (mapping->a_ops->error_remove_page) {
memory-failure.c:		int err = mapping->a_ops->error_remove_page(mapping, p);
memory-failure.c:		mapping_set_error(mapping, -EIO);
memory-failure.c: *      - clear dirty bit to prevent IO
memory-failure.c: *      - remove from LRU
memory-failure.c: *      - but keep in the swap cache, so that when we return to it on
memory-failure.c: * - Error on hugepage is contained in hugepage unit (not in raw page unit.)
memory-failure.c: * A page state is defined by its current page->flags bits.
memory-failure.c:	result = ps->action(p, pfn);
memory-failure.c:	count = page_count(p) - 1;
memory-failure.c:	if (ps->action == me_swapcache_dirty && result == MF_DELAYED)
memory-failure.c:		count--;
memory-failure.c:		       pfn, action_page_types[ps->type], count);
memory-failure.c:	action_result(pfn, ps->type, result);
memory-failure.c:	return (result == MF_RECOVERED || result == MF_DELAYED) ? 0 : -EBUSY;
memory-failure.c: * get_hwpoison_page() - Get refcount for memory error handling:
memory-failure.c: * non-zero value.)
memory-failure.c:	 * Here we are interested only in user-mapped pages, so skip any
memory-failure.c:	 * use a more force-full uncatchable kill to prevent
memory-failure.c:		if ((p->flags & ps->mask) == ps->res)
memory-failure.c:	page_flags |= (p->flags & (1UL << PG_dirty));
memory-failure.c:	if (!ps->mask)
memory-failure.c:			if ((page_flags & ps->mask) == ps->res)
memory-failure.c:	page_flags = head->flags;
memory-failure.c:	 * TODO: hwpoison for pud-sized hugetlb doesn't work right now, so
memory-failure.c:	 *  - conversion of a pud that maps an error hugetlb into hwpoison
memory-failure.c:	 *  - other mm code walking over page table is aware of pud-aligned
memory-failure.c:		res = -EBUSY;
memory-failure.c:		res = -EBUSY;
memory-failure.c: * memory_failure - Handle memory failure of a page.
memory-failure.c:		return -ENXIO;
memory-failure.c:	 * 2) it's part of a non-compound high order page.
memory-failure.c:			return -EBUSY;
memory-failure.c:			return -EBUSY;
memory-failure.c:	 * We ignore non-LRU pages for good reasons.
memory-failure.c:	 * - PG_locked is only well defined for LRU pages and a few others
memory-failure.c:	 * - to avoid races with __SetPageLocked()
memory-failure.c:	 * - to avoid races with __SetPageSlab*() (and more non-atomic ops)
memory-failure.c:		res = -EBUSY;
memory-failure.c:		page_flags = hpage->flags;
memory-failure.c:		page_flags = p->flags;
memory-failure.c:		res = -EBUSY;
memory-failure.c:	if (PageLRU(p) && !PageSwapCache(p) && p->mapping == NULL) {
memory-failure.c:		res = -EBUSY;
memory-failure.c: * memory_failure_queue - Schedule handling memory failure of a page.
memory-failure.c:	spin_lock_irqsave(&mf_cpu->lock, proc_flags);
memory-failure.c:	if (kfifo_put(&mf_cpu->fifo, entry))
memory-failure.c:		schedule_work_on(smp_processor_id(), &mf_cpu->work);
memory-failure.c:	spin_unlock_irqrestore(&mf_cpu->lock, proc_flags);
memory-failure.c:		spin_lock_irqsave(&mf_cpu->lock, proc_flags);
memory-failure.c:		gotten = kfifo_get(&mf_cpu->fifo, &entry);
memory-failure.c:		spin_unlock_irqrestore(&mf_cpu->lock, proc_flags);
memory-failure.c:		spin_lock_init(&mf_cpu->lock);
memory-failure.c:		INIT_KFIFO(mf_cpu->fifo);
memory-failure.c:		INIT_WORK(&mf_cpu->work, memory_failure_work_func);
memory-failure.c: * unpoison_memory - Unpoison a previously poisoned page
memory-failure.c: * Software-unpoison a page that has been poisoned by
memory-failure.c: * This is only done on the software-level, so it only works
memory-failure.c: * Returns 0 for success, otherwise -errno.
memory-failure.c:		return -ENXIO;
memory-failure.c:		unpoison_pr_info("Unpoison: the hwpoison page has non-NULL mapping %#lx\n",
memory-failure.c:		unpoison_pr_info("Unpoison: Software-unpoisoned free page %#lx\n",
memory-failure.c:		unpoison_pr_info("Unpoison: Software-unpoisoned page %#lx\n",
memory-failure.c: * Returns 0 for a free page, -EIO for a zero refcount page
memory-failure.c:				__func__, pfn, p->flags);
memory-failure.c:			ret = -EIO;
memory-failure.c:				pfn, page->flags, &page->flags);
memory-failure.c:			return -EIO;
memory-failure.c:	 * This double-check of PageHWPoison is to avoid the race with
memory-failure.c:		return -EBUSY;
memory-failure.c:		return -EBUSY;
memory-failure.c:			pfn, ret, page->flags, &page->flags);
memory-failure.c:			ret = -EIO;
memory-failure.c:	 * memory_failure() also double-checks PageHWPoison inside page lock,
memory-failure.c:		return -EBUSY;
memory-failure.c:	 * RED-PEN would be better to keep it isolated here, but we
memory-failure.c:		list_add(&page->lru, &pagelist);
memory-failure.c:				pfn, ret, page->flags, &page->flags);
memory-failure.c:				ret = -EIO;
memory-failure.c:			pfn, ret, page_count(page), page->flags, &page->flags);
memory-failure.c:			return -EBUSY;
memory-failure.c: * soft_offline_page - Soft offline a page.
memory-failure.c:		return -EBUSY;
pagewalk.c:// SPDX-License-Identifier: GPL-2.0
pagewalk.c:		err = walk->pte_entry(pte, addr, addr + PAGE_SIZE, walk);
pagewalk.c:		if (pmd_none(*pmd) || !walk->vma) {
pagewalk.c:			if (walk->pte_hole)
pagewalk.c:				err = walk->pte_hole(addr, next, walk);
pagewalk.c:		 * This implies that each ->pmd_entry() handler
pagewalk.c:		if (walk->pmd_entry)
pagewalk.c:			err = walk->pmd_entry(pmd, addr, next, walk);
pagewalk.c:		if (!walk->pte_entry)
pagewalk.c:		split_huge_pmd(walk->vma, pmd, addr);
pagewalk.c:		if (pud_none(*pud) || !walk->vma) {
pagewalk.c:			if (walk->pte_hole)
pagewalk.c:				err = walk->pte_hole(addr, next, walk);
pagewalk.c:		if (walk->pud_entry) {
pagewalk.c:			spinlock_t *ptl = pud_trans_huge_lock(pud, walk->vma);
pagewalk.c:				err = walk->pud_entry(pud, addr, next, walk);
pagewalk.c:		split_huge_pud(walk->vma, pud, addr);
pagewalk.c:		if (walk->pmd_entry || walk->pte_entry)
pagewalk.c:			if (walk->pte_hole)
pagewalk.c:				err = walk->pte_hole(addr, next, walk);
pagewalk.c:		if (walk->pmd_entry || walk->pte_entry)
pagewalk.c:	pgd = pgd_offset(walk->mm, addr);
pagewalk.c:			if (walk->pte_hole)
pagewalk.c:				err = walk->pte_hole(addr, next, walk);
pagewalk.c:		if (walk->pmd_entry || walk->pte_entry)
pagewalk.c:	struct vm_area_struct *vma = walk->vma;
pagewalk.c:		pte = huge_pte_offset(walk->mm, addr & hmask, sz);
pagewalk.c:			err = walk->hugetlb_entry(pte, hmask, addr, next, walk);
pagewalk.c:		else if (walk->pte_hole)
pagewalk.c:			err = walk->pte_hole(addr, next, walk);
pagewalk.c:	struct vm_area_struct *vma = walk->vma;
pagewalk.c:	if (walk->test_walk)
pagewalk.c:		return walk->test_walk(start, end, walk);
pagewalk.c:	 * define their ->pte_hole() callbacks, so let's delegate them to handle
pagewalk.c:	if (vma->vm_flags & VM_PFNMAP) {
pagewalk.c:		if (walk->pte_hole)
pagewalk.c:			err = walk->pte_hole(start, end, walk);
pagewalk.c:	struct vm_area_struct *vma = walk->vma;
pagewalk.c:		if (walk->hugetlb_entry)
pagewalk.c: * walk_page_range - walk page table with caller specific callbacks
pagewalk.c: * Recursively walk the page table tree of the process represented by @walk->mm
pagewalk.c: * some caller-specific works for each entry, by setting up pmd_entry(),
pagewalk.c: *  - 0  : succeeded to handle the current entry, and if you don't reach the
pagewalk.c: *  - >0 : succeeded to handle the current entry, and return to the caller
pagewalk.c: *  - <0 : failed to handle the current entry, and return to the caller
pagewalk.c: * its vm_flags. walk_page_test() and @walk->test_walk() are used for this
pagewalk.c: * caller-specific data to callbacks, @walk->private should be helpful.
pagewalk.c: *   @walk->mm->mmap_sem, because these function traverse vma list and/or
pagewalk.c:		return -EINVAL;
pagewalk.c:	if (!walk->mm)
pagewalk.c:		return -EINVAL;
pagewalk.c:	VM_BUG_ON_MM(!rwsem_is_locked(&walk->mm->mmap_sem), walk->mm);
pagewalk.c:	vma = find_vma(walk->mm, start);
pagewalk.c:			walk->vma = NULL;
pagewalk.c:		} else if (start < vma->vm_start) { /* outside vma */
pagewalk.c:			walk->vma = NULL;
pagewalk.c:			next = min(end, vma->vm_start);
pagewalk.c:			walk->vma = vma;
pagewalk.c:			next = min(end, vma->vm_end);
pagewalk.c:			vma = vma->vm_next;
pagewalk.c:		if (walk->vma || walk->pte_hole)
pagewalk.c:	if (!walk->mm)
pagewalk.c:		return -EINVAL;
pagewalk.c:	VM_BUG_ON(!rwsem_is_locked(&walk->mm->mmap_sem));
pagewalk.c:	walk->vma = vma;
pagewalk.c:	err = walk_page_test(vma->vm_start, vma->vm_end, walk);
pagewalk.c:	return __walk_page_range(vma->vm_start, vma->vm_end, walk);
memtest.c:// SPDX-License-Identifier: GPL-2.0
memtest.c:	0x7a6c7258554e494cULL, /* yeah ;-) */
memtest.c:	pr_info("  %016llx bad mem addr %pa - %pa reserved\n",
memtest.c:	memblock_reserve(start_bad, end_bad - start_bad);
memtest.c:	end = start + (size - (start_phys_aligned - start_phys)) / incr;
memtest.c:			pr_info("  %pa - %pa pattern %016llx\n",
memtest.c:			memtest(pattern, this_start, this_end - this_start);
memtest.c:	for (i = memtest_pattern-1; i < UINT_MAX; --i) {
cma.h:/* SPDX-License-Identifier: GPL-2.0 */
cma.h:	return cma->count >> cma->order_per_bit;
filemap.c: * Copyright (C) 1994-1999  Linus Torvalds
filemap.c:#include <linux/backing-dev.h>
filemap.c: * finished 'unifying' the page and buffer cache and SMP-threaded the
filemap.c: * page-cache, 21.05.1999, Ingo Molnar <mingo@redhat.com>
filemap.c: * SMP-threaded pagemap-LRU 1999, Andrea Arcangeli <andrea@suse.de>
filemap.c: *  ->i_mmap_rwsem		(truncate_pagecache)
filemap.c: *    ->private_lock		(__free_pte->__set_page_dirty_buffers)
filemap.c: *      ->swap_lock		(exclusive_swap_page, others)
filemap.c: *        ->mapping->tree_lock
filemap.c: *  ->i_mutex
filemap.c: *    ->i_mmap_rwsem		(truncate->unmap_mapping_range)
filemap.c: *  ->mmap_sem
filemap.c: *    ->i_mmap_rwsem
filemap.c: *      ->page_table_lock or pte_lock	(various, mainly in memory.c)
filemap.c: *        ->mapping->tree_lock	(arch-dependent flush_dcache_mmap_lock)
filemap.c: *  ->mmap_sem
filemap.c: *    ->lock_page		(access_process_vm)
filemap.c: *  ->i_mutex			(generic_perform_write)
filemap.c: *    ->mmap_sem		(fault_in_pages_readable->do_page_fault)
filemap.c: *  bdi->wb.list_lock
filemap.c: *    sb_lock			(fs/fs-writeback.c)
filemap.c: *    ->mapping->tree_lock	(__sync_single_inode)
filemap.c: *  ->i_mmap_rwsem
filemap.c: *    ->anon_vma.lock		(vma_adjust)
filemap.c: *  ->anon_vma.lock
filemap.c: *    ->page_table_lock or pte_lock	(anon_vma_prepare and various)
filemap.c: *  ->page_table_lock or pte_lock
filemap.c: *    ->swap_lock		(try_to_unmap_one)
filemap.c: *    ->private_lock		(try_to_unmap_one)
filemap.c: *    ->tree_lock		(try_to_unmap_one)
filemap.c: *    ->zone_lru_lock(zone)	(follow_page->mark_page_accessed)
filemap.c: *    ->zone_lru_lock(zone)	(check_pte_range->isolate_lru_page)
filemap.c: *    ->private_lock		(page_remove_rmap->set_page_dirty)
filemap.c: *    ->tree_lock		(page_remove_rmap->set_page_dirty)
filemap.c: *    bdi.wb->list_lock		(page_remove_rmap->set_page_dirty)
filemap.c: *    ->inode->i_lock		(page_remove_rmap->set_page_dirty)
filemap.c: *    ->memcg->move_lock	(page_remove_rmap->lock_page_memcg)
filemap.c: *    bdi.wb->list_lock		(zap_pte_range->set_page_dirty)
filemap.c: *    ->inode->i_lock		(zap_pte_range->set_page_dirty)
filemap.c: *    ->private_lock		(zap_pte_range->__set_page_dirty_buffers)
filemap.c: * ->i_mmap_rwsem
filemap.c: *   ->tasklist_lock            (memory_failure, collect_procs_ao)
filemap.c:	error = __radix_tree_create(&mapping->page_tree, page->index, 0,
filemap.c:		p = radix_tree_deref_slot_protected(slot, &mapping->tree_lock);
filemap.c:			return -EEXIST;
filemap.c:		mapping->nrexceptional--;
filemap.c:	__radix_tree_replace(&mapping->page_tree, node, slot, page,
filemap.c:	mapping->nrpages++;
filemap.c:		__radix_tree_lookup(&mapping->page_tree, page->index + i,
filemap.c:		radix_tree_clear_tags(&mapping->page_tree, node, slot);
filemap.c:		__radix_tree_replace(&mapping->page_tree, node, slot, shadow,
filemap.c:		mapping->nrexceptional += nr;
filemap.c:	mapping->nrpages -= nr;
filemap.c: * sure the page is locked and that nobody else uses it - or that usage
filemap.c:	struct address_space *mapping = page->mapping;
filemap.c:			 current->comm, page_to_pfn(page));
filemap.c:	page->mapping = NULL;
filemap.c:	/* Leave page->index set: truncation lookup relies upon it */
filemap.c:	__mod_node_page_state(page_pgdat(page), NR_FILE_PAGES, -nr);
filemap.c:		__mod_node_page_state(page_pgdat(page), NR_SHMEM, -nr);
filemap.c:		account_page_cleaned(page, mapping, inode_to_wb(mapping->host));
filemap.c: * delete_from_page_cache - delete page from page cache
filemap.c:	freepage = mapping->a_ops->freepage;
filemap.c:	spin_lock_irqsave(&mapping->tree_lock, flags);
filemap.c:	spin_unlock_irqrestore(&mapping->tree_lock, flags);
filemap.c:	if (test_bit(AS_ENOSPC, &mapping->flags) &&
filemap.c:	    test_and_clear_bit(AS_ENOSPC, &mapping->flags))
filemap.c:		ret = -ENOSPC;
filemap.c:	if (test_bit(AS_EIO, &mapping->flags) &&
filemap.c:	    test_and_clear_bit(AS_EIO, &mapping->flags))
filemap.c:		ret = -EIO;
filemap.c:	if (test_bit(AS_EIO, &mapping->flags))
filemap.c:		return -EIO;
filemap.c:	if (test_bit(AS_ENOSPC, &mapping->flags))
filemap.c:		return -ENOSPC;
filemap.c: * __filemap_fdatawrite_range - start writeback on mapping dirty pages in range
filemap.c:	wbc_attach_fdatawrite_inode(&wbc, mapping->host);
filemap.c: * filemap_flush - mostly a non-blocking flush
filemap.c: * This is a mostly non-blocking flush.  Not suitable for data-integrity
filemap.c: * purposes - I/O may not be started against all dirty pages.
filemap.c: * filemap_range_has_page - check if a page exists in range.
filemap.c:	if (mapping->nrpages == 0)
filemap.c:			min(end - index, (pgoff_t)PAGEVEC_SIZE-1) + 1)) != 0) {
filemap.c:			if (page->index > end)
filemap.c: * filemap_fdatawait_range - wait for writeback to complete
filemap.c: * Walk the list of under-writeback pages of the given address space
filemap.c: * file_fdatawait_range - wait for writeback to complete
filemap.c: * Walk the list of under-writeback pages of the address space that file
filemap.c: * status of the address space vs. the file->f_wb_err cursor and return it.
filemap.c:	struct address_space *mapping = file->f_mapping;
filemap.c: * filemap_fdatawait_keep_errors - wait for writeback without clearing errors
filemap.c: * Walk the list of under-writeback pages of the given address space
filemap.c: * call sites are system-wide / filesystem-wide data flushers: e.g. sync(2),
filemap.c:	return (!dax_mapping(mapping) && mapping->nrpages) ||
filemap.c:	    (dax_mapping(mapping) && mapping->nrexceptional);
filemap.c:		 * written partially (e.g. -ENOSPC), so we wait for it.
filemap.c:		 * But the -EIO is special case, it may indicate the worst
filemap.c:		if (err != -EIO) {
filemap.c: * filemap_write_and_wait_range - write out & wait on a file range
filemap.c: * Write out and wait upon file offsets lstart->lend, inclusive.
filemap.c: * that this function can be used to write to the very end-of-file (end = -1).
filemap.c:		if (err != -EIO) {
filemap.c:	errseq_t eseq = errseq_set(&mapping->wb_err, err);
filemap.c: * file_check_and_advance_wb_err - report wb error (if any) that was previously
filemap.c: * While we handle mapping->wb_err with atomic operations, the f_wb_err
filemap.c:	errseq_t old = READ_ONCE(file->f_wb_err);
filemap.c:	struct address_space *mapping = file->f_mapping;
filemap.c:	if (errseq_check(&mapping->wb_err, old)) {
filemap.c:		spin_lock(&file->f_lock);
filemap.c:		old = file->f_wb_err;
filemap.c:		err = errseq_check_and_advance(&mapping->wb_err,
filemap.c:						&file->f_wb_err);
filemap.c:		spin_unlock(&file->f_lock);
filemap.c:	clear_bit(AS_EIO, &mapping->flags);
filemap.c:	clear_bit(AS_ENOSPC, &mapping->flags);
filemap.c: * file_write_and_wait_range - write out & wait on a file range
filemap.c: * Write out and wait upon file offsets lstart->lend, inclusive.
filemap.c: * that this function can be used to write to the very end-of-file (end = -1).
filemap.c:	struct address_space *mapping = file->f_mapping;
filemap.c:		if (err != -EIO)
filemap.c: * replace_page_cache_page - replace a pagecache page with a new one
filemap.c:	VM_BUG_ON_PAGE(new->mapping, new);
filemap.c:		struct address_space *mapping = old->mapping;
filemap.c:		pgoff_t offset = old->index;
filemap.c:		freepage = mapping->a_ops->freepage;
filemap.c:		new->mapping = mapping;
filemap.c:		new->index = offset;
filemap.c:		spin_lock_irqsave(&mapping->tree_lock, flags);
filemap.c:		spin_unlock_irqrestore(&mapping->tree_lock, flags);
filemap.c:		error = mem_cgroup_try_charge(page, current->mm,
filemap.c:	page->mapping = mapping;
filemap.c:	page->index = offset;
filemap.c:	spin_lock_irq(&mapping->tree_lock);
filemap.c:	spin_unlock_irq(&mapping->tree_lock);
filemap.c:	page->mapping = NULL;
filemap.c:	/* Leave page->index set: truncation relies upon it */
filemap.c:	spin_unlock_irq(&mapping->tree_lock);
filemap.c: * add_to_page_cache_locked - add a locked page to the pagecache
filemap.c:/* This has the same layout as wait_bit_key - see fs/cachefiles/rdwr.c */
filemap.c:	if (wait_page->page != key->page)
filemap.c:	key->page_match = 1;
filemap.c:	if (wait_page->bit_nr != key->bit_nr)
filemap.c:	if (test_bit(key->bit_nr, &key->page->flags))
filemap.c:		return -1;
filemap.c:	spin_lock_irqsave(&q->lock, flags);
filemap.c:		spin_unlock_irqrestore(&q->lock, flags);
filemap.c:		spin_lock_irqsave(&q->lock, flags);
filemap.c:	 * hash, so in that case check for a page match. That prevents a long-
filemap.c:	spin_unlock_irqrestore(&q->lock, flags);
filemap.c:	wait->flags = lock ? WQ_FLAG_EXCLUSIVE : 0;
filemap.c:	wait->func = wake_page_function;
filemap.c:		spin_lock_irq(&q->lock);
filemap.c:		if (likely(list_empty(&wait->entry))) {
filemap.c:		spin_unlock_irq(&q->lock);
filemap.c:		if (likely(test_bit(bit_nr, &page->flags))) {
filemap.c:			if (!test_and_set_bit_lock(bit_nr, &page->flags))
filemap.c:			if (!test_bit(bit_nr, &page->flags))
filemap.c:			ret = -EINTR;
filemap.c:	 * !waitqueue_active would be possible (by open-coding finish_wait),
filemap.c: * add_page_wait_queue - Add an arbitrary waiter to a page's wait queue
filemap.c:	spin_lock_irqsave(&q->lock, flags);
filemap.c:	spin_unlock_irqrestore(&q->lock, flags);
filemap.c: * unlock_page - unlock a locked page
filemap.c: * But that's OK - sleepers in wait_on_page_writeback() just go back to sleep.
filemap.c: * that contains PG_locked - thus the BUILD_BUG_ON(). That allows us to
filemap.c:	if (clear_bit_unlock_is_negative_byte(PG_locked, &page->flags))
filemap.c: * end_page_writeback - end writeback against a page
filemap.c: * __lock_page - get a lock on the page, assuming we need to sleep to get it
filemap.c: * 1 - page is locked; mmap_sem is still held.
filemap.c: * 0 - page is not locked.
filemap.c:		up_read(&mm->mmap_sem);
filemap.c:				up_read(&mm->mmap_sem);
filemap.c: * page_cache_next_hole - find the next hole (not-present entry)
filemap.c: * Search the set [index, min(index+max_scan-1, MAX_INDEX)] for the
filemap.c: * outside of the set specified (in which case 'return - index >=
filemap.c: * max_scan' will be true). In rare cases of index wrap-around, 0 will
filemap.c:		page = radix_tree_lookup(&mapping->page_tree, index);
filemap.c: * page_cache_prev_hole - find the prev hole (not-present entry)
filemap.c: * Search backwards in the range [max(index-max_scan+1, 0), index] for
filemap.c: * outside of the set specified (in which case 'index - return >=
filemap.c: * max_scan' will be true). In rare cases of wrap-around, ULONG_MAX
filemap.c:		page = radix_tree_lookup(&mapping->page_tree, index);
filemap.c:		index--;
filemap.c: * find_get_entry - find and get a page cache entry
filemap.c:	pagep = radix_tree_lookup_slot(&mapping->page_tree, offset);
filemap.c: * find_lock_entry - locate, pin and lock a page cache entry
filemap.c: * pagecache_get_page - find and get a page reference
filemap.c: * - FGP_ACCESSED: the page will be marked accessed
filemap.c: * - FGP_LOCK: Page is return locked
filemap.c: * - FGP_CREAT: If page is not present then a new page is allocated using
filemap.c:		if (unlikely(page->mapping != mapping)) {
filemap.c:		VM_BUG_ON_PAGE(page->index != offset, page);
filemap.c:			if (err == -EEXIST)
filemap.c: * find_get_entries - gang pagecache lookup
filemap.c: * The search returns a group of mapping-contiguous page cache entries
filemap.c: * not-present pages.
filemap.c:	radix_tree_for_each_slot(slot, &mapping->page_tree, &iter, start) {
filemap.c: * find_get_pages_range - gang pagecache lookup
filemap.c: * The search returns a group of mapping-contiguous pages with ascending
filemap.c: * indexes.  There may be holes in the indices due to not-present pages.
filemap.c:	radix_tree_for_each_slot(slot, &mapping->page_tree, &iter, *start) {
filemap.c:			*start = pages[ret - 1]->index + 1;
filemap.c:	 * breaks the iteration when there is page at index -1 but that is
filemap.c:	if (end == (pgoff_t)-1)
filemap.c:		*start = (pgoff_t)-1;
filemap.c: * find_get_pages_contig - gang contiguous pagecache lookup
filemap.c:	radix_tree_for_each_contig(slot, &mapping->page_tree, &iter, index) {
filemap.c:		if (page->mapping == NULL || page_to_pgoff(page) != iter.index) {
filemap.c: * find_get_pages_tag - find and return pages that match @tag
filemap.c:	radix_tree_for_each_tagged(slot, &mapping->page_tree,
filemap.c:		*index = pages[ret - 1]->index + 1;
filemap.c: * find_get_entries_tag - find and return entries that match @tag
filemap.c:	radix_tree_for_each_tagged(slot, &mapping->page_tree,
filemap.c: *      ---R__________________________________________B__________
filemap.c:	ra->ra_pages /= 4;
filemap.c: * generic_file_buffered_read - generic file read routine
filemap.c: * mapping->a_ops->readpage() function for the actual low-level stuff.
filemap.c:	struct file *filp = iocb->ki_filp;
filemap.c:	struct address_space *mapping = filp->f_mapping;
filemap.c:	struct inode *inode = mapping->host;
filemap.c:	struct file_ra_state *ra = &filp->f_ra;
filemap.c:	loff_t *ppos = &iocb->ki_pos;
filemap.c:	if (unlikely(*ppos >= inode->i_sb->s_maxbytes))
filemap.c:	iov_iter_truncate(iter, inode->i_sb->s_maxbytes);
filemap.c:	prev_index = ra->prev_pos >> PAGE_SHIFT;
filemap.c:	prev_offset = ra->prev_pos & (PAGE_SIZE-1);
filemap.c:	last_index = (*ppos + iter->count + PAGE_SIZE-1) >> PAGE_SHIFT;
filemap.c:			error = -EINTR;
filemap.c:			if (iocb->ki_flags & IOCB_NOWAIT)
filemap.c:					index, last_index - index);
filemap.c:					index, last_index - index);
filemap.c:			if (iocb->ki_flags & IOCB_NOWAIT) {
filemap.c:			if (inode->i_blkbits == PAGE_SHIFT ||
filemap.c:					!mapping->a_ops->is_partially_uptodate)
filemap.c:			if (unlikely(iter->type & ITER_PIPE))
filemap.c:			if (!page->mapping)
filemap.c:			if (!mapping->a_ops->is_partially_uptodate(page,
filemap.c:							offset, iter->count))
filemap.c:		 * the correct value for "nr", which means the zero-filled
filemap.c:		 * another truncate extends the file - this is desired though).
filemap.c:		end_index = (isize - 1) >> PAGE_SHIFT;
filemap.c:			nr = ((isize - 1) & ~PAGE_MASK) + 1;
filemap.c:		nr = nr - offset;
filemap.c:		 * Ok, we have the page, and it's up-to-date, so
filemap.c:			error = -EFAULT;
filemap.c:		if (!page->mapping) {
filemap.c:		error = mapping->a_ops->readpage(filp, page);
filemap.c:				if (page->mapping == NULL) {
filemap.c:				error = -EIO;
filemap.c:			error = -ENOMEM;
filemap.c:			if (error == -EEXIST) {
filemap.c:	error = -EAGAIN;
filemap.c:	ra->prev_pos = prev_index;
filemap.c:	ra->prev_pos <<= PAGE_SHIFT;
filemap.c:	ra->prev_pos |= prev_offset;
filemap.c: * generic_file_read_iter - generic filesystem read routine
filemap.c:	if (iocb->ki_flags & IOCB_DIRECT) {
filemap.c:		struct file *file = iocb->ki_filp;
filemap.c:		struct address_space *mapping = file->f_mapping;
filemap.c:		struct inode *inode = mapping->host;
filemap.c:		if (iocb->ki_flags & IOCB_NOWAIT) {
filemap.c:			if (filemap_range_has_page(mapping, iocb->ki_pos,
filemap.c:						   iocb->ki_pos + count - 1))
filemap.c:				return -EAGAIN;
filemap.c:						iocb->ki_pos,
filemap.c:					        iocb->ki_pos + count - 1);
filemap.c:		retval = mapping->a_ops->direct_IO(iocb, iter);
filemap.c:			iocb->ki_pos += retval;
filemap.c:			count -= retval;
filemap.c:		iov_iter_revert(iter, count - iov_iter_count(iter));
filemap.c:		if (retval < 0 || !count || iocb->ki_pos >= size ||
filemap.c: * page_cache_read - adds requested page to the page cache if not already there
filemap.c:	struct address_space *mapping = file->f_mapping;
filemap.c:			return -ENOMEM;
filemap.c:			ret = mapping->a_ops->readpage(file, page);
filemap.c:		else if (ret == -EEXIST)
filemap.c:	struct address_space *mapping = file->f_mapping;
filemap.c:	/* If we don't want any read-ahead, don't bother */
filemap.c:	if (vma->vm_flags & VM_RAND_READ)
filemap.c:	if (!ra->ra_pages)
filemap.c:	if (vma->vm_flags & VM_SEQ_READ) {
filemap.c:					  ra->ra_pages);
filemap.c:	if (ra->mmap_miss < MMAP_LOTSAMISS * 10)
filemap.c:		ra->mmap_miss++;
filemap.c:	 * stop bothering with read-ahead. It will only hurt.
filemap.c:	if (ra->mmap_miss > MMAP_LOTSAMISS)
filemap.c:	 * mmap read-around
filemap.c:	ra->start = max_t(long, 0, offset - ra->ra_pages / 2);
filemap.c:	ra->size = ra->ra_pages;
filemap.c:	ra->async_size = ra->ra_pages / 4;
filemap.c:	struct address_space *mapping = file->f_mapping;
filemap.c:	/* If we don't want any read-ahead, don't bother */
filemap.c:	if (vma->vm_flags & VM_RAND_READ)
filemap.c:	if (ra->mmap_miss > 0)
filemap.c:		ra->mmap_miss--;
filemap.c:					   page, offset, ra->ra_pages);
filemap.c: * filemap_fault - read in file data for page fault handling
filemap.c: * vma->vm_mm->mmap_sem must be held on entry.
filemap.c:	struct file *file = vmf->vma->vm_file;
filemap.c:	struct address_space *mapping = file->f_mapping;
filemap.c:	struct file_ra_state *ra = &file->f_ra;
filemap.c:	struct inode *inode = mapping->host;
filemap.c:	pgoff_t offset = vmf->pgoff;
filemap.c:	if (likely(page) && !(vmf->flags & FAULT_FLAG_TRIED)) {
filemap.c:		do_async_mmap_readahead(vmf->vma, ra, file, page, offset);
filemap.c:		do_sync_mmap_readahead(vmf->vma, ra, file, offset);
filemap.c:		count_memcg_event_mm(vmf->vma->vm_mm, PGMAJFAULT);
filemap.c:	if (!lock_page_or_retry(page, vmf->vma->vm_mm, vmf->flags)) {
filemap.c:	if (unlikely(page->mapping != mapping)) {
filemap.c:	VM_BUG_ON_PAGE(page->index != offset, page);
filemap.c:	 * that it's up-to-date. If not, it is going to be due to an error.
filemap.c:	vmf->page = page;
filemap.c:	error = page_cache_read(file, offset, vmf->gfp_mask);
filemap.c:	if (error == -ENOMEM)
filemap.c:	 * Umm, take care of errors if the page isn't up-to-date.
filemap.c:	 * Try to re-read it _once_. We do this synchronously,
filemap.c:	error = mapping->a_ops->readpage(file, page);
filemap.c:			error = -EIO;
filemap.c:	struct file *file = vmf->vma->vm_file;
filemap.c:	struct address_space *mapping = file->f_mapping;
filemap.c:	radix_tree_for_each_slot(slot, &mapping->page_tree, &iter,
filemap.c:		if (page->mapping != mapping || !PageUptodate(page))
filemap.c:		max_idx = DIV_ROUND_UP(i_size_read(mapping->host), PAGE_SIZE);
filemap.c:		if (page->index >= max_idx)
filemap.c:		if (file->f_ra.mmap_miss > 0)
filemap.c:			file->f_ra.mmap_miss--;
filemap.c:		vmf->address += (iter.index - last_pgoff) << PAGE_SHIFT;
filemap.c:		if (vmf->pte)
filemap.c:			vmf->pte += iter.index - last_pgoff;
filemap.c:		if (pmd_trans_huge(*vmf->pmd))
filemap.c:	struct page *page = vmf->page;
filemap.c:	struct inode *inode = file_inode(vmf->vma->vm_file);
filemap.c:	sb_start_pagefault(inode->i_sb);
filemap.c:	file_update_time(vmf->vma->vm_file);
filemap.c:	if (page->mapping != inode->i_mapping) {
filemap.c:	sb_end_pagefault(inode->i_sb);
filemap.c:	struct address_space *mapping = file->f_mapping;
filemap.c:	if (!mapping->a_ops->readpage)
filemap.c:		return -ENOEXEC;
filemap.c:	vma->vm_ops = &generic_file_vm_ops;
filemap.c: * This is for filesystems which do not implement ->writepage.
filemap.c:	if ((vma->vm_flags & VM_SHARED) && (vma->vm_flags & VM_MAYWRITE))
filemap.c:		return -EINVAL;
filemap.c:	return -ENOSYS;
filemap.c:	return -ENOSYS;
filemap.c:			page = ERR_PTR(-EIO);
filemap.c:			return ERR_PTR(-ENOMEM);
filemap.c:			if (err == -EEXIST)
filemap.c:	if (!page->mapping) {
filemap.c: * read_cache_page - read into page cache, fill it if needed
filemap.c: * If the page does not get brought uptodate, return -EIO.
filemap.c: * read_cache_page_gfp - read into page cache, using specified page allocation flags.
filemap.c: * If the page does not get brought uptodate, return -EIO.
filemap.c:	filler_t *filler = (filler_t *)mapping->a_ops->readpage;
filemap.c:	struct file *file = iocb->ki_filp;
filemap.c:	struct inode *inode = file->f_mapping->host;
filemap.c:	if (iocb->ki_flags & IOCB_APPEND)
filemap.c:		iocb->ki_pos = i_size_read(inode);
filemap.c:	pos = iocb->ki_pos;
filemap.c:	if ((iocb->ki_flags & IOCB_NOWAIT) && !(iocb->ki_flags & IOCB_DIRECT))
filemap.c:		return -EINVAL;
filemap.c:		if (iocb->ki_pos >= limit) {
filemap.c:			return -EFBIG;
filemap.c:		iov_iter_truncate(from, limit - (unsigned long)pos);
filemap.c:				!(file->f_flags & O_LARGEFILE))) {
filemap.c:			return -EFBIG;
filemap.c:		iov_iter_truncate(from, MAX_NON_LFS - (unsigned long)pos);
filemap.c:	if (unlikely(pos >= inode->i_sb->s_maxbytes))
filemap.c:		return -EFBIG;
filemap.c:	iov_iter_truncate(from, inode->i_sb->s_maxbytes - pos);
filemap.c:	const struct address_space_operations *aops = mapping->a_ops;
filemap.c:	return aops->write_begin(file, mapping, pos, len, flags,
filemap.c:	const struct address_space_operations *aops = mapping->a_ops;
filemap.c:	return aops->write_end(file, mapping, pos, len, copied, page, fsdata);
filemap.c:	struct file	*file = iocb->ki_filp;
filemap.c:	struct address_space *mapping = file->f_mapping;
filemap.c:	struct inode	*inode = mapping->host;
filemap.c:	loff_t		pos = iocb->ki_pos;
filemap.c:	end = (pos + write_len - 1) >> PAGE_SHIFT;
filemap.c:	if (iocb->ki_flags & IOCB_NOWAIT) {
filemap.c:		if (filemap_range_has_page(inode->i_mapping, pos,
filemap.c:			return -EAGAIN;
filemap.c:							pos + write_len - 1);
filemap.c:	 * without clobbering -EIOCBQUEUED from ->direct_IO().
filemap.c:		if (written == -EBUSY)
filemap.c:	written = mapping->a_ops->direct_IO(iocb, from);
filemap.c:	 * cached by non-direct readahead, or faulted in by get_user_pages()
filemap.c:	if (mapping->nrpages)
filemap.c:		write_len -= written;
filemap.c:		if (pos > i_size_read(inode) && !S_ISBLK(inode->i_mode)) {
filemap.c:		iocb->ki_pos = pos;
filemap.c:	iov_iter_revert(from, write_len - iov_iter_count(from));
filemap.c:	struct address_space *mapping = file->f_mapping;
filemap.c:	const struct address_space_operations *a_ops = mapping->a_ops;
filemap.c:		offset = (pos & (PAGE_SIZE - 1));
filemap.c:		bytes = min_t(unsigned long, PAGE_SIZE - offset,
filemap.c:		 * up-to-date.
filemap.c:			status = -EFAULT;
filemap.c:			status = -EINTR;
filemap.c:		status = a_ops->write_begin(file, mapping, pos, bytes, flags,
filemap.c:		status = a_ops->write_end(file, mapping, pos, bytes, copied,
filemap.c:			bytes = min_t(unsigned long, PAGE_SIZE - offset,
filemap.c: * __generic_file_write_iter - write data to a file
filemap.c:	struct file *file = iocb->ki_filp;
filemap.c:	struct address_space * mapping = file->f_mapping;
filemap.c:	struct inode 	*inode = mapping->host;
filemap.c:	current->backing_dev_info = inode_to_bdi(inode);
filemap.c:	if (iocb->ki_flags & IOCB_DIRECT) {
filemap.c:		 * page-cache pages correctly).
filemap.c:		status = generic_perform_write(file, from, pos = iocb->ki_pos);
filemap.c:		 * direct-written, or the error code if that was zero.  Note
filemap.c:		 * that this differs from normal direct-io semantics, which
filemap.c:		 * will return -EFOO even if some bytes were written.
filemap.c:		endbyte = pos + status - 1;
filemap.c:			iocb->ki_pos = endbyte + 1;
filemap.c:			 * the number of bytes which were direct-written
filemap.c:		written = generic_perform_write(file, from, iocb->ki_pos);
filemap.c:			iocb->ki_pos += written;
filemap.c:	current->backing_dev_info = NULL;
filemap.c: * generic_file_write_iter - write data to a file
filemap.c:	struct file *file = iocb->ki_filp;
filemap.c:	struct inode *inode = file->f_mapping->host;
filemap.c: * try_to_release_page() - release old fs-specific metadata on a page
filemap.c: * (presumably at page->private).  If the release was successful, return '1'.
filemap.c:	struct address_space * const mapping = page->mapping;
filemap.c:	if (mapping && mapping->a_ops->releasepage)
filemap.c:		return mapping->a_ops->releasepage(page, gfp_mask);
page_isolation.c:// SPDX-License-Identifier: GPL-2.0
page_isolation.c:#include <linux/page-isolation.h>
page_isolation.c:#include <linux/pageblock-flags.h>
page_isolation.c:	int ret = -EBUSY;
page_isolation.c:	spin_lock_irqsave(&zone->lock, flags);
page_isolation.c:	 * immobile means "not-on-lru" pages. If immobile is larger than
page_isolation.c:	 * removable-by-driver pages reported by notifier, we'll fail.
page_isolation.c:		zone->nr_isolate_pageblock++;
page_isolation.c:		__mod_zone_freepage_state(zone, -nr_pages, migratetype);
page_isolation.c:	spin_unlock_irqrestore(&zone->lock, flags);
page_isolation.c:	spin_lock_irqsave(&zone->lock, flags);
page_isolation.c:			buddy = page + (buddy_pfn - pfn);
page_isolation.c:	zone->nr_isolate_pageblock--;
page_isolation.c:	spin_unlock_irqrestore(&zone->lock, flags);
page_isolation.c: * start_isolate_page_range() -- make page-allocation-type of range of pages
page_isolation.c: * Making page-allocation-type to be MIGRATE_ISOLATE means free pages in
page_isolation.c: * Returns 0 on success and -EBUSY if any part of range cannot be isolated.
page_isolation.c:	return -EBUSY;
page_isolation.c: * zone->lock must be held before call this.
page_isolation.c:	page = __first_valid_page(start_pfn, end_pfn - start_pfn);
page_isolation.c:		return -EBUSY;
page_isolation.c:	spin_lock_irqsave(&zone->lock, flags);
page_isolation.c:	spin_unlock_irqrestore(&zone->lock, flags);
page_isolation.c:	return pfn < end_pfn ? -EBUSY : 0;
hugetlb.c:	bool free = (spool->count == 0) && (spool->used_hpages == 0);
hugetlb.c:	spin_unlock(&spool->lock);
hugetlb.c:		if (spool->min_hpages != -1)
hugetlb.c:			hugetlb_acct_memory(spool->hstate,
hugetlb.c:						-spool->min_hpages);
hugetlb.c:	spin_lock_init(&spool->lock);
hugetlb.c:	spool->count = 1;
hugetlb.c:	spool->max_hpages = max_hpages;
hugetlb.c:	spool->hstate = h;
hugetlb.c:	spool->min_hpages = min_hpages;
hugetlb.c:	if (min_hpages != -1 && hugetlb_acct_memory(h, min_hpages)) {
hugetlb.c:	spool->rsv_hpages = min_hpages;
hugetlb.c:	spin_lock(&spool->lock);
hugetlb.c:	BUG_ON(!spool->count);
hugetlb.c:	spool->count--;
hugetlb.c: * Return -ENOMEM if there are not enough resources to satisfy the
hugetlb.c:	spin_lock(&spool->lock);
hugetlb.c:	if (spool->max_hpages != -1) {		/* maximum size accounting */
hugetlb.c:		if ((spool->used_hpages + delta) <= spool->max_hpages)
hugetlb.c:			spool->used_hpages += delta;
hugetlb.c:			ret = -ENOMEM;
hugetlb.c:	if (spool->min_hpages != -1 && spool->rsv_hpages) {
hugetlb.c:		if (delta > spool->rsv_hpages) {
hugetlb.c:			ret = delta - spool->rsv_hpages;
hugetlb.c:			spool->rsv_hpages = 0;
hugetlb.c:			spool->rsv_hpages -= delta;
hugetlb.c:	spin_unlock(&spool->lock);
hugetlb.c:	spin_lock(&spool->lock);
hugetlb.c:	if (spool->max_hpages != -1)		/* maximum size accounting */
hugetlb.c:		spool->used_hpages -= delta;
hugetlb.c:	if (spool->min_hpages != -1 && spool->used_hpages < spool->min_hpages) {
hugetlb.c:		if (spool->rsv_hpages + delta <= spool->min_hpages)
hugetlb.c:			ret = spool->rsv_hpages + delta - spool->min_hpages;
hugetlb.c:		spool->rsv_hpages += delta;
hugetlb.c:		if (spool->rsv_hpages > spool->min_hpages)
hugetlb.c:			spool->rsv_hpages = spool->min_hpages;
hugetlb.c:	return HUGETLBFS_SB(inode->i_sb)->spool;
hugetlb.c:	return subpool_inode(file_inode(vma->vm_file));
hugetlb.c: * Region tracking -- allows tracking of reservations and instantiated pages
hugetlb.c: * arithmetic as 4(to) - 0(from) = 4 huge pages in the region.
hugetlb.c:	struct list_head *head = &resv->regions;
hugetlb.c:	spin_lock(&resv->lock);
hugetlb.c:		if (f <= rg->to)
hugetlb.c:	if (&rg->link == head || t < rg->from) {
hugetlb.c:		VM_BUG_ON(resv->region_cache_count <= 0);
hugetlb.c:		resv->region_cache_count--;
hugetlb.c:		nrg = list_first_entry(&resv->region_cache, struct file_region,
hugetlb.c:		list_del(&nrg->link);
hugetlb.c:		nrg->from = f;
hugetlb.c:		nrg->to = t;
hugetlb.c:		list_add(&nrg->link, rg->link.prev);
hugetlb.c:		add += t - f;
hugetlb.c:	if (f > rg->from)
hugetlb.c:		f = rg->from;
hugetlb.c:	list_for_each_entry_safe(rg, trg, rg->link.prev, link) {
hugetlb.c:		if (&rg->link == head)
hugetlb.c:		if (rg->from > t)
hugetlb.c:		if (rg->to > t)
hugetlb.c:			t = rg->to;
hugetlb.c:			add -= (rg->to - rg->from);
hugetlb.c:			list_del(&rg->link);
hugetlb.c:	add += (nrg->from - f);		/* Added to beginning of region */
hugetlb.c:	nrg->from = f;
hugetlb.c:	add += t - nrg->to;		/* Added to end of region */
hugetlb.c:	nrg->to = t;
hugetlb.c:	resv->adds_in_progress--;
hugetlb.c:	spin_unlock(&resv->lock);
hugetlb.c: * zero.  -ENOMEM is returned if a new file_region structure or cache entry
hugetlb.c:	struct list_head *head = &resv->regions;
hugetlb.c:	spin_lock(&resv->lock);
hugetlb.c:	resv->adds_in_progress++;
hugetlb.c:	if (resv->adds_in_progress > resv->region_cache_count) {
hugetlb.c:		VM_BUG_ON(resv->adds_in_progress - resv->region_cache_count > 1);
hugetlb.c:		resv->adds_in_progress--;
hugetlb.c:		spin_unlock(&resv->lock);
hugetlb.c:			return -ENOMEM;
hugetlb.c:		spin_lock(&resv->lock);
hugetlb.c:		list_add(&trg->link, &resv->region_cache);
hugetlb.c:		resv->region_cache_count++;
hugetlb.c:		if (f <= rg->to)
hugetlb.c:	if (&rg->link == head || t < rg->from) {
hugetlb.c:			resv->adds_in_progress--;
hugetlb.c:			spin_unlock(&resv->lock);
hugetlb.c:				return -ENOMEM;
hugetlb.c:			nrg->from = f;
hugetlb.c:			nrg->to   = f;
hugetlb.c:			INIT_LIST_HEAD(&nrg->link);
hugetlb.c:		list_add(&nrg->link, rg->link.prev);
hugetlb.c:		chg = t - f;
hugetlb.c:	if (f > rg->from)
hugetlb.c:		f = rg->from;
hugetlb.c:	chg = t - f;
hugetlb.c:	list_for_each_entry(rg, rg->link.prev, link) {
hugetlb.c:		if (&rg->link == head)
hugetlb.c:		if (rg->from > t)
hugetlb.c:		if (rg->to > t) {
hugetlb.c:			chg += rg->to - t;
hugetlb.c:			t = rg->to;
hugetlb.c:		chg -= rg->to - rg->from;
hugetlb.c:	spin_unlock(&resv->lock);
hugetlb.c:	spin_unlock(&resv->lock);
hugetlb.c:	spin_lock(&resv->lock);
hugetlb.c:	VM_BUG_ON(!resv->region_cache_count);
hugetlb.c:	resv->adds_in_progress--;
hugetlb.c:	spin_unlock(&resv->lock);
hugetlb.c: * be allocated.  If the allocation fails, -ENOMEM will be returned.
hugetlb.c: * a region and possibly return -ENOMEM.  Callers specifying
hugetlb.c: * t == LONG_MAX do not need to check for -ENOMEM error.
hugetlb.c:	struct list_head *head = &resv->regions;
hugetlb.c:	spin_lock(&resv->lock);
hugetlb.c:		if (rg->to <= f && (rg->to != rg->from || rg->to != f))
hugetlb.c:		if (rg->from >= t)
hugetlb.c:		if (f > rg->from && t < rg->to) { /* Must split region */
hugetlb.c:			    resv->region_cache_count > resv->adds_in_progress) {
hugetlb.c:				nrg = list_first_entry(&resv->region_cache,
hugetlb.c:				list_del(&nrg->link);
hugetlb.c:				resv->region_cache_count--;
hugetlb.c:				spin_unlock(&resv->lock);
hugetlb.c:					return -ENOMEM;
hugetlb.c:			del += t - f;
hugetlb.c:			nrg->from = t;
hugetlb.c:			nrg->to = rg->to;
hugetlb.c:			INIT_LIST_HEAD(&nrg->link);
hugetlb.c:			rg->to = f;
hugetlb.c:			list_add(&nrg->link, &rg->link);
hugetlb.c:		if (f <= rg->from && t >= rg->to) { /* Remove entire region */
hugetlb.c:			del += rg->to - rg->from;
hugetlb.c:			list_del(&rg->link);
hugetlb.c:		if (f <= rg->from) {	/* Trim beginning of region */
hugetlb.c:			del += t - rg->from;
hugetlb.c:			rg->from = t;
hugetlb.c:			del += rg->to - f;
hugetlb.c:			rg->to = f;
hugetlb.c:	spin_unlock(&resv->lock);
hugetlb.c:	struct list_head *head = &resv->regions;
hugetlb.c:	spin_lock(&resv->lock);
hugetlb.c:		if (rg->to <= f)
hugetlb.c:		if (rg->from >= t)
hugetlb.c:		seg_from = max(rg->from, f);
hugetlb.c:		seg_to = min(rg->to, t);
hugetlb.c:		chg += seg_to - seg_from;
hugetlb.c:	spin_unlock(&resv->lock);
hugetlb.c:	return ((address - vma->vm_start) >> huge_page_shift(h)) +
hugetlb.c:			(vma->vm_pgoff >> huge_page_order(h));
hugetlb.c: * architectures where it differs, an architecture-specific version of this
hugetlb.c:	return (unsigned long)vma->vm_private_data;
hugetlb.c:	vma->vm_private_data = (void *)value;
hugetlb.c:	kref_init(&resv_map->refs);
hugetlb.c:	spin_lock_init(&resv_map->lock);
hugetlb.c:	INIT_LIST_HEAD(&resv_map->regions);
hugetlb.c:	resv_map->adds_in_progress = 0;
hugetlb.c:	INIT_LIST_HEAD(&resv_map->region_cache);
hugetlb.c:	list_add(&rg->link, &resv_map->region_cache);
hugetlb.c:	resv_map->region_cache_count = 1;
hugetlb.c:	struct list_head *head = &resv_map->region_cache;
hugetlb.c:		list_del(&rg->link);
hugetlb.c:	VM_BUG_ON(resv_map->adds_in_progress);
hugetlb.c:	return inode->i_mapping->private_data;
hugetlb.c:	if (vma->vm_flags & VM_MAYSHARE) {
hugetlb.c:		struct address_space *mapping = vma->vm_file->f_mapping;
hugetlb.c:		struct inode *inode = mapping->host;
hugetlb.c:	VM_BUG_ON_VMA(vma->vm_flags & VM_MAYSHARE, vma);
hugetlb.c:	VM_BUG_ON_VMA(vma->vm_flags & VM_MAYSHARE, vma);
hugetlb.c:	if (!(vma->vm_flags & VM_MAYSHARE))
hugetlb.c:		vma->vm_private_data = (void *)0;
hugetlb.c:	if (vma->vm_flags & VM_NORESERVE) {
hugetlb.c:		 * properly, so add work-around here.
hugetlb.c:		if (vma->vm_flags & VM_MAYSHARE && chg == 0)
hugetlb.c:	if (vma->vm_flags & VM_MAYSHARE) {
hugetlb.c:		 * Very Subtle - The value of chg comes from a previous
hugetlb.c:	list_move(&page->lru, &h->hugepage_freelists[nid]);
hugetlb.c:	h->free_huge_pages++;
hugetlb.c:	h->free_huge_pages_node[nid]++;
hugetlb.c:	list_for_each_entry(page, &h->hugepage_freelists[nid], lru)
hugetlb.c:	 * if 'non-isolated free hugepage' not found on the list,
hugetlb.c:	if (&h->hugepage_freelists[nid] == &page->lru)
hugetlb.c:	list_move(&page->lru, &h->hugepage_activelist);
hugetlb.c:	h->free_huge_pages--;
hugetlb.c:	h->free_huge_pages_node[nid]--;
hugetlb.c:	int node = -1;
hugetlb.c:			h->free_huge_pages - h->resv_huge_pages == 0)
hugetlb.c:	if (avoid_reserve && h->free_huge_pages - h->resv_huge_pages == 0)
hugetlb.c:		h->resv_huge_pages--;
hugetlb.c: * nodes_allowed previously, so h->next_node_to_{alloc|free} might
hugetlb.c:	nid = get_valid_node_allowed(h->next_nid_to_alloc, nodes_allowed);
hugetlb.c:	h->next_nid_to_alloc = next_node_allowed(nid, nodes_allowed);
hugetlb.c: * helper for free_pool_huge_page() - return the previously saved
hugetlb.c:	nid = get_valid_node_allowed(h->next_nid_to_free, nodes_allowed);
hugetlb.c:	h->next_nid_to_free = next_node_allowed(nid, nodes_allowed);
hugetlb.c:		nr_nodes--)
hugetlb.c:		nr_nodes--)
hugetlb.c:	unsigned long last_pfn = start_pfn + nr_pages - 1;
hugetlb.c:		spin_lock_irqsave(&zone->lock, flags);
hugetlb.c:		pfn = ALIGN(zone->zone_start_pfn, nr_pages);
hugetlb.c:				spin_unlock_irqrestore(&zone->lock, flags);
hugetlb.c:				spin_lock_irqsave(&zone->lock, flags);
hugetlb.c:		spin_unlock_irqrestore(&zone->lock, flags);
hugetlb.c:	h->nr_huge_pages--;
hugetlb.c:	h->nr_huge_pages_node[page_to_nid(page)]--;
hugetlb.c: * Test to determine whether the hugepage is "active/in-use" (i.e. being linked
hugetlb.c: * to hstate->hugepage_activelist.)
hugetlb.c:	page->mapping = NULL;
hugetlb.c:		h->resv_huge_pages++;
hugetlb.c:	if (h->surplus_huge_pages_node[nid]) {
hugetlb.c:		list_del(&page->lru);
hugetlb.c:		h->surplus_huge_pages--;
hugetlb.c:		h->surplus_huge_pages_node[nid]--;
hugetlb.c:	INIT_LIST_HEAD(&page->lru);
hugetlb.c:	h->nr_huge_pages++;
hugetlb.c:	h->nr_huge_pages_node[nid]++;
hugetlb.c:		 * boot, it's safer to be consistent with the not-gigantic
hugetlb.c:	atomic_set(compound_mapcount_ptr(page), -1);
hugetlb.c:		compound_idx = page_to_pfn(page) - page_to_pfn(page_head);
hugetlb.c:		compound_idx = page - page_head;
hugetlb.c:		if ((!acct_surplus || h->surplus_huge_pages_node[node]) &&
hugetlb.c:		    !list_empty(&h->hugepage_freelists[node])) {
hugetlb.c:				list_entry(h->hugepage_freelists[node].next,
hugetlb.c:			list_del(&page->lru);
hugetlb.c:			h->free_huge_pages--;
hugetlb.c:			h->free_huge_pages_node[node]--;
hugetlb.c:				h->surplus_huge_pages--;
hugetlb.c:				h->surplus_huge_pages_node[node]--;
hugetlb.c: * nothing for in-use (including surplus) hugepages. Returns -EBUSY if the
hugetlb.c:		if (h->free_huge_pages - h->resv_huge_pages == 0) {
hugetlb.c:			rc = -EBUSY;
hugetlb.c:		list_del(&head->lru);
hugetlb.c:		h->free_huge_pages--;
hugetlb.c:		h->free_huge_pages_node[nid]--;
hugetlb.c:		h->max_huge_pages--;
hugetlb.c:	 * called by process A. B will only examine the per-node
hugetlb.c:	 * won't be able to increment the per-node counter, until the
hugetlb.c:	 * per-node value is checked there.
hugetlb.c:	if (h->surplus_huge_pages >= h->nr_overcommit_huge_pages) {
hugetlb.c:		h->nr_huge_pages++;
hugetlb.c:		h->surplus_huge_pages++;
hugetlb.c:		INIT_LIST_HEAD(&page->lru);
hugetlb.c:		h->nr_huge_pages_node[r_nid]++;
hugetlb.c:		h->surplus_huge_pages_node[r_nid]++;
hugetlb.c:		h->nr_huge_pages--;
hugetlb.c:		h->surplus_huge_pages--;
hugetlb.c: * E.g. soft-offlining uses this function because it only cares physical
hugetlb.c:	if (h->free_huge_pages - h->resv_huge_pages > 0)
hugetlb.c:	if (h->free_huge_pages - h->resv_huge_pages > 0) {
hugetlb.c:	needed = (h->resv_huge_pages + delta) - h->free_huge_pages;
hugetlb.c:		h->resv_huge_pages += delta;
hugetlb.c:	ret = -ENOMEM;
hugetlb.c:		list_add(&page->lru, &surplus_list);
hugetlb.c:	needed = (h->resv_huge_pages + delta) -
hugetlb.c:			(h->free_huge_pages + allocated);
hugetlb.c:	h->resv_huge_pages += delta;
hugetlb.c:		if ((--needed) < 0)
hugetlb.c:		 * no users -- drop the buddy allocator's reference.
hugetlb.c:	 * by pre-allocated pages. Only free surplus pages.
hugetlb.c:	nr_pages = min(unused_resv_pages, h->surplus_huge_pages);
hugetlb.c:	 * on-line nodes with memory and will handle the hstate accounting.
hugetlb.c:	while (nr_pages--) {
hugetlb.c:		h->resv_huge_pages--;
hugetlb.c:		unused_resv_pages--;
hugetlb.c:	h->resv_huge_pages -= unused_resv_pages;
hugetlb.c:		if (vma->vm_flags & VM_MAYSHARE)
hugetlb.c:	if (vma->vm_flags & VM_MAYSHARE)
hugetlb.c:		 * Subtle - The reserve map for private mappings has the
hugetlb.c:		return ERR_PTR(-ENOMEM);
hugetlb.c:			return ERR_PTR(-ENOSPC);
hugetlb.c:			h->resv_huge_pages--;
hugetlb.c:		list_move(&page->lru, &h->hugepage_activelist);
hugetlb.c:		hugetlb_acct_memory(h, -rsv_adjust);
hugetlb.c:	return ERR_PTR(-ENOSPC);
hugetlb.c:	list_add(&m->list, &huge_boot_pages);
hugetlb.c:	m->hstate = h;
hugetlb.c:	if (unlikely(order > (MAX_ORDER - 1)))
hugetlb.c:		struct hstate *h = m->hstate;
hugetlb.c:		page = pfn_to_page(m->phys >> PAGE_SHIFT);
hugetlb.c:		prep_compound_huge_page(page, h->order);
hugetlb.c:		 * side-effects, like CommitLimit going negative.
hugetlb.c:			adjust_managed_page_count(page, 1 << h->order);
hugetlb.c:	for (i = 0; i < h->max_huge_pages; ++i) {
hugetlb.c:	if (i < h->max_huge_pages) {
hugetlb.c:			h->max_huge_pages, buf, i);
hugetlb.c:		h->max_huge_pages = i;
hugetlb.c:		pr_info("HugeTLB registered %s page size, pre-allocated %ld pages\n",
hugetlb.c:			buf, h->free_huge_pages);
hugetlb.c:		struct list_head *freel = &h->hugepage_freelists[i];
hugetlb.c:			if (count >= h->nr_huge_pages)
hugetlb.c:			list_del(&page->lru);
hugetlb.c:			h->free_huge_pages--;
hugetlb.c:			h->free_huge_pages_node[page_to_nid(page)]--;
hugetlb.c: * Increment or decrement surplus_huge_pages.  Keep node-specific counters
hugetlb.c: * balanced by operating on them in a round-robin fashion.
hugetlb.c:	VM_BUG_ON(delta != -1 && delta != 1);
hugetlb.c:			if (h->surplus_huge_pages_node[node])
hugetlb.c:			if (h->surplus_huge_pages_node[node] <
hugetlb.c:					h->nr_huge_pages_node[node])
hugetlb.c:	h->surplus_huge_pages += delta;
hugetlb.c:	h->surplus_huge_pages_node[node] += delta;
hugetlb.c:#define persistent_huge_pages(h) (h->nr_huge_pages - h->surplus_huge_pages)
hugetlb.c:		return h->max_huge_pages;
hugetlb.c:	while (h->surplus_huge_pages && count > persistent_huge_pages(h)) {
hugetlb.c:		if (!adjust_pool_surplus(h, nodes_allowed, -1))
hugetlb.c:		/* Bail for signals. Probably ctrl-c from user */
hugetlb.c:	min_count = h->resv_huge_pages + h->nr_huge_pages - h->free_huge_pages;
hugetlb.c:		nr_huge_pages = h->nr_huge_pages;
hugetlb.c:		nr_huge_pages = h->nr_huge_pages_node[nid];
hugetlb.c:		err = -EINVAL;
hugetlb.c:		count += h->nr_huge_pages - h->nr_huge_pages_node[nid];
hugetlb.c:	h->max_huge_pages = set_max_huge_pages(h, count, nodes_allowed);
hugetlb.c: * hstate attribute for optionally mempolicy-based constraint on persistent
hugetlb.c:	return sprintf(buf, "%lu\n", h->nr_overcommit_huge_pages);
hugetlb.c:		return -EINVAL;
hugetlb.c:	h->nr_overcommit_huge_pages = input;
hugetlb.c:		free_huge_pages = h->free_huge_pages;
hugetlb.c:		free_huge_pages = h->free_huge_pages_node[nid];
hugetlb.c:	return sprintf(buf, "%lu\n", h->resv_huge_pages);
hugetlb.c:		surplus_huge_pages = h->surplus_huge_pages;
hugetlb.c:		surplus_huge_pages = h->surplus_huge_pages_node[nid];
hugetlb.c:	hstate_kobjs[hi] = kobject_create_and_add(h->name, parent);
hugetlb.c:		return -ENOMEM;
hugetlb.c:			pr_err("Hugetlb: Unable to add hstate %s", h->name);
hugetlb.c: * node_hstate/s - associate per node hstate attributes, via their kobjects,
hugetlb.c: * kobj_to_node_hstate - lookup global hstate for node device hstate attr kobj.
hugetlb.c: * Returns node id via non-NULL nidp.
hugetlb.c:			if (nhs->hstate_kobjs[i] == kobj) {
hugetlb.c: * No-op if no hstate attributes attached.
hugetlb.c:	struct node_hstate *nhs = &node_hstates[node->dev.id];
hugetlb.c:	if (!nhs->hugepages_kobj)
hugetlb.c:		if (nhs->hstate_kobjs[idx]) {
hugetlb.c:			kobject_put(nhs->hstate_kobjs[idx]);
hugetlb.c:			nhs->hstate_kobjs[idx] = NULL;
hugetlb.c:	kobject_put(nhs->hugepages_kobj);
hugetlb.c:	nhs->hugepages_kobj = NULL;
hugetlb.c: * No-op if attributes already registered.
hugetlb.c:	struct node_hstate *nhs = &node_hstates[node->dev.id];
hugetlb.c:	if (nhs->hugepages_kobj)
hugetlb.c:	nhs->hugepages_kobj = kobject_create_and_add("hugepages",
hugetlb.c:							&node->dev.kobj);
hugetlb.c:	if (!nhs->hugepages_kobj)
hugetlb.c:		err = hugetlb_sysfs_add_hstate(h, nhs->hugepages_kobj,
hugetlb.c:						nhs->hstate_kobjs,
hugetlb.c:				h->name, node->dev.id);
hugetlb.c: * devices of nodes that have memory.  All on-line nodes should have
hugetlb.c:		if (node->dev.id == nid)
hugetlb.c:		*nidp = -1;
hugetlb.c:	h->order = order;
hugetlb.c:	h->mask = ~((1ULL << (order + PAGE_SHIFT)) - 1);
hugetlb.c:	h->nr_huge_pages = 0;
hugetlb.c:	h->free_huge_pages = 0;
hugetlb.c:		INIT_LIST_HEAD(&h->hugepage_freelists[i]);
hugetlb.c:	INIT_LIST_HEAD(&h->hugepage_activelist);
hugetlb.c:	h->next_nid_to_alloc = first_memory_node;
hugetlb.c:	h->next_nid_to_free = first_memory_node;
hugetlb.c:	snprintf(h->name, HSTATE_NAME_LEN, "hugepages-%lukB",
hugetlb.c:		mhp = &parsed_hstate->max_huge_pages;
hugetlb.c:	if (hugetlb_max_hstate && parsed_hstate->order >= MAX_ORDER)
hugetlb.c:	unsigned long tmp = h->max_huge_pages;
hugetlb.c:		return -EOPNOTSUPP;
hugetlb.c:	table->data = &tmp;
hugetlb.c:	table->maxlen = sizeof(unsigned long);
hugetlb.c:		return -EOPNOTSUPP;
hugetlb.c:	tmp = h->nr_overcommit_huge_pages;
hugetlb.c:		return -EINVAL;
hugetlb.c:	table->data = &tmp;
hugetlb.c:	table->maxlen = sizeof(unsigned long);
hugetlb.c:		h->nr_overcommit_huge_pages = tmp;
hugetlb.c:			h->nr_huge_pages,
hugetlb.c:			h->free_huge_pages,
hugetlb.c:			h->resv_huge_pages,
hugetlb.c:			h->surplus_huge_pages,
hugetlb.c:			1UL << (huge_page_order(h) + PAGE_SHIFT - 10));
hugetlb.c:		nid, h->nr_huge_pages_node[nid],
hugetlb.c:		nid, h->free_huge_pages_node[nid],
hugetlb.c:		nid, h->surplus_huge_pages_node[nid]);
hugetlb.c:				h->nr_huge_pages_node[nid],
hugetlb.c:				h->free_huge_pages_node[nid],
hugetlb.c:				h->surplus_huge_pages_node[nid],
hugetlb.c:				1UL << (huge_page_order(h) + PAGE_SHIFT - 10));
hugetlb.c:		   atomic_long_read(&mm->hugetlb_usage) << (PAGE_SHIFT - 10));
hugetlb.c:		nr_total_pages += h->nr_huge_pages * pages_per_huge_page(h);
hugetlb.c:	int ret = -ENOMEM;
hugetlb.c:		if (delta > cpuset_mems_nr(h->free_huge_pages_node)) {
hugetlb.c:		return_unused_surplus_pages(h, (unsigned long) -delta);
hugetlb.c:		kref_get(&resv->refs);
hugetlb.c:	start = vma_hugecache_offset(h, vma, vma->vm_start);
hugetlb.c:	end = vma_hugecache_offset(h, vma, vma->vm_end);
hugetlb.c:	reserve = (end - start) - region_count(resv, start, end);
hugetlb.c:	kref_put(&resv->refs, resv_map_release);
hugetlb.c:		hugetlb_acct_memory(h, -gbl_reserve);
hugetlb.c:		return -EINVAL;
hugetlb.c: * handle_mm_fault() to try to instantiate regular-sized pages in the
hugetlb.c:					 vma->vm_page_prot)));
hugetlb.c:					   vma->vm_page_prot));
hugetlb.c:	cow = (vma->vm_flags & (VM_SHARED | VM_MAYWRITE)) == VM_MAYWRITE;
hugetlb.c:	mmun_start = vma->vm_start;
hugetlb.c:	mmun_end = vma->vm_end;
hugetlb.c:	for (addr = vma->vm_start; addr < vma->vm_end; addr += sz) {
hugetlb.c:			ret = -ENOMEM;
hugetlb.c:	struct mm_struct *mm = vma->vm_mm;
hugetlb.c:	vma->vm_flags &= ~VM_MAYSHARE;
hugetlb.c:	mm = vma->vm_mm;
hugetlb.c:	pgoff = ((address - vma->vm_start) >> PAGE_SHIFT) +
hugetlb.c:			vma->vm_pgoff;
hugetlb.c:	mapping = vma->vm_file->f_mapping;
hugetlb.c:	vma_interval_tree_foreach(iter_vma, &mapping->i_mmap, pgoff, pgoff) {
hugetlb.c:		if (iter_vma->vm_flags & VM_MAYSHARE)
hugetlb.c:		 * areas. This is because a future no-page fault on this VMA
hugetlb.c:	/* If no-one else is actually using this page, avoid the copy
hugetlb.c:			 * race occurs while re-acquiring page table
hugetlb.c:		ret = (PTR_ERR(new_page) == -ENOMEM) ?
hugetlb.c:	mapping = vma->vm_file->f_mapping;
hugetlb.c:	mapping = vma->vm_file->f_mapping;
hugetlb.c:	struct inode *inode = mapping->host;
hugetlb.c:	spin_lock(&inode->i_lock);
hugetlb.c:	inode->i_blocks += blocks_per_huge_page(h);
hugetlb.c:	spin_unlock(&inode->i_lock);
hugetlb.c:			   current->pid);
hugetlb.c:		size = i_size_read(mapping->host) >> huge_page_shift(h);
hugetlb.c:			if (ret == -ENOMEM)
hugetlb.c:		if (vma->vm_flags & VM_MAYSHARE) {
hugetlb.c:				if (err == -EEXIST)
hugetlb.c:	if ((flags & FAULT_FLAG_WRITE) && !(vma->vm_flags & VM_SHARED)) {
hugetlb.c:	size = i_size_read(mapping->host) >> huge_page_shift(h);
hugetlb.c:	new_pte = make_huge_pte(vma, page, ((vma->vm_flags & VM_WRITE)
hugetlb.c:				&& (vma->vm_flags & VM_SHARED)));
hugetlb.c:	if ((flags & FAULT_FLAG_WRITE) && !(vma->vm_flags & VM_SHARED)) {
hugetlb.c:	if (vma->vm_flags & VM_SHARED) {
hugetlb.c:	return hash & (num_fault_mutexes - 1);
hugetlb.c:	mapping = vma->vm_file->f_mapping;
hugetlb.c:		if (!(vma->vm_flags & VM_MAYSHARE))
hugetlb.c:	int vm_shared = dst_vma->vm_flags & VM_SHARED;
hugetlb.c:		ret = -ENOMEM;
hugetlb.c:			ret = -EFAULT;
hugetlb.c:	mapping = dst_vma->vm_file->f_mapping;
hugetlb.c:		size = i_size_read(mapping->host) >> huge_page_shift(h);
hugetlb.c:		ret = -EFAULT;
hugetlb.c:	size = i_size_read(mapping->host) >> huge_page_shift(h);
hugetlb.c:	ret = -EFAULT;
hugetlb.c:	ret = -EEXIST;
hugetlb.c:	_dst_pte = make_huge_pte(dst_vma, page, dst_vma->vm_flags & VM_WRITE);
hugetlb.c:	if (dst_vma->vm_flags & VM_WRITE)
hugetlb.c:					dst_vma->vm_flags & VM_WRITE);
hugetlb.c:	/* No need to invalidate - it was non-present before */
hugetlb.c:	int err = -EFAULT;
hugetlb.c:	while (vaddr < vma->vm_end && remainder) {
hugetlb.c:		--remainder;
hugetlb.c:		if (vaddr < vma->vm_end && remainder &&
hugetlb.c:	struct mm_struct *mm = vma->vm_mm;
hugetlb.c:	i_mmap_lock_write(vma->vm_file->f_mapping);
hugetlb.c:	i_mmap_unlock_write(vma->vm_file->f_mapping);
hugetlb.c:	return pages << h->order;
hugetlb.c:		return -EINVAL;
hugetlb.c:	 * to reserve the full area even if read-only as mprotect() may be
hugetlb.c:	 * called to make the mapping read-write. Assume !vma is a shm mapping
hugetlb.c:	if (!vma || vma->vm_flags & VM_MAYSHARE) {
hugetlb.c:			return -ENOMEM;
hugetlb.c:		chg = to - from;
hugetlb.c:		ret = -ENOSPC;
hugetlb.c:	 * the reservation was consumed. Private mappings are per-VMA and
hugetlb.c:	if (!vma || vma->vm_flags & VM_MAYSHARE) {
hugetlb.c:								chg - add);
hugetlb.c:			hugetlb_acct_memory(h, -rsv_adjust);
hugetlb.c:	if (!vma || vma->vm_flags & VM_MAYSHARE)
hugetlb.c:		kref_put(&resv_map->refs, resv_map_release);
hugetlb.c:	spin_lock(&inode->i_lock);
hugetlb.c:	inode->i_blocks -= (blocks_per_huge_page(h) * freed);
hugetlb.c:	spin_unlock(&inode->i_lock);
hugetlb.c:	gbl_reserve = hugepage_subpool_put_pages(spool, (chg - freed));
hugetlb.c:	hugetlb_acct_memory(h, -gbl_reserve);
hugetlb.c:	unsigned long saddr = ((idx - svma->vm_pgoff) << PAGE_SHIFT) +
hugetlb.c:				svma->vm_start;
hugetlb.c:	unsigned long vm_flags = vma->vm_flags & VM_LOCKED_CLEAR_MASK;
hugetlb.c:	unsigned long svm_flags = svma->vm_flags & VM_LOCKED_CLEAR_MASK;
hugetlb.c:	    sbase < svma->vm_start || svma->vm_end < s_end)
hugetlb.c:	if (vma->vm_flags & VM_MAYSHARE &&
hugetlb.c:	    vma->vm_start <= base && end <= vma->vm_end)
hugetlb.c: * pud has to be populated inside the same i_mmap_rwsem section - otherwise
hugetlb.c:	struct address_space *mapping = vma->vm_file->f_mapping;
hugetlb.c:	pgoff_t idx = ((addr - vma->vm_start) >> PAGE_SHIFT) +
hugetlb.c:			vma->vm_pgoff;
hugetlb.c:	vma_interval_tree_foreach(svma, &mapping->i_mmap, idx, idx) {
hugetlb.c:			spte = huge_pte_offset(svma->vm_mm, saddr,
hugetlb.c:	*addr = ALIGN(*addr, HPAGE_SIZE * PTRS_PER_PTE) - HPAGE_SIZE;
hugetlb.c: * huge_pte_offset() - Walk the page table to resolve the hugepage
hugetlb.c:	return ERR_PTR(-EINVAL);
hugetlb.c:	list_move_tail(&page->lru, list);
hugetlb.c:	list_move_tail(&page->lru, &(page_hstate(page))->hugepage_activelist);
frontswap.c: * Copyright (C) 2009-2012 Oracle Corp.  All rights reserved.
frontswap.c: * is a simple singly-linked list of all registered implementations.
frontswap.c:	for ((ops) = frontswap_ops; (ops); (ops) = (ops)->next)
frontswap.c:		if (!WARN_ON(!si->frontswap_map))
frontswap.c:			set_bit(si->type, a);
frontswap.c:		ops->init(i);
frontswap.c:	 * Setting frontswap_ops must happen after the ops->init() calls
frontswap.c:		ops->next = frontswap_ops;
frontswap.c:	} while (cmpxchg(&frontswap_ops, ops->next, ops) != ops->next);
frontswap.c:		if (si->frontswap_map)
frontswap.c:			set_bit(si->type, b);
frontswap.c:	 * calls, we re-check and do init or invalidate for any changed
frontswap.c:				ops->init(i);
frontswap.c:				ops->invalidate_area(i);
frontswap.c:	 * p->frontswap is a bitmap that we MUST have to figure out which page
frontswap.c:	 * p->frontswap set to something valid to work properly.
frontswap.c:		ops->init(type);
frontswap.c:	if (sis->frontswap_map)
frontswap.c:		return test_bit(offset, sis->frontswap_map);
frontswap.c:	set_bit(offset, sis->frontswap_map);
frontswap.c:	atomic_inc(&sis->frontswap_pages);
frontswap.c:	clear_bit(offset, sis->frontswap_map);
frontswap.c:	atomic_dec(&sis->frontswap_pages);
frontswap.c:	int ret = -1;
frontswap.c:			ops->invalidate_page(type, offset);
frontswap.c:		ret = ops->store(type, offset, page);
frontswap.c:		ret = -1;
frontswap.c:	int ret = -1;
frontswap.c:		return -1;
frontswap.c:		ret = ops->load(type, offset, page);
frontswap.c:		ops->invalidate_page(type, offset);
frontswap.c:	if (sis->frontswap_map == NULL)
frontswap.c:		ops->invalidate_area(type);
frontswap.c:	atomic_set(&sis->frontswap_pages, 0);
frontswap.c:	bitmap_zero(sis->frontswap_map, sis->max);
frontswap.c:		totalpages += atomic_read(&si->frontswap_pages);
frontswap.c:	int ret = -EINVAL;
frontswap.c:		si_frontswap_pages = atomic_read(&si->frontswap_pages);
frontswap.c:		if (security_vm_enough_memory_mm(current->mm, pages)) {
frontswap.c:			ret = -ENOMEM;
frontswap.c:		*swapid = si->type;
frontswap.c:	total_pages_to_unuse = total_pages - target_pages;
frontswap.c: * unuse enough frontswap pages to attempt to -- subject to memory
frontswap.c: * constraints -- reduce the number of pages in frontswap to the
frontswap.c:		return -ENXIO;
balloon_compaction.c: * balloon_page_enqueue - allocates a new page and inserts it into the balloon
balloon_compaction.c:	spin_lock_irqsave(&b_dev_info->pages_lock, flags);
balloon_compaction.c:	spin_unlock_irqrestore(&b_dev_info->pages_lock, flags);
balloon_compaction.c: * balloon_page_dequeue - removes a page from balloon's page list and returns
balloon_compaction.c: * Driver must call it to properly de-allocate a previous enlisted balloon page
balloon_compaction.c:	spin_lock_irqsave(&b_dev_info->pages_lock, flags);
balloon_compaction.c:	list_for_each_entry_safe(page, tmp, &b_dev_info->pages, lru) {
balloon_compaction.c:	spin_unlock_irqrestore(&b_dev_info->pages_lock, flags);
balloon_compaction.c:		spin_lock_irqsave(&b_dev_info->pages_lock, flags);
balloon_compaction.c:		if (unlikely(list_empty(&b_dev_info->pages) &&
balloon_compaction.c:			     !b_dev_info->isolated_pages))
balloon_compaction.c:		spin_unlock_irqrestore(&b_dev_info->pages_lock, flags);
balloon_compaction.c:	spin_lock_irqsave(&b_dev_info->pages_lock, flags);
balloon_compaction.c:	list_del(&page->lru);
balloon_compaction.c:	b_dev_info->isolated_pages++;
balloon_compaction.c:	spin_unlock_irqrestore(&b_dev_info->pages_lock, flags);
balloon_compaction.c:	spin_lock_irqsave(&b_dev_info->pages_lock, flags);
balloon_compaction.c:	list_add(&page->lru, &b_dev_info->pages);
balloon_compaction.c:	b_dev_info->isolated_pages--;
balloon_compaction.c:	spin_unlock_irqrestore(&b_dev_info->pages_lock, flags);
balloon_compaction.c:		return -EINVAL;
balloon_compaction.c:	return balloon->migratepage(balloon, newpage, page, mode);
hwpoison-inject.c:		return -EPERM;
hwpoison-inject.c:		return -ENXIO;
hwpoison-inject.c:	 * This implies unable to support non-LRU pages.
hwpoison-inject.c:		return -EPERM;
hwpoison-inject.c:		return -ENOMEM;
hwpoison-inject.c:	dentry = debugfs_create_file("corrupt-pfn", 0200, hwpoison_dir,
hwpoison-inject.c:	dentry = debugfs_create_file("unpoison-pfn", 0200, hwpoison_dir,
hwpoison-inject.c:	dentry = debugfs_create_u32("corrupt-filter-enable", 0600,
hwpoison-inject.c:	dentry = debugfs_create_u32("corrupt-filter-dev-major", 0600,
hwpoison-inject.c:	dentry = debugfs_create_u32("corrupt-filter-dev-minor", 0600,
hwpoison-inject.c:	dentry = debugfs_create_u64("corrupt-filter-flags-mask", 0600,
hwpoison-inject.c:	dentry = debugfs_create_u64("corrupt-filter-flags-value", 0600,
hwpoison-inject.c:	dentry = debugfs_create_u64("corrupt-filter-memcg", 0600,
hwpoison-inject.c:	return -ENOMEM;
swap.c: * Linux VM subsystem. Fine-tuning documentation can be found in
swap.c:#include <linux/backing-dev.h>
swap.c: * This path almost never happens for VM activity - pages are normally
swap.c:		lruvec = mem_cgroup_page_lruvec(page, zone->zone_pgdat);
swap.c:		put_dev_pagemap(page->pgmap);
swap.c: * put_pages_list() - release a list of pages
swap.c: * @pages: list of pages threaded on page->lru
swap.c:		victim = list_entry(pages->prev, struct page, lru);
swap.c:		list_del(&victim->lru);
swap.c: * get_kernel_pages() - pin kernel pages in memory
swap.c: * were pinned, returns -errno. Each page returned must be released
swap.c: * get_kernel_page() - pin a kernel page in memory
swap.c: * -errno. The page returned must be released with a put_page() call
swap.c:		struct page *page = pvec->pages[i];
swap.c:				spin_unlock_irqrestore(&pgdat->lru_lock, flags);
swap.c:			spin_lock_irqsave(&pgdat->lru_lock, flags);
swap.c:		spin_unlock_irqrestore(&pgdat->lru_lock, flags);
swap.c:	release_pages(pvec->pages, pvec->nr, pvec->cold);
swap.c:	struct zone_reclaim_stat *reclaim_stat = &lruvec->reclaim_stat;
swap.c:	reclaim_stat->recent_scanned[file]++;
swap.c:		reclaim_stat->recent_rotated[file]++;
swap.c:	__activate_page(page, mem_cgroup_page_lruvec(page, zone->zone_pgdat), NULL);
swap.c:	for (i = pagevec_count(pvec) - 1; i >= 0; i--) {
swap.c:		struct page *pagevec_page = pvec->pages[i];
swap.c: * inactive,unreferenced	->	inactive,referenced
swap.c: * inactive,referenced		->	active,unreferenced
swap.c: * active,unreferenced		->	active,referenced
swap.c: * When a newly allocated page is not yet visible, so safe for non-atomic ops,
swap.c: * lru_cache_add - add a page to a page list
swap.c: * add_page_to_unevictable_list - add a page to the unevictable list
swap.c:	spin_lock_irq(&pgdat->lru_lock);
swap.c:	spin_unlock_irq(&pgdat->lru_lock);
swap.c:	if (likely((vma->vm_flags & (VM_LOCKED | VM_SPECIAL)) != VM_LOCKED)) {
swap.c:		 * We use the irq-unsafe __mod_zone_page_stat because this
swap.c: * effective than the single-page writeout from reclaim.
swap.c: * 1. active, mapped page -> none
swap.c: * 2. active, dirty/writeback page -> inactive, head, PG_reclaim
swap.c: * 3. inactive, mapped page -> none
swap.c: * 4. inactive, dirty/writeback page -> inactive, head, PG_reclaim
swap.c: * 5. inactive, clean -> inactive, tail
swap.c: * 6. Others -> none
swap.c: * than the single-page writeout from reclaim.
swap.c:		 * is _really_ small and  it's non-critical problem.
swap.c:		list_move_tail(&page->lru, &lruvec->lists[lru]);
swap.c: * disabled; or "cpu" is being hot-unplugged, and is already dead.
swap.c: * deactivate_file_page - forcefully deactivate a file page
swap.c: * mark_page_lazyfree - make an anon page lazyfree
swap.c: * release_pages - batched put_page()
swap.c:		 * Make sure the IRQ-safe lock-holding time does not get
swap.c:			spin_unlock_irqrestore(&locked_pgdat->lru_lock, flags);
swap.c:				spin_unlock_irqrestore(&locked_pgdat->lru_lock,
swap.c:				spin_unlock_irqrestore(&locked_pgdat->lru_lock, flags);
swap.c:					spin_unlock_irqrestore(&locked_pgdat->lru_lock,
swap.c:				spin_lock_irqsave(&locked_pgdat->lru_lock, flags);
swap.c:		list_add(&page->lru, &pages_to_free);
swap.c:		spin_unlock_irqrestore(&locked_pgdat->lru_lock, flags);
swap.c: * The pages which we're about to release may be in the deferred lru-addition
swap.c: * OK from a correctness point of view but is inefficient - those pages may be
swap.c: * cache-warm and we want to give them back to the page allocator ASAP.
swap.c:	release_pages(pvec->pages, pagevec_count(pvec), pvec->cold);
swap.c:		  !spin_is_locked(&lruvec_pgdat(lruvec)->lru_lock));
swap.c:		list_add_tail(&page_tail->lru, &page->lru);
swap.c:		list_add_tail(&page_tail->lru, list);
swap.c:		list_head = page_tail->lru.prev;
swap.c:		list_move_tail(&page_tail->lru, list_head);
swap.c: * pagevec_lookup_entries - gang pagecache lookup
swap.c: * The search returns a group of mapping-contiguous entries with
swap.c: * not-present entries.
swap.c:	pvec->nr = find_get_entries(mapping, start, nr_pages,
swap.c:				    pvec->pages, indices);
swap.c: * pagevec_remove_exceptionals - pagevec exceptionals pruning
swap.c: * passed on to page-only pagevec operations.
swap.c:		struct page *page = pvec->pages[i];
swap.c:			pvec->pages[j++] = page;
swap.c:	pvec->nr = j;
swap.c: * pagevec_lookup_range - gang pagecache lookup
swap.c: * The search returns a group of mapping-contiguous pages with ascending
swap.c: * indexes.  There may be holes in the indices due to not-present pages. We
swap.c:	pvec->nr = find_get_pages_range(mapping, start, end, PAGEVEC_SIZE,
swap.c:					pvec->pages);
swap.c:	pvec->nr = find_get_pages_tag(mapping, index, tag,
swap.c:					nr_pages, pvec->pages);
swap.c:	unsigned long megs = totalram_pages >> (20 - PAGE_SHIFT);
swap.c:	/* Use a smaller cluster for small-memory machines */
memory.c: * demand-loading started 01.12.91 - seems it is high on the list of
memory.c: * things wanted, and it should be easy to implement. - Linus
memory.c: * Ok, demand-loading was easy, shared pages a little bit tricker. Shared
memory.c: * pages started 02.12.91, seems to work. - Linus.
memory.c: * Also corrected some "invalidate()"s - I wasn't doing enough of them.
memory.c: * 19.12.91  -  works, somewhat. Sometimes I get faults, don't know why.
memory.c: * 20.12.91  -  Ok, making the swap-device changeable like the root.
memory.c: * 05.04.94  -  Multi-page memory management added for v1.1.
memory.c: * 16.07.99  -  Support of BIGMEM added by Gerhard Wichert, Siemens AG
memory.c:#include <linux/dma-debug.h>
memory.c:#warning Unfortunate NUMA and NUMA Balancing config, growing page-frame for last_cpupid.
memory.c:/* use the per-pgdat data instead for discontigmem - mbligh */
memory.c:		if (current->rss_stat.count[i]) {
memory.c:			add_mm_counter(mm, i, current->rss_stat.count[i]);
memory.c:			current->rss_stat.count[i] = 0;
memory.c:	current->rss_stat.events = 0;
memory.c:	if (likely(task->mm == mm))
memory.c:		task->rss_stat.count[member] += val;
memory.c:#define dec_mm_counter_fast(mm, member) add_mm_counter_fast(mm, member, -1)
memory.c:	if (unlikely(task->rss_stat.events++ > TASK_RSS_EVENTS_THRESH))
memory.c:		sync_mm_rss(task->mm);
memory.c:	batch = tlb->active;
memory.c:	if (batch->next) {
memory.c:		tlb->active = batch->next;
memory.c:	if (tlb->batch_count == MAX_GATHER_BATCH_COUNT)
memory.c:	tlb->batch_count++;
memory.c:	batch->next = NULL;
memory.c:	batch->nr   = 0;
memory.c:	batch->max  = MAX_GATHER_BATCH;
memory.c:	tlb->active->next = batch;
memory.c:	tlb->active = batch;
memory.c:	tlb->mm = mm;
memory.c:	tlb->fullmm     = !(start | (end+1));
memory.c:	tlb->need_flush_all = 0;
memory.c:	tlb->local.next = NULL;
memory.c:	tlb->local.nr   = 0;
memory.c:	tlb->local.max  = ARRAY_SIZE(tlb->__pages);
memory.c:	tlb->active     = &tlb->local;
memory.c:	tlb->batch_count = 0;
memory.c:	tlb->batch = NULL;
memory.c:	tlb->page_size = 0;
memory.c:	if (!tlb->end)
memory.c:	mmu_notifier_invalidate_range(tlb->mm, tlb->start, tlb->end);
memory.c:	for (batch = &tlb->local; batch && batch->nr; batch = batch->next) {
memory.c:		free_pages_and_swap_cache(batch->pages, batch->nr);
memory.c:		batch->nr = 0;
memory.c:	tlb->active = &tlb->local;
memory.c:		__tlb_adjust_range(tlb, start, end - start);
memory.c:	for (batch = tlb->local.next; batch; batch = next) {
memory.c:		next = batch->next;
memory.c:	tlb->local.next = NULL;
memory.c:	VM_BUG_ON(!tlb->end);
memory.c:	VM_WARN_ON(tlb->page_size != page_size);
memory.c:	batch = tlb->active;
memory.c:	batch->pages[batch->nr++] = page;
memory.c:	if (batch->nr == batch->max) {
memory.c:		batch = tlb->active;
memory.c:	VM_BUG_ON_PAGE(batch->nr > batch->max, page);
memory.c:	 * This isn't an RCU grace period and hence the page-tables cannot be
memory.c:	 * assumed to be actually RCU-freed.
memory.c:	 * It is however sufficient for software page-table walkers that rely on
memory.c:	for (i = 0; i < batch->nr; i++)
memory.c:		__tlb_remove_table(batch->tables[i]);
memory.c:	struct mmu_table_batch **batch = &tlb->batch;
memory.c:		call_rcu_sched(&(*batch)->rcu, tlb_remove_table_rcu);
memory.c:	struct mmu_table_batch **batch = &tlb->batch;
memory.c:	 * concurrent page-table walk.
memory.c:	if (atomic_read(&tlb->mm->mm_users) < 2) {
memory.c:		(*batch)->nr = 0;
memory.c:	(*batch)->tables[(*batch)->nr++] = table;
memory.c:	if ((*batch)->nr == MAX_TABLE_BATCH)
memory.c: *	Called to initialize an (on-stack) mmu_gather structure for page-table
memory.c: *	tear-down from @mm. The @fullmm argument is used when @mm is without
memory.c:	inc_tlb_flush_pending(tlb->mm);
memory.c:	 * under non-exclusive lock(e.g., mmap_sem read-side) but defer TLB
memory.c:	bool force = mm_tlb_flush_nested(tlb->mm);
memory.c:	dec_tlb_flush_pending(tlb->mm);
memory.c:	atomic_long_dec(&tlb->mm->nr_ptes);
memory.c:	if (end - 1 > ceiling - 1)
memory.c:	mm_dec_nr_pmds(tlb->mm);
memory.c:	if (end - 1 > ceiling - 1)
memory.c:	if (end - 1 > ceiling - 1)
memory.c: * This function frees user-level page tables of a process.
memory.c:	 * Why all these "- 1"s?  Because 0 represents both the bottom
memory.c:	 * of the address space and the top of it (using -1 for the
memory.c:	 * Comparisons need to use "end - 1" and "ceiling - 1" (though
memory.c:	 * bother to round floor or end up - the tests don't need that.
memory.c:	if (end - 1 > ceiling - 1)
memory.c:		end -= PMD_SIZE;
memory.c:	if (addr > end - 1)
memory.c:	pgd = pgd_offset(tlb->mm, addr);
memory.c:		struct vm_area_struct *next = vma->vm_next;
memory.c:		unsigned long addr = vma->vm_start;
memory.c:			hugetlb_free_pgd_range(tlb, addr, vma->vm_end,
memory.c:				floor, next ? next->vm_start : ceiling);
memory.c:			while (next && next->vm_start <= vma->vm_end + PMD_SIZE
memory.c:				next = vma->vm_next;
memory.c:			free_pgd_range(tlb, addr, vma->vm_end,
memory.c:				floor, next ? next->vm_start : ceiling);
memory.c:		return -ENOMEM;
memory.c:	 * of a chain of data-dependent loads, meaning most CPUs (alpha
memory.c:	 * seen in-order. See the alpha page table accessors for the
memory.c:		atomic_long_inc(&mm->nr_ptes);
memory.c:		return -ENOMEM;
memory.c:	if (current->mm == mm)
memory.c: * is found. For example, we might have a PFN-mapped pte in
memory.c:	pgd_t *pgd = pgd_offset(vma->vm_mm, addr);
memory.c:	mapping = vma->vm_file ? vma->vm_file->f_mapping : NULL;
memory.c:		 current->comm,
memory.c:		 (void *)addr, vma->vm_flags, vma->anon_vma, mapping, index);
memory.c:		 vma->vm_file,
memory.c:		 vma->vm_ops ? vma->vm_ops->fault : NULL,
memory.c:		 vma->vm_file ? vma->vm_file->f_op->mmap : NULL,
memory.c:		 mapping ? mapping->a_ops->readpage : NULL);
memory.c: * vm_normal_page -- This function gets the "struct page" associated with a pte.
memory.c: *	pfn_of_page == vma->vm_pgoff + ((addr - vma->vm_start) >> PAGE_SHIFT)
memory.c:		if (vma->vm_ops && vma->vm_ops->find_special_page)
memory.c:			return vma->vm_ops->find_special_page(vma, addr);
memory.c:		if (vma->vm_flags & (VM_PFNMAP | VM_MIXEDMAP))
memory.c:		 * not on the lru and thus should never be involve with any-
memory.c:	if (unlikely(vma->vm_flags & (VM_PFNMAP|VM_MIXEDMAP))) {
memory.c:		if (vma->vm_flags & VM_MIXEDMAP) {
memory.c:			off = (addr - vma->vm_start) >> PAGE_SHIFT;
memory.c:			if (pfn == vma->vm_pgoff + off)
memory.c:			if (!is_cow_mapping(vma->vm_flags))
memory.c:	 * in a direct-access (dax) mapping, so let's just replicate the
memory.c:	if (unlikely(vma->vm_flags & (VM_PFNMAP|VM_MIXEDMAP))) {
memory.c:		if (vma->vm_flags & VM_MIXEDMAP) {
memory.c:			off = (addr - vma->vm_start) >> PAGE_SHIFT;
memory.c:			if (pfn == vma->vm_pgoff + off)
memory.c:			if (!is_cow_mapping(vma->vm_flags))
memory.c:	unsigned long vm_flags = vma->vm_flags;
memory.c:			if (unlikely(list_empty(&dst_mm->mmlist))) {
memory.c:				if (list_empty(&dst_mm->mmlist))
memory.c:					list_add(&dst_mm->mmlist,
memory.c:							&src_mm->mmlist);
memory.c:			 * We do not preserve soft-dirty information, because so
memory.c:		return -ENOMEM;
memory.c:		 * We are holding two locks at this point - either of them
memory.c:			return -ENOMEM;
memory.c:		return -ENOMEM;
memory.c:			VM_BUG_ON_VMA(next-addr != HPAGE_PMD_SIZE, vma);
memory.c:			if (err == -ENOMEM)
memory.c:				return -ENOMEM;
memory.c:			return -ENOMEM;
memory.c:		return -ENOMEM;
memory.c:			VM_BUG_ON_VMA(next-addr != HPAGE_PUD_SIZE, vma);
memory.c:			if (err == -ENOMEM)
memory.c:				return -ENOMEM;
memory.c:			return -ENOMEM;
memory.c:		return -ENOMEM;
memory.c:			return -ENOMEM;
memory.c:	unsigned long addr = vma->vm_start;
memory.c:	unsigned long end = vma->vm_end;
memory.c:	if (!(vma->vm_flags & (VM_HUGETLB | VM_PFNMAP | VM_MIXEDMAP)) &&
memory.c:			!vma->anon_vma)
memory.c:	if (unlikely(vma->vm_flags & VM_PFNMAP)) {
memory.c:	is_cow = is_cow_mapping(vma->vm_flags);
memory.c:			ret = -ENOMEM;
memory.c:	struct mm_struct *mm = tlb->mm;
memory.c:				if (details->check_mapping &&
memory.c:				    details->check_mapping != page_rmapping(page))
memory.c:							tlb->fullmm);
memory.c:				    likely(!(vma->vm_flags & VM_SEQ_READ)))
memory.c:			rss[mm_counter(page)]--;
memory.c:			if (unlikely(details && details->check_mapping)) {
memory.c:				if (details->check_mapping !=
memory.c:			pte_clear_not_present_full(mm, addr, pte, tlb->fullmm);
memory.c:			rss[mm_counter(page)]--;
memory.c:		/* If details->check_mapping, we leave swap entries. */
memory.c:			rss[MM_SWAPENTS]--;
memory.c:			rss[mm_counter(page)]--;
memory.c:		pte_clear_not_present_full(mm, addr, pte, tlb->fullmm);
memory.c:			if (next - addr != HPAGE_PMD_SIZE) {
memory.c:				    !rwsem_is_locked(&tlb->mm->mmap_sem), vma);
memory.c:			if (next - addr != HPAGE_PUD_SIZE) {
memory.c:				VM_BUG_ON_VMA(!rwsem_is_locked(&tlb->mm->mmap_sem), vma);
memory.c:	pgd = pgd_offset(vma->vm_mm, addr);
memory.c:	unsigned long start = max(vma->vm_start, start_addr);
memory.c:	if (start >= vma->vm_end)
memory.c:	end = min(vma->vm_end, end_addr);
memory.c:	if (end <= vma->vm_start)
memory.c:	if (vma->vm_file)
memory.c:	if (unlikely(vma->vm_flags & VM_PFNMAP))
memory.c:			 * It is undesirable to test vma->vm_file as it
memory.c:			 * should be non-null for valid hugetlb area.
memory.c:			 * hugetlbfs ->mmap method fails,
memory.c:			 * mmap_region() nullifies vma->vm_file
memory.c:			if (vma->vm_file) {
memory.c:				i_mmap_lock_write(vma->vm_file->f_mapping);
memory.c:				i_mmap_unlock_write(vma->vm_file->f_mapping);
memory.c: * unmap_vmas - unmap a range of memory covered by a list of vma's
memory.c: * ensure that any thus-far unmapped pages are flushed before unmap_vmas()
memory.c:	struct mm_struct *mm = vma->vm_mm;
memory.c:	for ( ; vma && vma->vm_start < end_addr; vma = vma->vm_next)
memory.c: * zap_page_range - remove user pages in a given range
memory.c:	struct mm_struct *mm = vma->vm_mm;
memory.c:	for ( ; vma && vma->vm_start < end; vma = vma->vm_next) {
memory.c: * zap_page_range_single - remove user pages in a given range
memory.c:	struct mm_struct *mm = vma->vm_mm;
memory.c: * zap_vma_ptes - remove ptes mapping the vma
memory.c:	if (address < vma->vm_start || address + size > vma->vm_end ||
memory.c:	    		!(vma->vm_flags & VM_PFNMAP))
memory.c:		return -1;
memory.c:	struct mm_struct *mm = vma->vm_mm;
memory.c:	retval = -EINVAL;
memory.c:	retval = -ENOMEM;
memory.c:	retval = -EBUSY;
memory.c: * vm_insert_page - insert single page into user vma
memory.c: * Usually this function is called from f_op->mmap() handler
memory.c: * under mm->mmap_sem write-lock, so it can change vma->vm_flags.
memory.c: * function from other places, for example from page-fault handler.
memory.c:	if (addr < vma->vm_start || addr >= vma->vm_end)
memory.c:		return -EFAULT;
memory.c:		return -EINVAL;
memory.c:	if (!(vma->vm_flags & VM_MIXEDMAP)) {
memory.c:		BUG_ON(down_read_trylock(&vma->vm_mm->mmap_sem));
memory.c:		BUG_ON(vma->vm_flags & VM_PFNMAP);
memory.c:		vma->vm_flags |= VM_MIXEDMAP;
memory.c:	return insert_page(vma, addr, page, vma->vm_page_prot);
memory.c:	struct mm_struct *mm = vma->vm_mm;
memory.c:	retval = -ENOMEM;
memory.c:	retval = -EBUSY;
memory.c: * vm_insert_pfn - insert single pfn into user vma
memory.c: * This function should only be called from a vm_ops->fault handler, and
memory.c:	return vm_insert_pfn_prot(vma, addr, pfn, vma->vm_page_prot);
memory.c: * vm_insert_pfn_prot - insert single pfn into user vma with specified pgprot
memory.c: * to override pgprot on a per-page basis.
memory.c:	BUG_ON(!(vma->vm_flags & (VM_PFNMAP|VM_MIXEDMAP)));
memory.c:	BUG_ON((vma->vm_flags & (VM_PFNMAP|VM_MIXEDMAP)) ==
memory.c:	BUG_ON((vma->vm_flags & VM_PFNMAP) && is_cow_mapping(vma->vm_flags));
memory.c:	BUG_ON((vma->vm_flags & VM_MIXEDMAP) && pfn_valid(pfn));
memory.c:	if (addr < vma->vm_start || addr >= vma->vm_end)
memory.c:		return -EFAULT;
memory.c:	pgprot_t pgprot = vma->vm_page_prot;
memory.c:	BUG_ON(!(vma->vm_flags & VM_MIXEDMAP));
memory.c:	if (addr < vma->vm_start || addr >= vma->vm_end)
memory.c:		return -EFAULT;
memory.c: * in null mappings (currently treated as "copy-on-access")
memory.c:		return -ENOMEM;
memory.c:	pte_unmap_unlock(pte - 1, ptl);
memory.c:	pfn -= addr >> PAGE_SHIFT;
memory.c:		return -ENOMEM;
memory.c:			return -ENOMEM;
memory.c:	pfn -= addr >> PAGE_SHIFT;
memory.c:		return -ENOMEM;
memory.c:			return -ENOMEM;
memory.c:	pfn -= addr >> PAGE_SHIFT;
memory.c:		return -ENOMEM;
memory.c:			return -ENOMEM;
memory.c: * remap_pfn_range - remap kernel memory to userspace
memory.c:	struct mm_struct *mm = vma->vm_mm;
memory.c:	 * There's a horrible special case to handle copy-on-write
memory.c:	 * un-COW'ed pages by matching them up with "vma->vm_pgoff".
memory.c:	if (is_cow_mapping(vma->vm_flags)) {
memory.c:		if (addr != vma->vm_start || end != vma->vm_end)
memory.c:			return -EINVAL;
memory.c:		vma->vm_pgoff = pfn;
memory.c:		return -EINVAL;
memory.c:	vma->vm_flags |= VM_IO | VM_PFNMAP | VM_DONTEXPAND | VM_DONTDUMP;
memory.c:	pfn -= addr >> PAGE_SHIFT;
memory.c: * vm_iomap_memory - remap memory to userspace
memory.c: * NOTE! Some drivers might want to tweak vma->vm_page_prot first to get
memory.c: * whatever write-combining details or similar.
memory.c:		return -EINVAL;
memory.c:	 * You *really* shouldn't map things that aren't page-aligned,
memory.c:		return -EINVAL;
memory.c:	if (vma->vm_pgoff > pages)
memory.c:		return -EINVAL;
memory.c:	pfn += vma->vm_pgoff;
memory.c:	pages -= vma->vm_pgoff;
memory.c:	vm_len = vma->vm_end - vma->vm_start;
memory.c:		return -EINVAL;
memory.c:	return io_remap_pfn_range(vma, vma->vm_start, pfn, vm_len, vma->vm_page_prot);
memory.c:		return -ENOMEM;
memory.c:		pte_unmap_unlock(pte-1, ptl);
memory.c:		return -ENOMEM;
memory.c:		return -ENOMEM;
memory.c:		return -ENOMEM;
memory.c:		return -EINVAL;
memory.c: * read non-atomically.  Before making any commitment, on those architectures
memory.c:	 * a "struct page" for it. We do a best-effort copy by
memory.c:	 * fails, we just zero-fill it. Live with it.
memory.c:	struct file *vm_file = vma->vm_file;
memory.c:		return mapping_gfp_mask(vm_file->f_mapping) | __GFP_FS | __GFP_IO;
memory.c:	struct page *page = vmf->page;
memory.c:	unsigned int old_flags = vmf->flags;
memory.c:	vmf->flags = FAULT_FLAG_WRITE|FAULT_FLAG_MKWRITE;
memory.c:	ret = vmf->vma->vm_ops->page_mkwrite(vmf);
memory.c:	vmf->flags = old_flags;
memory.c:		if (!page->mapping) {
memory.c:	bool page_mkwrite = vma->vm_ops && vma->vm_ops->page_mkwrite;
memory.c:	 * Take a local copy of the address_space - page.mapping may be zeroed
memory.c:	 * pinned by vma->vm_file's reference.  We rely on unlock_page()'s
memory.c:		file_update_time(vma->vm_file);
memory.c: * any related book-keeping.
memory.c:	__releases(vmf->ptl)
memory.c:	struct vm_area_struct *vma = vmf->vma;
memory.c:	struct page *page = vmf->page;
memory.c:		page_cpupid_xchg_last(page, (1 << LAST_CPUPID_SHIFT) - 1);
memory.c:	flush_cache_page(vma, vmf->address, pte_pfn(vmf->orig_pte));
memory.c:	entry = pte_mkyoung(vmf->orig_pte);
memory.c:	if (ptep_set_access_flags(vma, vmf->address, vmf->pte, entry, 1))
memory.c:		update_mmu_cache(vma, vmf->address, vmf->pte);
memory.c:	pte_unmap_unlock(vmf->pte, vmf->ptl);
memory.c: * - Allocate a page, copy the content of the old page to the new one.
memory.c: * - Handle book keeping and accounting - cgroups, mmu-notifiers, etc.
memory.c: * - Take the PTL. If the pte changed, bail out and release the allocated page
memory.c: * - If the pte is still the way we remember it, update the page table and all
memory.c: *   relevant references. This includes dropping the reference the page-table
memory.c: * - In any case, unlock the PTL and drop the reference we took to the old page.
memory.c:	struct vm_area_struct *vma = vmf->vma;
memory.c:	struct mm_struct *mm = vma->vm_mm;
memory.c:	struct page *old_page = vmf->page;
memory.c:	const unsigned long mmun_start = vmf->address & PAGE_MASK;
memory.c:	if (is_zero_pfn(pte_pfn(vmf->orig_pte))) {
memory.c:							      vmf->address);
memory.c:				vmf->address);
memory.c:		cow_user_page(new_page, old_page, vmf->address, vma);
memory.c:	 * Re-check the pte - we dropped the lock
memory.c:	vmf->pte = pte_offset_map_lock(mm, vmf->pmd, vmf->address, &vmf->ptl);
memory.c:	if (likely(pte_same(*vmf->pte, vmf->orig_pte))) {
memory.c:		flush_cache_page(vma, vmf->address, pte_pfn(vmf->orig_pte));
memory.c:		entry = mk_pte(new_page, vma->vm_page_prot);
memory.c:		ptep_clear_flush_notify(vma, vmf->address, vmf->pte);
memory.c:		page_add_new_anon_rmap(new_page, vma, vmf->address, false);
memory.c:		set_pte_at_notify(mm, vmf->address, vmf->pte, entry);
memory.c:		update_mmu_cache(vma, vmf->address, vmf->pte);
memory.c:	pte_unmap_unlock(vmf->pte, vmf->ptl);
memory.c:		if (page_copied && (vma->vm_flags & VM_LOCKED)) {
memory.c: * finish_mkwrite_fault - finish page fault for a shared mapping, making PTE
memory.c: * shared mapping due to PTE being read-only once the mapped page is prepared.
memory.c:	WARN_ON_ONCE(!(vmf->vma->vm_flags & VM_SHARED));
memory.c:	vmf->pte = pte_offset_map_lock(vmf->vma->vm_mm, vmf->pmd, vmf->address,
memory.c:				       &vmf->ptl);
memory.c:	if (!pte_same(*vmf->pte, vmf->orig_pte)) {
memory.c:		pte_unmap_unlock(vmf->pte, vmf->ptl);
memory.c:	struct vm_area_struct *vma = vmf->vma;
memory.c:	if (vma->vm_ops && vma->vm_ops->pfn_mkwrite) {
memory.c:		pte_unmap_unlock(vmf->pte, vmf->ptl);
memory.c:		vmf->flags |= FAULT_FLAG_MKWRITE;
memory.c:		ret = vma->vm_ops->pfn_mkwrite(vmf);
memory.c:	__releases(vmf->ptl)
memory.c:	struct vm_area_struct *vma = vmf->vma;
memory.c:	get_page(vmf->page);
memory.c:	if (vma->vm_ops && vma->vm_ops->page_mkwrite) {
memory.c:		pte_unmap_unlock(vmf->pte, vmf->ptl);
memory.c:			put_page(vmf->page);
memory.c:			unlock_page(vmf->page);
memory.c:			put_page(vmf->page);
memory.c:		lock_page(vmf->page);
memory.c:	fault_dirty_shared_page(vma, vmf->page);
memory.c:	put_page(vmf->page);
memory.c: * and decrementing the shared-page counter for the old page.
memory.c: * done by the caller (the low-level page fault routine in most cases).
memory.c: * We enter with non-exclusive mmap_sem (to exclude vma changes,
memory.c:	__releases(vmf->ptl)
memory.c:	struct vm_area_struct *vma = vmf->vma;
memory.c:	vmf->page = vm_normal_page(vma, vmf->address, vmf->orig_pte);
memory.c:	if (!vmf->page) {
memory.c:		 * Just mark the pages writable and/or call ops->pfn_mkwrite.
memory.c:		if ((vma->vm_flags & (VM_WRITE|VM_SHARED)) ==
memory.c:		pte_unmap_unlock(vmf->pte, vmf->ptl);
memory.c:	if (PageAnon(vmf->page) && !PageKsm(vmf->page)) {
memory.c:		if (!trylock_page(vmf->page)) {
memory.c:			get_page(vmf->page);
memory.c:			pte_unmap_unlock(vmf->pte, vmf->ptl);
memory.c:			lock_page(vmf->page);
memory.c:			vmf->pte = pte_offset_map_lock(vma->vm_mm, vmf->pmd,
memory.c:					vmf->address, &vmf->ptl);
memory.c:			if (!pte_same(*vmf->pte, vmf->orig_pte)) {
memory.c:				unlock_page(vmf->page);
memory.c:				pte_unmap_unlock(vmf->pte, vmf->ptl);
memory.c:				put_page(vmf->page);
memory.c:			put_page(vmf->page);
memory.c:		if (reuse_swap_page(vmf->page, &total_map_swapcount)) {
memory.c:				page_move_anon_rmap(vmf->page, vma);
memory.c:			unlock_page(vmf->page);
memory.c:		unlock_page(vmf->page);
memory.c:	} else if (unlikely((vma->vm_flags & (VM_WRITE|VM_SHARED)) ==
memory.c:	get_page(vmf->page);
memory.c:	pte_unmap_unlock(vmf->pte, vmf->ptl);
memory.c:	zap_page_range_single(vma, start_addr, end_addr - start_addr, details);
memory.c:			details->first_index, details->last_index) {
memory.c:		vba = vma->vm_pgoff;
memory.c:		vea = vba + vma_pages(vma) - 1;
memory.c:		zba = details->first_index;
memory.c:		zea = details->last_index;
memory.c:			((zba - vba) << PAGE_SHIFT) + vma->vm_start,
memory.c:			((zea - vba + 1) << PAGE_SHIFT) + vma->vm_start,
memory.c: * unmap_mapping_range - unmap the portion of all mmaps in the specified
memory.c:	pgoff_t hlen = (holelen + PAGE_SIZE - 1) >> PAGE_SHIFT;
memory.c:			(holebegin + holelen + PAGE_SIZE - 1) >> PAGE_SHIFT;
memory.c:			hlen = ULONG_MAX - hba + 1;
memory.c:	details.last_index = hba + hlen - 1;
memory.c:	if (unlikely(!RB_EMPTY_ROOT(&mapping->i_mmap.rb_root)))
memory.c:		unmap_mapping_range_tree(&mapping->i_mmap, &details);
memory.c: * We enter with non-exclusive mmap_sem (to exclude vma changes,
memory.c:	struct vm_area_struct *vma = vmf->vma;
memory.c:	if (!pte_unmap_same(vma->vm_mm, vmf->pmd, vmf->pte, vmf->orig_pte)) {
memory.c:	entry = pte_to_swp_entry(vmf->orig_pte);
memory.c:			migration_entry_wait(vma->vm_mm, vmf->pmd,
memory.c:					     vmf->address);
memory.c:			 * For un-addressable device memory we call the pgmap
memory.c:			ret = device_private_entry_fault(vma, vmf->address, entry,
memory.c:						 vmf->flags, vmf->pmd);
memory.c:			print_bad_pte(vma, vmf->address, vmf->orig_pte, NULL);
memory.c:					 vmf->address);
memory.c:				GFP_HIGHUSER_MOVABLE, vma, vmf->address);
memory.c:			vmf->pte = pte_offset_map_lock(vma->vm_mm, vmf->pmd,
memory.c:					vmf->address, &vmf->ptl);
memory.c:			if (likely(pte_same(*vmf->pte, vmf->orig_pte)))
memory.c:		count_memcg_event_mm(vma->vm_mm, PGMAJFAULT);
memory.c:	locked = lock_page_or_retry(page, vma->vm_mm, vmf->flags);
memory.c:	page = ksm_might_need_to_copy(page, vma, vmf->address);
memory.c:	if (mem_cgroup_try_charge(page, vma->vm_mm, GFP_KERNEL,
memory.c:	vmf->pte = pte_offset_map_lock(vma->vm_mm, vmf->pmd, vmf->address,
memory.c:			&vmf->ptl);
memory.c:	if (unlikely(!pte_same(*vmf->pte, vmf->orig_pte)))
memory.c:	inc_mm_counter_fast(vma->vm_mm, MM_ANONPAGES);
memory.c:	dec_mm_counter_fast(vma->vm_mm, MM_SWAPENTS);
memory.c:	pte = mk_pte(page, vma->vm_page_prot);
memory.c:	if ((vmf->flags & FAULT_FLAG_WRITE) && reuse_swap_page(page, NULL)) {
memory.c:		vmf->flags &= ~FAULT_FLAG_WRITE;
memory.c:	if (pte_swp_soft_dirty(vmf->orig_pte))
memory.c:	set_pte_at(vma->vm_mm, vmf->address, vmf->pte, pte);
memory.c:	vmf->orig_pte = pte;
memory.c:		do_page_add_anon_rmap(page, vma, vmf->address, exclusive);
memory.c:		page_add_new_anon_rmap(page, vma, vmf->address, false);
memory.c:	    (vma->vm_flags & VM_LOCKED) || PageMlocked(page))
memory.c:	if (vmf->flags & FAULT_FLAG_WRITE) {
memory.c:	/* No need to invalidate - it was non-present before */
memory.c:	update_mmu_cache(vma, vmf->address, vmf->pte);
memory.c:	pte_unmap_unlock(vmf->pte, vmf->ptl);
memory.c:	pte_unmap_unlock(vmf->pte, vmf->ptl);
memory.c: * We enter with non-exclusive mmap_sem (to exclude vma changes,
memory.c:	struct vm_area_struct *vma = vmf->vma;
memory.c:	/* File mapping without ->vm_ops ? */
memory.c:	if (vma->vm_flags & VM_SHARED)
memory.c:	if (pte_alloc(vma->vm_mm, vmf->pmd, vmf->address))
memory.c:	if (unlikely(pmd_trans_unstable(vmf->pmd)))
memory.c:	/* Use the zero-page for reads */
memory.c:	if (!(vmf->flags & FAULT_FLAG_WRITE) &&
memory.c:			!mm_forbids_zeropage(vma->vm_mm)) {
memory.c:		entry = pte_mkspecial(pfn_pte(my_zero_pfn(vmf->address),
memory.c:						vma->vm_page_prot));
memory.c:		vmf->pte = pte_offset_map_lock(vma->vm_mm, vmf->pmd,
memory.c:				vmf->address, &vmf->ptl);
memory.c:		if (!pte_none(*vmf->pte))
memory.c:		ret = check_stable_address_space(vma->vm_mm);
memory.c:			pte_unmap_unlock(vmf->pte, vmf->ptl);
memory.c:	page = alloc_zeroed_user_highpage_movable(vma, vmf->address);
memory.c:	if (mem_cgroup_try_charge(page, vma->vm_mm, GFP_KERNEL, &memcg, false))
memory.c:	entry = mk_pte(page, vma->vm_page_prot);
memory.c:	if (vma->vm_flags & VM_WRITE)
memory.c:	vmf->pte = pte_offset_map_lock(vma->vm_mm, vmf->pmd, vmf->address,
memory.c:			&vmf->ptl);
memory.c:	if (!pte_none(*vmf->pte))
memory.c:	ret = check_stable_address_space(vma->vm_mm);
memory.c:		pte_unmap_unlock(vmf->pte, vmf->ptl);
memory.c:	inc_mm_counter_fast(vma->vm_mm, MM_ANONPAGES);
memory.c:	page_add_new_anon_rmap(page, vma, vmf->address, false);
memory.c:	set_pte_at(vma->vm_mm, vmf->address, vmf->pte, entry);
memory.c:	/* No need to invalidate - it was non-present before */
memory.c:	update_mmu_cache(vma, vmf->address, vmf->pte);
memory.c:	pte_unmap_unlock(vmf->pte, vmf->ptl);
memory.c: * released depending on flags and vma->vm_ops->fault() return value.
memory.c:	struct vm_area_struct *vma = vmf->vma;
memory.c:	ret = vma->vm_ops->fault(vmf);
memory.c:	if (unlikely(PageHWPoison(vmf->page))) {
memory.c:			unlock_page(vmf->page);
memory.c:		put_page(vmf->page);
memory.c:		vmf->page = NULL;
memory.c:		lock_page(vmf->page);
memory.c:		VM_BUG_ON_PAGE(!PageLocked(vmf->page), vmf->page);
memory.c:	struct vm_area_struct *vma = vmf->vma;
memory.c:	if (!pmd_none(*vmf->pmd))
memory.c:	if (vmf->prealloc_pte) {
memory.c:		vmf->ptl = pmd_lock(vma->vm_mm, vmf->pmd);
memory.c:		if (unlikely(!pmd_none(*vmf->pmd))) {
memory.c:			spin_unlock(vmf->ptl);
memory.c:		atomic_long_inc(&vma->vm_mm->nr_ptes);
memory.c:		pmd_populate(vma->vm_mm, vmf->pmd, vmf->prealloc_pte);
memory.c:		spin_unlock(vmf->ptl);
memory.c:		vmf->prealloc_pte = NULL;
memory.c:	} else if (unlikely(pte_alloc(vma->vm_mm, vmf->pmd, vmf->address))) {
memory.c:	if (pmd_devmap_trans_unstable(vmf->pmd))
memory.c:	 * At this point we know that our vmf->pmd points to a page of ptes
memory.c:	 * we zap the ptes pointed to by our vmf->pmd, the vmf->ptl will still
memory.c:	 * be valid and we will re-check to make sure the vmf->pte isn't
memory.c:	 * pte_none() under vmf->ptl protection when we return to
memory.c:	vmf->pte = pte_offset_map_lock(vma->vm_mm, vmf->pmd, vmf->address,
memory.c:			&vmf->ptl);
memory.c:#define HPAGE_CACHE_INDEX_MASK (HPAGE_PMD_NR - 1)
memory.c:	if (((vma->vm_start >> PAGE_SHIFT) & HPAGE_CACHE_INDEX_MASK) !=
memory.c:			(vma->vm_pgoff & HPAGE_CACHE_INDEX_MASK))
memory.c:	if (haddr < vma->vm_start || haddr + HPAGE_PMD_SIZE > vma->vm_end)
memory.c:	struct vm_area_struct *vma = vmf->vma;
memory.c:	pgtable_trans_huge_deposit(vma->vm_mm, vmf->pmd, vmf->prealloc_pte);
memory.c:	atomic_long_inc(&vma->vm_mm->nr_ptes);
memory.c:	vmf->prealloc_pte = NULL;
memory.c:	struct vm_area_struct *vma = vmf->vma;
memory.c:	bool write = vmf->flags & FAULT_FLAG_WRITE;
memory.c:	unsigned long haddr = vmf->address & HPAGE_PMD_MASK;
memory.c:	if (arch_needs_pgtable_deposit() && !vmf->prealloc_pte) {
memory.c:		vmf->prealloc_pte = pte_alloc_one(vma->vm_mm, vmf->address);
memory.c:		if (!vmf->prealloc_pte)
memory.c:	vmf->ptl = pmd_lock(vma->vm_mm, vmf->pmd);
memory.c:	if (unlikely(!pmd_none(*vmf->pmd)))
memory.c:	entry = mk_huge_pmd(page, vma->vm_page_prot);
memory.c:	add_mm_counter(vma->vm_mm, MM_FILEPAGES, HPAGE_PMD_NR);
memory.c:	set_pmd_at(vma->vm_mm, haddr, vmf->pmd, entry);
memory.c:	update_mmu_cache_pmd(vma, haddr, vmf->pmd);
memory.c:	spin_unlock(vmf->ptl);
memory.c: * alloc_set_pte - setup new PTE entry for given page and add reverse page
memory.c: * mapping. If needed, the fucntion allocates page table or use pre-allocated.
memory.c: * Caller must take care of unlocking vmf->ptl, if vmf->pte is non-NULL on
memory.c: * vm_ops->map_pages.
memory.c:	struct vm_area_struct *vma = vmf->vma;
memory.c:	bool write = vmf->flags & FAULT_FLAG_WRITE;
memory.c:	if (pmd_none(*vmf->pmd) && PageTransCompound(page) &&
memory.c:	if (!vmf->pte) {
memory.c:	/* Re-check under ptl */
memory.c:	if (unlikely(!pte_none(*vmf->pte)))
memory.c:	entry = mk_pte(page, vma->vm_page_prot);
memory.c:	/* copy-on-write page */
memory.c:	if (write && !(vma->vm_flags & VM_SHARED)) {
memory.c:		inc_mm_counter_fast(vma->vm_mm, MM_ANONPAGES);
memory.c:		page_add_new_anon_rmap(page, vma, vmf->address, false);
memory.c:		inc_mm_counter_fast(vma->vm_mm, mm_counter_file(page));
memory.c:	set_pte_at(vma->vm_mm, vmf->address, vmf->pte, entry);
memory.c:	/* no need to invalidate: a not-present page won't be cached */
memory.c:	update_mmu_cache(vma, vmf->address, vmf->pte);
memory.c: * finish_fault - finish page fault once we have prepared the page to fault
memory.c:	if ((vmf->flags & FAULT_FLAG_WRITE) &&
memory.c:	    !(vmf->vma->vm_flags & VM_SHARED))
memory.c:		page = vmf->cow_page;
memory.c:		page = vmf->page;
memory.c:	if (!(vmf->vma->vm_flags & VM_SHARED))
memory.c:		ret = check_stable_address_space(vmf->vma->vm_mm);
memory.c:		ret = alloc_set_pte(vmf, vmf->memcg, page);
memory.c:	if (vmf->pte)
memory.c:		pte_unmap_unlock(vmf->pte, vmf->ptl);
memory.c:		return -EINVAL;
memory.c: * It uses vm_ops->map_pages() to map the pages, which skips the page if it's
memory.c: * not ready to be mapped: not up-to-date, locked, etc.
memory.c:	unsigned long address = vmf->address, nr_pages, mask;
memory.c:	pgoff_t start_pgoff = vmf->pgoff;
memory.c:	mask = ~(nr_pages * PAGE_SIZE - 1) & PAGE_MASK;
memory.c:	vmf->address = max(address & mask, vmf->vma->vm_start);
memory.c:	off = ((address - vmf->address) >> PAGE_SHIFT) & (PTRS_PER_PTE - 1);
memory.c:	start_pgoff -= off;
memory.c:	end_pgoff = start_pgoff -
memory.c:		((vmf->address >> PAGE_SHIFT) & (PTRS_PER_PTE - 1)) +
memory.c:		PTRS_PER_PTE - 1;
memory.c:	end_pgoff = min3(end_pgoff, vma_pages(vmf->vma) + vmf->vma->vm_pgoff - 1,
memory.c:			start_pgoff + nr_pages - 1);
memory.c:	if (pmd_none(*vmf->pmd)) {
memory.c:		vmf->prealloc_pte = pte_alloc_one(vmf->vma->vm_mm,
memory.c:						  vmf->address);
memory.c:		if (!vmf->prealloc_pte)
memory.c:	vmf->vma->vm_ops->map_pages(vmf, start_pgoff, end_pgoff);
memory.c:	if (pmd_trans_huge(*vmf->pmd)) {
memory.c:	/* ->map_pages() haven't done anything useful. Cold page cache? */
memory.c:	if (!vmf->pte)
memory.c:	vmf->pte -= (vmf->address >> PAGE_SHIFT) - (address >> PAGE_SHIFT);
memory.c:	if (!pte_none(*vmf->pte))
memory.c:	pte_unmap_unlock(vmf->pte, vmf->ptl);
memory.c:	vmf->address = address;
memory.c:	vmf->pte = NULL;
memory.c:	struct vm_area_struct *vma = vmf->vma;
memory.c:	 * Let's call ->map_pages() first and use ->fault() as fallback
memory.c:	if (vma->vm_ops->map_pages && fault_around_bytes >> PAGE_SHIFT > 1) {
memory.c:	unlock_page(vmf->page);
memory.c:		put_page(vmf->page);
memory.c:	struct vm_area_struct *vma = vmf->vma;
memory.c:	vmf->cow_page = alloc_page_vma(GFP_HIGHUSER_MOVABLE, vma, vmf->address);
memory.c:	if (!vmf->cow_page)
memory.c:	if (mem_cgroup_try_charge(vmf->cow_page, vma->vm_mm, GFP_KERNEL,
memory.c:				&vmf->memcg, false)) {
memory.c:		put_page(vmf->cow_page);
memory.c:	copy_user_highpage(vmf->cow_page, vmf->page, vmf->address, vma);
memory.c:	__SetPageUptodate(vmf->cow_page);
memory.c:	unlock_page(vmf->page);
memory.c:	put_page(vmf->page);
memory.c:	mem_cgroup_cancel_charge(vmf->cow_page, vmf->memcg, false);
memory.c:	put_page(vmf->cow_page);
memory.c:	struct vm_area_struct *vma = vmf->vma;
memory.c:	if (vma->vm_ops->page_mkwrite) {
memory.c:		unlock_page(vmf->page);
memory.c:			put_page(vmf->page);
memory.c:		unlock_page(vmf->page);
memory.c:		put_page(vmf->page);
memory.c:	fault_dirty_shared_page(vma, vmf->page);
memory.c: * We enter with non-exclusive mmap_sem (to exclude vma changes,
memory.c:	struct vm_area_struct *vma = vmf->vma;
memory.c:	if (!vma->vm_ops->fault)
memory.c:	else if (!(vmf->flags & FAULT_FLAG_WRITE))
memory.c:	else if (!(vma->vm_flags & VM_SHARED))
memory.c:	if (vmf->prealloc_pte) {
memory.c:		pte_free(vma->vm_mm, vmf->prealloc_pte);
memory.c:		vmf->prealloc_pte = NULL;
memory.c:	struct vm_area_struct *vma = vmf->vma;
memory.c:	int page_nid = -1;
memory.c:	bool was_writable = pte_savedwrite(vmf->orig_pte);
memory.c:	vmf->ptl = pte_lockptr(vma->vm_mm, vmf->pmd);
memory.c:	spin_lock(vmf->ptl);
memory.c:	if (unlikely(!pte_same(*vmf->pte, vmf->orig_pte))) {
memory.c:		pte_unmap_unlock(vmf->pte, vmf->ptl);
memory.c:	pte = ptep_modify_prot_start(vma->vm_mm, vmf->address, vmf->pte);
memory.c:	pte = pte_modify(pte, vma->vm_page_prot);
memory.c:	ptep_modify_prot_commit(vma->vm_mm, vmf->address, vmf->pte, pte);
memory.c:	update_mmu_cache(vma, vmf->address, vmf->pte);
memory.c:	page = vm_normal_page(vma, vmf->address, pte);
memory.c:		pte_unmap_unlock(vmf->pte, vmf->ptl);
memory.c:	/* TODO: handle PTE-mapped THP */
memory.c:		pte_unmap_unlock(vmf->pte, vmf->ptl);
memory.c:	if (page_mapcount(page) > 1 && (vma->vm_flags & VM_SHARED))
memory.c:	target_nid = numa_migrate_prep(page, vma, vmf->address, page_nid,
memory.c:	pte_unmap_unlock(vmf->pte, vmf->ptl);
memory.c:	if (target_nid == -1) {
memory.c:	if (page_nid != -1)
memory.c:	if (vma_is_anonymous(vmf->vma))
memory.c:	if (vmf->vma->vm_ops->huge_fault)
memory.c:		return vmf->vma->vm_ops->huge_fault(vmf, PE_SIZE_PMD);
memory.c:	if (vma_is_anonymous(vmf->vma))
memory.c:	if (vmf->vma->vm_ops->huge_fault)
memory.c:		return vmf->vma->vm_ops->huge_fault(vmf, PE_SIZE_PMD);
memory.c:	VM_BUG_ON_VMA(vmf->vma->vm_flags & VM_SHARED, vmf->vma);
memory.c:	__split_huge_pmd(vmf->vma, vmf->pmd, vmf->address, false, NULL);
memory.c:	return vma->vm_flags & (VM_READ | VM_EXEC | VM_WRITE);
memory.c:	if (vma_is_anonymous(vmf->vma))
memory.c:	if (vmf->vma->vm_ops->huge_fault)
memory.c:		return vmf->vma->vm_ops->huge_fault(vmf, PE_SIZE_PUD);
memory.c:	if (vma_is_anonymous(vmf->vma))
memory.c:	if (vmf->vma->vm_ops->huge_fault)
memory.c:		return vmf->vma->vm_ops->huge_fault(vmf, PE_SIZE_PUD);
memory.c: * We enter with non-exclusive mmap_sem (to exclude vma changes, but allow
memory.c:	if (unlikely(pmd_none(*vmf->pmd))) {
memory.c:		 * Leave __pte_alloc() until later: because vm_ops->fault may
memory.c:		vmf->pte = NULL;
memory.c:		if (pmd_devmap_trans_unstable(vmf->pmd))
memory.c:		vmf->pte = pte_offset_map(vmf->pmd, vmf->address);
memory.c:		vmf->orig_pte = *vmf->pte;
memory.c:		 * e.g.ppc44x-defconfig has CONFIG_PTE_64BIT=y and
memory.c:		if (pte_none(vmf->orig_pte)) {
memory.c:			pte_unmap(vmf->pte);
memory.c:			vmf->pte = NULL;
memory.c:	if (!vmf->pte) {
memory.c:		if (vma_is_anonymous(vmf->vma))
memory.c:	if (!pte_present(vmf->orig_pte))
memory.c:	if (pte_protnone(vmf->orig_pte) && vma_is_accessible(vmf->vma))
memory.c:	vmf->ptl = pte_lockptr(vmf->vma->vm_mm, vmf->pmd);
memory.c:	spin_lock(vmf->ptl);
memory.c:	entry = vmf->orig_pte;
memory.c:	if (unlikely(!pte_same(*vmf->pte, entry)))
memory.c:	if (vmf->flags & FAULT_FLAG_WRITE) {
memory.c:	if (ptep_set_access_flags(vmf->vma, vmf->address, vmf->pte, entry,
memory.c:				vmf->flags & FAULT_FLAG_WRITE)) {
memory.c:		update_mmu_cache(vmf->vma, vmf->address, vmf->pte);
memory.c:		if (vmf->flags & FAULT_FLAG_WRITE)
memory.c:			flush_tlb_fix_spurious_fault(vmf->vma, vmf->address);
memory.c:	pte_unmap_unlock(vmf->pte, vmf->ptl);
memory.c:	struct mm_struct *mm = vma->vm_mm;
memory.c:	count_memcg_event_mm(vma->vm_mm, PGFAULT);
memory.c:		ret = hugetlb_fault(vma->vm_mm, vma, address, flags);
memory.c: * We've already handled the fast-path in-line.
memory.c:		return -ENOMEM;
memory.c:	spin_lock(&mm->page_table_lock);
memory.c:	spin_unlock(&mm->page_table_lock);
memory.c: * We've already handled the fast-path in-line.
memory.c:		return -ENOMEM;
memory.c:	spin_lock(&mm->page_table_lock);
memory.c:	spin_unlock(&mm->page_table_lock);
memory.c: * We've already handled the fast-path in-line.
memory.c:		return -ENOMEM;
memory.c:	return -EINVAL;
memory.c: * follow_pfn - look up PFN at a user virtual address
memory.c: * Returns zero and the pfn at @pfn on success, -ve otherwise.
memory.c:	int ret = -EINVAL;
memory.c:	if (!(vma->vm_flags & (VM_IO | VM_PFNMAP)))
memory.c:	ret = follow_pte(vma->vm_mm, address, &ptep, &ptl);
memory.c:	int ret = -EINVAL;
memory.c:	if (!(vma->vm_flags & (VM_IO | VM_PFNMAP)))
memory.c:	if (follow_pte(vma->vm_mm, address, &ptep, &ptl))
memory.c:	int offset = addr & (PAGE_SIZE-1);
memory.c:		return -EINVAL;
memory.c: * Access another process' address space as given in mm.  If non-NULL, use the
memory.c:	down_read(&mm->mmap_sem);
memory.c:			if (!vma || vma->vm_start > addr)
memory.c:			if (vma->vm_ops && vma->vm_ops->access)
memory.c:				ret = vma->vm_ops->access(vma, addr, buf,
memory.c:			offset = addr & (PAGE_SIZE-1);
memory.c:			if (bytes > PAGE_SIZE-offset)
memory.c:				bytes = PAGE_SIZE-offset;
memory.c:		len -= bytes;
memory.c:	up_read(&mm->mmap_sem);
memory.c:	return buf - old_buf;
memory.c: * access_remote_vm - access another process' address space
memory.c:	struct mm_struct *mm = current->mm;
memory.c:	down_read(&mm->mmap_sem);
memory.c:	if (vma && vma->vm_file) {
memory.c:		struct file *f = vma->vm_file;
memory.c:					vma->vm_start,
memory.c:					vma->vm_end - vma->vm_start);
memory.c:	up_read(&mm->mmap_sem);
memory.c:	if (current->mm)
memory.c:		might_lock_read(&current->mm->mmap_sem);
memory.c:		~(((unsigned long)pages_per_huge_page << PAGE_SHIFT) - 1);
memory.c:	/* Clear sub-page to access last to keep its cache lines hot */
memory.c:	n = (addr_hint - addr) / PAGE_SIZE;
memory.c:		/* If sub-page to access in first half of huge page */
memory.c:		/* Clear sub-pages at the end of huge page */
memory.c:		for (i = pages_per_huge_page - 1; i >= 2 * n; i--) {
memory.c:		/* If sub-page to access in second half of huge page */
memory.c:		base = pages_per_huge_page - 2 * (pages_per_huge_page - n);
memory.c:		l = pages_per_huge_page - n;
memory.c:		/* Clear sub-pages at the begin of huge page */
memory.c:	 * Clear remaining sub-pages in left-right-left-right pattern
memory.c:	 * towards the sub-page to access
memory.c:		int right_idx = base + 2 * l - 1 - i;
memory.c:		ret_val -= (PAGE_SIZE - rc);
memory.c:	page_ptl_cachep = kmem_cache_create("page->ptl", sizeof(spinlock_t), 0,
memory.c:	page->ptl = ptl;
memory.c:	kmem_cache_free(page_ptl_cachep, page->ptl);
mlock.c:// SPDX-License-Identifier: GPL-2.0
mlock.c: * in vmscan and, possibly, the fault path; and to support semi-accurate
mlock.c:			    -hpage_nr_pages(page));
mlock.c: * munlock_vma_page - munlock a vma page
mlock.c: * @page - page to be unlocked, either a normal page or THP page head
mlock.c: *         HPAGE_PMD_NR - 1 for THP head page)
mlock.c:		/* Potentially, PTE-mapped THP: do not skip the rest PTEs */
mlock.c:	__mod_zone_page_state(zone, NR_MLOCK, -nr_pages);
mlock.c:	return nr_pages - 1;
mlock.c:	if (retval == -EFAULT)
mlock.c:		retval = -ENOMEM;
mlock.c:	else if (retval == -ENOMEM)
mlock.c:		retval = -EAGAIN;
mlock.c: * Then we can bypass the per-cpu pvec and get better performance.
mlock.c: * Batched putback of evictable pages that bypasses the per-cpu pvec. Some of
mlock.c:	int delta_munlocked = -nr;
mlock.c:		struct page *page = pvec->pages[i];
mlock.c:		pagevec_add(&pvec_putback, pvec->pages[i]);
mlock.c:		pvec->pages[i] = NULL;
mlock.c:		struct page *page = pvec->pages[i];
mlock.c: * a non-TPH page already pinned and in the @pvec, and that it belongs to @zone.
mlock.c:	pte = get_locked_pte(vma->vm_mm, start,	&ptl);
mlock.c:		 * Do not use pagevec for PTE-mapped THP,
mlock.c: * munlock_vma_pages_range() - munlock all pages in the vma range.'
mlock.c: * @vma - vma containing range to be munlock()ed.
mlock.c: * @start - start address in @vma of the range
mlock.c: * @end - end of range in @vma.
mlock.c: * and re-mlocked by try_to_{munlock|unmap} before we unmap and
mlock.c:	vma->vm_flags &= VM_LOCKED_CLEAR_MASK;
mlock.c:				 * Non-huge pages are handled in batches via
mlock.c: * mlock_fixup  - handle mlock[all]/munlock[all] requests.
mlock.c: * Filters out "special" vmas -- VM_LOCKED never gets set for these, and
mlock.c: * munlock is a no-op.  However, for some special vmas, we go ahead and
mlock.c:	struct mm_struct *mm = vma->vm_mm;
mlock.c:	vm_flags_t old_flags = vma->vm_flags;
mlock.c:	if (newflags == vma->vm_flags || (vma->vm_flags & VM_SPECIAL) ||
mlock.c:	    is_vm_hugetlb_page(vma) || vma == get_gate_vma(current->mm))
mlock.c:	pgoff = vma->vm_pgoff + ((start - vma->vm_start) >> PAGE_SHIFT);
mlock.c:	*prev = vma_merge(mm, *prev, start, end, newflags, vma->anon_vma,
mlock.c:			  vma->vm_file, pgoff, vma_policy(vma),
mlock.c:			  vma->vm_userfaultfd_ctx);
mlock.c:	if (start != vma->vm_start) {
mlock.c:	if (end != vma->vm_end) {
mlock.c:	nr_pages = (end - start) >> PAGE_SHIFT;
mlock.c:		nr_pages = -nr_pages;
mlock.c:	mm->locked_vm += nr_pages;
mlock.c:		vma->vm_flags = newflags;
mlock.c:		return -EINVAL;
mlock.c:	vma = find_vma(current->mm, start);
mlock.c:	if (!vma || vma->vm_start > start)
mlock.c:		return -ENOMEM;
mlock.c:	prev = vma->vm_prev;
mlock.c:	if (start > vma->vm_start)
mlock.c:		vm_flags_t newflags = vma->vm_flags & VM_LOCKED_CLEAR_MASK;
mlock.c:		/* Here we know that  vma->vm_start <= nstart < vma->vm_end. */
mlock.c:		tmp = vma->vm_end;
mlock.c:		if (nstart < prev->vm_end)
mlock.c:			nstart = prev->vm_end;
mlock.c:		vma = prev->vm_next;
mlock.c:		if (!vma || vma->vm_start != nstart) {
mlock.c:			error = -ENOMEM;
mlock.c:		mm = current->mm;
mlock.c:		vma = mm->mmap;
mlock.c:	for (; vma ; vma = vma->vm_next) {
mlock.c:		if (start >= vma->vm_end)
mlock.c:		if (start + len <=  vma->vm_start)
mlock.c:		if (vma->vm_flags & VM_LOCKED) {
mlock.c:			if (start > vma->vm_start)
mlock.c:				count -= (start - vma->vm_start);
mlock.c:			if (start + len < vma->vm_end) {
mlock.c:				count += start + len - vma->vm_start;
mlock.c:			count += vma->vm_end - vma->vm_start;
mlock.c:	int error = -ENOMEM;
mlock.c:		return -EPERM;
mlock.c:	if (down_write_killable(&current->mm->mmap_sem))
mlock.c:		return -EINTR;
mlock.c:	locked += current->mm->locked_vm;
mlock.c:		 * previously mlocked areas, that part area in "mm->locked_vm"
mlock.c:		locked -= count_mm_mlocked_page_nr(current->mm,
mlock.c:	up_write(&current->mm->mmap_sem);
mlock.c:		return -EINVAL;
mlock.c:	if (down_write_killable(&current->mm->mmap_sem))
mlock.c:		return -EINTR;
mlock.c:	up_write(&current->mm->mmap_sem);
mlock.c: * and translate into the appropriate modifications to mm->def_flags and/or the
mlock.c: * it, VM_LOCKED and VM_LOCKONFAULT will be cleared from mm->def_flags.
mlock.c:	current->mm->def_flags &= VM_LOCKED_CLEAR_MASK;
mlock.c:		current->mm->def_flags |= VM_LOCKED;
mlock.c:			current->mm->def_flags |= VM_LOCKONFAULT;
mlock.c:	for (vma = current->mm->mmap; vma ; vma = prev->vm_next) {
mlock.c:		newflags = vma->vm_flags & VM_LOCKED_CLEAR_MASK;
mlock.c:		mlock_fixup(vma, &prev, vma->vm_start, vma->vm_end, newflags);
mlock.c:		return -EINVAL;
mlock.c:		return -EPERM;
mlock.c:	if (down_write_killable(&current->mm->mmap_sem))
mlock.c:		return -EINTR;
mlock.c:	ret = -ENOMEM;
mlock.c:	if (!(flags & MCL_CURRENT) || (current->mm->total_vm <= lock_limit) ||
mlock.c:	up_write(&current->mm->mmap_sem);
mlock.c:	if (down_write_killable(&current->mm->mmap_sem))
mlock.c:		return -EINTR;
mlock.c:	up_write(&current->mm->mmap_sem);
mlock.c:	locked = (size + PAGE_SIZE - 1) >> PAGE_SHIFT;
mlock.c:	    locked + user->locked_shm > lock_limit && !capable(CAP_IPC_LOCK))
mlock.c:	user->locked_shm += locked;
mlock.c:	user->locked_shm -= (size + PAGE_SIZE - 1) >> PAGE_SHIFT;
swap_state.c:// SPDX-License-Identifier: GPL-2.0
swap_state.c:#include <linux/backing-dev.h>
swap_state.c:#define SWAP_RA_HITS_MASK	((1UL << SWAP_RA_WIN_SHIFT) - 1)
swap_state.c:	(atomic_long_read(&(vma)->swap_readahead_info) ? : 4)
swap_state.c:		get_nr_swap_pages() << (PAGE_SHIFT - 10));
swap_state.c:	printk("Total swap = %lukB\n", total_swap_pages << (PAGE_SHIFT - 10));
swap_state.c:	spin_lock_irq(&address_space->tree_lock);
swap_state.c:		error = radix_tree_insert(&address_space->page_tree,
swap_state.c:		address_space->nrpages += nr;
swap_state.c:		 * So add_to_swap_cache() doesn't returns -EEXIST.
swap_state.c:		VM_BUG_ON(error == -EEXIST);
swap_state.c:		while (i--) {
swap_state.c:			radix_tree_delete(&address_space->page_tree, idx + i);
swap_state.c:	spin_unlock_irq(&address_space->tree_lock);
swap_state.c:		radix_tree_delete(&address_space->page_tree, idx + i);
swap_state.c:	address_space->nrpages -= nr;
swap_state.c:	__mod_node_page_state(page_pgdat(page), NR_FILE_PAGES, -nr);
swap_state.c: * add_to_swap - allocate swap space for a page
swap_state.c:	 * Radix-tree node allocations from PF_MEMALLOC contexts could
swap_state.c:	/* -ENOMEM radix-tree allocation failure */
swap_state.c:		 * add_to_swap_cache() doesn't return -EEXIST, so we can safely
swap_state.c:	spin_lock_irq(&address_space->tree_lock);
swap_state.c:	spin_unlock_irq(&address_space->tree_lock);
swap_state.c: * 					- Marcelo
swap_state.c: * unlocked and with its refcount incremented - we rely on the kernel
swap_state.c:			atomic_long_set(&vma->swap_readahead_info,
swap_state.c:		 * called after lookup_swap_cache() failed, re-calling
swap_state.c:		if (err == -EEXIST) {
swap_state.c:		/* May fail (-ENOMEM) if radix-tree node allocation failed. */
swap_state.c:		 * add_to_swap_cache() doesn't return -EEXIST, so we can safely
swap_state.c:	} while (err != -ENOMEM);
swap_state.c:		if (offset != prev_offset + 1 && offset != prev_offset - 1)
swap_state.c: * swapin_readahead - swap in pages in hope we need them soon
swap_state.c: * Caller must hold down_read on the vma->vm_mm if vma is not NULL.
swap_state.c:	mask = swapin_nr_pages(offset) - 1;
swap_state.c:		/* Ok, do the async read-ahead now */
swap_state.c:		return -ENOMEM;
swap_state.c:		INIT_RADIX_TREE(&space->page_tree, GFP_ATOMIC|__GFP_NOWARN);
swap_state.c:		atomic_set(&space->i_mmap_writable, 0);
swap_state.c:		space->a_ops = &swap_aops;
swap_state.c:		spin_lock_init(&space->tree_lock);
swap_state.c:	*start = max3(lpfn, PFN_DOWN(vma->vm_start),
swap_state.c:	*end = min3(rpfn, PFN_DOWN(vma->vm_end),
swap_state.c:	struct vm_area_struct *vma = vmf->vma;
swap_state.c:		swap_ra->win = 1;
swap_state.c:	faddr = vmf->address;
swap_state.c:	entry = pte_to_swp_entry(vmf->orig_pte);
swap_state.c:	swap_ra->win = win = __swapin_nr_pages(pfn, fpfn, hits,
swap_state.c:	atomic_long_set(&vma->swap_readahead_info,
swap_state.c:		swap_ra_clamp_pfn(vma, faddr, fpfn - win + 1, fpfn + 1,
swap_state.c:		left = (win - 1) / 2;
swap_state.c:		swap_ra_clamp_pfn(vma, faddr, fpfn - left, fpfn + win - left,
swap_state.c:	swap_ra->nr_pte = end - start;
swap_state.c:	swap_ra->offset = fpfn - start;
swap_state.c:	pte = vmf->pte - swap_ra->offset;
swap_state.c:	swap_ra->ptes = pte;
swap_state.c:	tpte = swap_ra->ptes;
swap_state.c:	struct vm_area_struct *vma = vmf->vma;
swap_state.c:	if (swap_ra->win == 1)
swap_state.c:	for (i = 0, pte = swap_ra->ptes; i < swap_ra->nr_pte;
swap_state.c:					       vmf->address, &page_allocated);
swap_state.c:			if (i != swap_ra->offset &&
swap_state.c:	return read_swap_cache_async(fentry, gfp_mask, vma, vmf->address,
swap_state.c:				     swap_ra->win == 1);
swap_state.c:		return -EINVAL;
swap_state.c:		return -ENOMEM;
usercopy.c: * Copyright (C) 2001-2016 PaX Team, Bradley Spengler, Open Source
usercopy.c: *	GOOD_STACK: fully on the stack (when can't do frame-checking)
usercopy.c:	 * but BUG() actually hooks all the lock-breaking and per-arch
usercopy.c:	/* Reject if NULL or ZERO-allocation. */
usercopy.c:	const void *end = ptr + n - 1;
usercopy.c:			return "<spans Reserved and non-Reserved pages>";
usercopy.c:			return "<spans CMA and non-CMA pages>";
usercopy.c: * - not bogus address
usercopy.c: * - known-safe heap or stack object
usercopy.c: * - not in kernel text
swap_slots.c:// SPDX-License-Identifier: GPL-2.0
swap_slots.c:		return -ENOMEM;
swap_slots.c:		return -ENOMEM;
swap_slots.c:	if (cache->slots || cache->slots_ret)
swap_slots.c:	if (!cache->lock_initialized) {
swap_slots.c:		mutex_init(&cache->alloc_lock);
swap_slots.c:		spin_lock_init(&cache->free_lock);
swap_slots.c:		cache->lock_initialized = true;
swap_slots.c:	cache->nr = 0;
swap_slots.c:	cache->cur = 0;
swap_slots.c:	cache->n_ret = 0;
swap_slots.c:	cache->slots = slots;
swap_slots.c:	cache->slots_ret = slots_ret;
swap_slots.c:	if ((type & SLOTS_CACHE) && cache->slots) {
swap_slots.c:		mutex_lock(&cache->alloc_lock);
swap_slots.c:		swapcache_free_entries(cache->slots + cache->cur, cache->nr);
swap_slots.c:		cache->cur = 0;
swap_slots.c:		cache->nr = 0;
swap_slots.c:		if (free_slots && cache->slots) {
swap_slots.c:			kvfree(cache->slots);
swap_slots.c:			cache->slots = NULL;
swap_slots.c:		mutex_unlock(&cache->alloc_lock);
swap_slots.c:	if ((type & SLOTS_CACHE_RET) && cache->slots_ret) {
swap_slots.c:		spin_lock_irq(&cache->free_lock);
swap_slots.c:		swapcache_free_entries(cache->slots_ret, cache->n_ret);
swap_slots.c:		cache->n_ret = 0;
swap_slots.c:		if (free_slots && cache->slots_ret) {
swap_slots.c:			slots = cache->slots_ret;
swap_slots.c:			cache->slots_ret = NULL;
swap_slots.c:		spin_unlock_irq(&cache->free_lock);
swap_slots.c:	 * cpu_up -> lock cpu_hotplug -> cpu hotplug state callback
swap_slots.c:	 *   -> memory allocation -> direct reclaim -> get_swap_page
swap_slots.c:	 *   -> drain_swap_slots_cache
swap_slots.c:	if (!use_swap_slot_cache || cache->nr)
swap_slots.c:	cache->cur = 0;
swap_slots.c:		cache->nr = get_swap_pages(SWAP_SLOTS_CACHE_SIZE, false,
swap_slots.c:					   cache->slots);
swap_slots.c:	return cache->nr;
swap_slots.c:	if (use_swap_slot_cache && cache->slots_ret) {
swap_slots.c:		spin_lock_irq(&cache->free_lock);
swap_slots.c:		if (!use_swap_slot_cache || !cache->slots_ret) {
swap_slots.c:			spin_unlock_irq(&cache->free_lock);
swap_slots.c:		if (cache->n_ret >= SWAP_SLOTS_CACHE_SIZE) {
swap_slots.c:			swapcache_free_entries(cache->slots_ret, cache->n_ret);
swap_slots.c:			cache->n_ret = 0;
swap_slots.c:		cache->slots_ret[cache->n_ret++] = entry;
swap_slots.c:		spin_unlock_irq(&cache->free_lock);
swap_slots.c:	 * accesses to the per-CPU data structure are protected by the
swap_slots.c:	 * mutex cache->alloc_lock.
swap_slots.c:	 * The alloc path here does not touch cache->slots_ret
swap_slots.c:	 * so cache->free_lock is not taken.
swap_slots.c:		mutex_lock(&cache->alloc_lock);
swap_slots.c:		if (cache->slots) {
swap_slots.c:			if (cache->nr) {
swap_slots.c:				pentry = &cache->slots[cache->cur++];
swap_slots.c:				pentry->val = 0;
swap_slots.c:				cache->nr--;
swap_slots.c:		mutex_unlock(&cache->alloc_lock);
rodata_test.c:	if (start & (PAGE_SIZE - 1)) {
rodata_test.c:	if (end & (PAGE_SIZE - 1)) {
mmu_context.c:	active_mm = tsk->active_mm;
mmu_context.c:		tsk->active_mm = mm;
mmu_context.c:	tsk->mm = mm;
mmu_context.c:	tsk->mm = NULL;
cma_debug.c:// SPDX-License-Identifier: GPL-2.0
cma_debug.c:	mutex_lock(&cma->lock);
cma_debug.c:	used = bitmap_weight(cma->bitmap, (int)cma_bitmap_maxno(cma));
cma_debug.c:	mutex_unlock(&cma->lock);
cma_debug.c:	*val = (u64)used << cma->order_per_bit;
cma_debug.c:	mutex_lock(&cma->lock);
cma_debug.c:		start = find_next_zero_bit(cma->bitmap, bitmap_maxno, end);
cma_debug.c:		if (start >= cma->count)
cma_debug.c:		end = find_next_bit(cma->bitmap, bitmap_maxno, start);
cma_debug.c:		maxchunk = max(end - start, maxchunk);
cma_debug.c:	mutex_unlock(&cma->lock);
cma_debug.c:	*val = (u64)maxchunk << cma->order_per_bit;
cma_debug.c:	spin_lock(&cma->mem_head_lock);
cma_debug.c:	hlist_add_head(&mem->node, &cma->mem_head);
cma_debug.c:	spin_unlock(&cma->mem_head_lock);
cma_debug.c:	spin_lock(&cma->mem_head_lock);
cma_debug.c:	if (!hlist_empty(&cma->mem_head)) {
cma_debug.c:		mem = hlist_entry(cma->mem_head.first, struct cma_mem, node);
cma_debug.c:		hlist_del_init(&mem->node);
cma_debug.c:	spin_unlock(&cma->mem_head_lock);
cma_debug.c:		if (mem->n <= count) {
cma_debug.c:			cma_release(cma, mem->p, mem->n);
cma_debug.c:			count -= mem->n;
cma_debug.c:		} else if (cma->order_per_bit == 0) {
cma_debug.c:			cma_release(cma, mem->p, count);
cma_debug.c:			mem->p += count;
cma_debug.c:			mem->n -= count;
cma_debug.c:		return -ENOMEM;
cma_debug.c:		return -ENOMEM;
cma_debug.c:	mem->p = p;
cma_debug.c:	mem->n = count;
cma_debug.c:	scnprintf(name, sizeof(name), "cma-%s", cma->name);
cma_debug.c:				&cma->base_pfn, &cma_debugfs_fops);
cma_debug.c:				&cma->count, &cma_debugfs_fops);
cma_debug.c:				&cma->order_per_bit, &cma_debugfs_fops);
cma_debug.c:	debugfs_create_u32_array("bitmap", S_IRUGO, tmp, (u32*)cma->bitmap, u32s);
cma_debug.c:		return -ENOMEM;
compaction.c:// SPDX-License-Identifier: GPL-2.0
compaction.c: * Copyright IBM Corp. 2007-2010 Mel Gorman <mel@csn.ul.ie>
compaction.c:#include <linux/backing-dev.h>
compaction.c:#include <linux/page-isolation.h>
compaction.c:		list_del(&page->lru);
compaction.c:		list_del(&page->lru);
compaction.c:			list_add(&page->lru, &tmp_list);
compaction.c:	if (mapping && mapping->a_ops && mapping->a_ops->isolate_page)
compaction.c:	page->mapping = (void *)((unsigned long)mapping | PAGE_MAPPING_MOVABLE);
compaction.c:	page->mapping = (void *)((unsigned long)page->mapping &
compaction.c:	zone->compact_considered = 0;
compaction.c:	zone->compact_defer_shift++;
compaction.c:	if (order < zone->compact_order_failed)
compaction.c:		zone->compact_order_failed = order;
compaction.c:	if (zone->compact_defer_shift > COMPACT_MAX_DEFER_SHIFT)
compaction.c:		zone->compact_defer_shift = COMPACT_MAX_DEFER_SHIFT;
compaction.c:	unsigned long defer_limit = 1UL << zone->compact_defer_shift;
compaction.c:	if (order < zone->compact_order_failed)
compaction.c:	if (++zone->compact_considered > defer_limit)
compaction.c:		zone->compact_considered = defer_limit;
compaction.c:	if (zone->compact_considered >= defer_limit)
compaction.c:		zone->compact_considered = 0;
compaction.c:		zone->compact_defer_shift = 0;
compaction.c:	if (order >= zone->compact_order_failed)
compaction.c:		zone->compact_order_failed = order + 1;
compaction.c:	if (order < zone->compact_order_failed)
compaction.c:	return zone->compact_defer_shift == COMPACT_MAX_DEFER_SHIFT &&
compaction.c:		zone->compact_considered >= 1UL << zone->compact_defer_shift;
compaction.c:	if (cc->ignore_skip_hint)
compaction.c:	zone->compact_cached_migrate_pfn[0] = zone->zone_start_pfn;
compaction.c:	zone->compact_cached_migrate_pfn[1] = zone->zone_start_pfn;
compaction.c:	zone->compact_cached_free_pfn =
compaction.c:				pageblock_start_pfn(zone_end_pfn(zone) - 1);
compaction.c:	unsigned long start_pfn = zone->zone_start_pfn;
compaction.c:	zone->compact_blockskip_flush = false;
compaction.c:		struct zone *zone = &pgdat->node_zones[zoneid];
compaction.c:		if (zone->compact_blockskip_flush)
compaction.c:	struct zone *zone = cc->zone;
compaction.c:	if (cc->ignore_skip_hint)
compaction.c:		if (pfn > zone->compact_cached_migrate_pfn[0])
compaction.c:			zone->compact_cached_migrate_pfn[0] = pfn;
compaction.c:		if (cc->mode != MIGRATE_ASYNC &&
compaction.c:		    pfn > zone->compact_cached_migrate_pfn[1])
compaction.c:			zone->compact_cached_migrate_pfn[1] = pfn;
compaction.c:		if (pfn < zone->compact_cached_free_pfn)
compaction.c:			zone->compact_cached_free_pfn = pfn;
compaction.c:	if (cc->mode == MIGRATE_ASYNC) {
compaction.c:			cc->contended = true;
compaction.c:		cc->contended = true;
compaction.c:		if (cc->mode == MIGRATE_ASYNC) {
compaction.c:			cc->contended = true;
compaction.c:		if (cc->mode == MIGRATE_ASYNC) {
compaction.c:			cc->contended = true;
compaction.c: * returning 0 on any invalid PFNs or non-free pages inside of the pageblock
compaction.c:		    && compact_unlock_should_abort(&cc->zone->lock, flags,
compaction.c:				blockpfn += (1UL << comp_order) - 1;
compaction.c:				cursor += (1UL << comp_order) - 1;
compaction.c:			locked = compact_trylock_irqsave(&cc->zone->lock,
compaction.c:		/* Found a free page, will break it into order-0 pages */
compaction.c:		cc->nr_freepages += isolated;
compaction.c:		list_add_tail(&page->lru, freelist);
compaction.c:		if (!strict && cc->nr_migratepages <= cc->nr_freepages) {
compaction.c:		blockpfn += isolated - 1;
compaction.c:		cursor += isolated - 1;
compaction.c:		spin_unlock_irqrestore(&cc->zone->lock, flags);
compaction.c:	/* Update the pageblock-skip if the whole pageblock was scanned */
compaction.c:	cc->total_free_scanned += nr_scanned;
compaction.c: * isolate_freepages_range() - isolate free pages.
compaction.c: * @end_pfn:   The one-past-last PFN.
compaction.c: * Non-free pages, invalid PFNs, or zone boundaries within the
compaction.c: * Otherwise, function returns one-past-the-last PFN of isolated page
compaction.c:	if (block_start_pfn < cc->zone->zone_start_pfn)
compaction.c:		block_start_pfn = cc->zone->zone_start_pfn;
compaction.c:					block_end_pfn, cc->zone))
compaction.c:		 * non-free pages).
compaction.c:		 * pageblock_nr_pages for some non-negative n.  (Max order
compaction.c:	inactive = node_page_state(zone->zone_pgdat, NR_INACTIVE_FILE) +
compaction.c:			node_page_state(zone->zone_pgdat, NR_INACTIVE_ANON);
compaction.c:	active = node_page_state(zone->zone_pgdat, NR_ACTIVE_FILE) +
compaction.c:			node_page_state(zone->zone_pgdat, NR_ACTIVE_ANON);
compaction.c:	isolated = node_page_state(zone->zone_pgdat, NR_ISOLATED_FILE) +
compaction.c:			node_page_state(zone->zone_pgdat, NR_ISOLATED_ANON);
compaction.c: * isolate_migratepages_block() - isolate all migrate-able pages within
compaction.c: * @end_pfn:	The one-past-the-last PFN to isolate, within same pageblock
compaction.c: * The pages are isolated on cc->migratepages list (not required to be empty),
compaction.c: * and cc->nr_migratepages is updated accordingly. The cc->migrate_pfn field
compaction.c:	struct zone *zone = cc->zone;
compaction.c:		if (cc->mode == MIGRATE_ASYNC)
compaction.c:	if (cc->direct_compaction && (cc->mode == MIGRATE_ASYNC)) {
compaction.c:		next_skip_pfn = block_end_pfn(low_pfn, cc->order);
compaction.c:			 * previous order-aligned block, and did not skip it due
compaction.c:			 * We failed to isolate in the previous order-aligned
compaction.c:			 * a compound or a high-order buddy page in the
compaction.c:			next_skip_pfn = block_end_pfn(low_pfn, cc->order);
compaction.c:				low_pfn += (1UL << freepage_order) - 1;
compaction.c:				low_pfn += (1UL << comp_order) - 1;
compaction.c:		 * It's possible to migrate LRU and non-lru movable pages.
compaction.c:		if (!(cc->gfp_mask & __GFP_FS) && page_mapping(page))
compaction.c:			 * Page become compound since the non-locked check,
compaction.c:				low_pfn += (1UL << compound_order(page)) - 1;
compaction.c:		lruvec = mem_cgroup_page_lruvec(page, zone->zone_pgdat);
compaction.c:		list_add(&page->lru, &cc->migratepages);
compaction.c:		cc->nr_migratepages++;
compaction.c:		 * - this is the lowest page that was isolated and likely be
compaction.c:		if (!cc->last_migrated_pfn)
compaction.c:			cc->last_migrated_pfn = low_pfn;
compaction.c:		if (cc->nr_migratepages == COMPACT_CLUSTER_MAX) {
compaction.c:		 * instead of migrating, as we cannot form the cc->order buddy
compaction.c:			putback_movable_pages(&cc->migratepages);
compaction.c:			cc->nr_migratepages = 0;
compaction.c:			cc->last_migrated_pfn = 0;
compaction.c:			low_pfn = next_skip_pfn - 1;
compaction.c:			next_skip_pfn += 1UL << cc->order;
compaction.c:	 * Update the pageblock-skip information and cached scanner pfn,
compaction.c:	cc->total_migrate_scanned += nr_scanned;
compaction.c: * isolate_migratepages_range() - isolate migrate-able pages in a PFN range
compaction.c: * @end_pfn:   The one-past-last PFN.
compaction.c: * Otherwise, function returns one-past-the-last PFN of isolated page
compaction.c:	if (block_start_pfn < cc->zone->zone_start_pfn)
compaction.c:		block_start_pfn = cc->zone->zone_start_pfn;
compaction.c:					block_end_pfn, cc->zone))
compaction.c:		if (cc->nr_migratepages == COMPACT_CLUSTER_MAX)
compaction.c:	if ((cc->mode != MIGRATE_ASYNC) || !cc->direct_compaction)
compaction.c:	if (cc->migratetype == MIGRATE_MOVABLE)
compaction.c:		return block_mt == cc->migratetype;
compaction.c:		 * We are checking page_order without zone->lock taken. But
compaction.c:	if (cc->ignore_block_suitable)
compaction.c:	return (cc->free_pfn >> pageblock_order)
compaction.c:		<= (cc->migrate_pfn >> pageblock_order);
compaction.c:	struct zone *zone = cc->zone;
compaction.c:	struct list_head *freelist = &cc->freepages;
compaction.c:	 * successfully isolated from, zone-cached value, or the end of the
compaction.c:	 * block_start_pfn -= pageblock_nr_pages in the for loop.
compaction.c:	isolate_start_pfn = cc->free_pfn;
compaction.c:	block_start_pfn = pageblock_start_pfn(cc->free_pfn);
compaction.c:	low_pfn = pageblock_end_pfn(cc->migrate_pfn);
compaction.c:	 * pages on cc->migratepages. We stop searching if the migrate
compaction.c:				block_start_pfn -= pageblock_nr_pages,
compaction.c:		if ((cc->nr_freepages >= cc->nr_migratepages)
compaction.c:							|| cc->contended) {
compaction.c:					block_start_pfn - pageblock_nr_pages;
compaction.c:	cc->free_pfn = isolate_start_pfn;
compaction.c: * This is a migrate-callback that "allocates" freepages by taking pages
compaction.c:	if (list_empty(&cc->freepages)) {
compaction.c:		if (!cc->contended)
compaction.c:		if (list_empty(&cc->freepages))
compaction.c:	freepage = list_entry(cc->freepages.next, struct page, lru);
compaction.c:	list_del(&freepage->lru);
compaction.c:	cc->nr_freepages--;
compaction.c: * This is a migrate-callback that "frees" freepages back to the isolated
compaction.c:	list_add(&page->lru, &cc->freepages);
compaction.c:	cc->nr_freepages++;
compaction.c:		(cc->mode != MIGRATE_SYNC ? ISOLATE_ASYNC_MIGRATE : 0);
compaction.c:	low_pfn = cc->migrate_pfn;
compaction.c:	if (block_start_pfn < zone->zone_start_pfn)
compaction.c:		block_start_pfn = zone->zone_start_pfn;
compaction.c:	for (; block_end_pfn <= cc->free_pfn;
compaction.c:		if (!low_pfn || cc->contended)
compaction.c:	cc->migrate_pfn = low_pfn;
compaction.c:	return cc->nr_migratepages ? ISOLATE_SUCCESS : ISOLATE_NONE;
compaction.c: * order == -1 is expected when compacting via
compaction.c:	return order == -1;
compaction.c:	const int migratetype = cc->migratetype;
compaction.c:	if (cc->contended || fatal_signal_pending(current))
compaction.c:		if (cc->direct_compaction)
compaction.c:			zone->compact_blockskip_flush = true;
compaction.c:		if (cc->whole_zone)
compaction.c:	if (is_via_compact_memory(cc->order))
compaction.c:	if (cc->finishing_block) {
compaction.c:		if (IS_ALIGNED(cc->migrate_pfn, pageblock_nr_pages))
compaction.c:			cc->finishing_block = false;
compaction.c:	for (order = cc->order; order < MAX_ORDER; order++) {
compaction.c:		struct free_area *area = &zone->free_area[order];
compaction.c:		if (!list_empty(&area->free_list[migratetype]))
compaction.c:			!list_empty(&area->free_list[MIGRATE_CMA]))
compaction.c:						true, &can_steal) != -1) {
compaction.c:			 * We are stealing for a non-movable allocation. Make
compaction.c:			if (cc->mode == MIGRATE_ASYNC ||
compaction.c:					IS_ALIGNED(cc->migrate_pfn,
compaction.c:			cc->finishing_block = true;
compaction.c:	trace_mm_compaction_finished(zone, cc->order, ret);
compaction.c: *   COMPACT_SKIPPED  - If there are too few free pages for compaction
compaction.c: *   COMPACT_SUCCESS  - If the allocation would succeed without compaction
compaction.c: *   COMPACT_CONTINUE - If compaction should run now
compaction.c:	watermark = zone->watermark[alloc_flags & ALLOC_WMARK_MASK];
compaction.c:	 * If watermarks for high-order allocation are already met, there
compaction.c:	 * Watermarks for order-0 must be met for compaction to be able to
compaction.c:	 * index of -1000 would imply allocations might succeed depending on
compaction.c:	 * watermarks, but we already failed the high-order watermark check
compaction.c:	 * ignore fragindex for non-costly orders where the alternative to
compaction.c:	for_each_zone_zonelist_nodemask(zone, z, ac->zonelist, ac->high_zoneidx,
compaction.c:					ac->nodemask) {
compaction.c:	unsigned long start_pfn = zone->zone_start_pfn;
compaction.c:	const bool sync = cc->mode != MIGRATE_ASYNC;
compaction.c:	cc->migratetype = gfpflags_to_migratetype(cc->gfp_mask);
compaction.c:	ret = compaction_suitable(zone, cc->order, cc->alloc_flags,
compaction.c:							cc->classzone_idx);
compaction.c:	if (compaction_restarting(zone, cc->order))
compaction.c:	if (cc->whole_zone) {
compaction.c:		cc->migrate_pfn = start_pfn;
compaction.c:		cc->free_pfn = pageblock_start_pfn(end_pfn - 1);
compaction.c:		cc->migrate_pfn = zone->compact_cached_migrate_pfn[sync];
compaction.c:		cc->free_pfn = zone->compact_cached_free_pfn;
compaction.c:		if (cc->free_pfn < start_pfn || cc->free_pfn >= end_pfn) {
compaction.c:			cc->free_pfn = pageblock_start_pfn(end_pfn - 1);
compaction.c:			zone->compact_cached_free_pfn = cc->free_pfn;
compaction.c:		if (cc->migrate_pfn < start_pfn || cc->migrate_pfn >= end_pfn) {
compaction.c:			cc->migrate_pfn = start_pfn;
compaction.c:			zone->compact_cached_migrate_pfn[0] = cc->migrate_pfn;
compaction.c:			zone->compact_cached_migrate_pfn[1] = cc->migrate_pfn;
compaction.c:		if (cc->migrate_pfn == start_pfn)
compaction.c:			cc->whole_zone = true;
compaction.c:	cc->last_migrated_pfn = 0;
compaction.c:	trace_mm_compaction_begin(start_pfn, cc->migrate_pfn,
compaction.c:				cc->free_pfn, end_pfn, sync);
compaction.c:			putback_movable_pages(&cc->migratepages);
compaction.c:			cc->nr_migratepages = 0;
compaction.c:			 * previous cc->order aligned block.
compaction.c:		err = migrate_pages(&cc->migratepages, compaction_alloc,
compaction.c:				compaction_free, (unsigned long)cc, cc->mode,
compaction.c:		trace_mm_compaction_migratepages(cc->nr_migratepages, err,
compaction.c:							&cc->migratepages);
compaction.c:		cc->nr_migratepages = 0;
compaction.c:			putback_movable_pages(&cc->migratepages);
compaction.c:			 * migrate_pages() may return -ENOMEM when scanners meet
compaction.c:			if (err == -ENOMEM && !compact_scanners_met(cc)) {
compaction.c:			 * order-aligned block, so skip the rest of it.
compaction.c:			if (cc->direct_compaction &&
compaction.c:						(cc->mode == MIGRATE_ASYNC)) {
compaction.c:				cc->migrate_pfn = block_end_pfn(
compaction.c:						cc->migrate_pfn - 1, cc->order);
compaction.c:				cc->last_migrated_pfn = 0;
compaction.c:		 * cc->order aligned block where we migrated from? If yes,
compaction.c:		if (cc->order > 0 && cc->last_migrated_pfn) {
compaction.c:				block_start_pfn(cc->migrate_pfn, cc->order);
compaction.c:			if (cc->last_migrated_pfn < current_block_start) {
compaction.c:				cc->last_migrated_pfn = 0;
compaction.c:	if (cc->nr_freepages > 0) {
compaction.c:		unsigned long free_pfn = release_freepages(&cc->freepages);
compaction.c:		cc->nr_freepages = 0;
compaction.c:		if (free_pfn > zone->compact_cached_free_pfn)
compaction.c:			zone->compact_cached_free_pfn = free_pfn;
compaction.c:	count_compact_events(COMPACTMIGRATE_SCANNED, cc->total_migrate_scanned);
compaction.c:	count_compact_events(COMPACTFREE_SCANNED, cc->total_free_scanned);
compaction.c:	trace_mm_compaction_end(start_pfn, cc->migrate_pfn,
compaction.c:				cc->free_pfn, end_pfn, sync, ret);
compaction.c: * try_to_compact_pages - Direct compact to satisfy a high-order allocation
compaction.c:	 * Check if the GFP flags allow compaction - GFP_NOIO is really
compaction.c:	for_each_zone_zonelist_nodemask(zone, z, ac->zonelist, ac->high_zoneidx,
compaction.c:								ac->nodemask) {
compaction.c:		.order = -1,
compaction.c:		zone = &pgdat->node_zones[zoneid];
compaction.c:	int nid = dev->id;
compaction.c:	return device_create_file(&node->dev, &dev_attr_compact);
compaction.c:	return device_remove_file(&node->dev, &dev_attr_compact);
compaction.c:	return pgdat->kcompactd_max_order > 0 || kthread_should_stop();
compaction.c:	enum zone_type classzone_idx = pgdat->kcompactd_classzone_idx;
compaction.c:		zone = &pgdat->node_zones[zoneid];
compaction.c:		if (compaction_suitable(zone, pgdat->kcompactd_max_order, 0,
compaction.c:		.order = pgdat->kcompactd_max_order,
compaction.c:		.classzone_idx = pgdat->kcompactd_classzone_idx,
compaction.c:	trace_mm_compaction_kcompactd_wake(pgdat->node_id, cc.order,
compaction.c:		zone = &pgdat->node_zones[zoneid];
compaction.c:	if (pgdat->kcompactd_max_order <= cc.order)
compaction.c:		pgdat->kcompactd_max_order = 0;
compaction.c:	if (pgdat->kcompactd_classzone_idx >= cc.classzone_idx)
compaction.c:		pgdat->kcompactd_classzone_idx = pgdat->nr_zones - 1;
compaction.c:	if (pgdat->kcompactd_max_order < order)
compaction.c:		pgdat->kcompactd_max_order = order;
compaction.c:	if (pgdat->kcompactd_classzone_idx > classzone_idx)
compaction.c:		pgdat->kcompactd_classzone_idx = classzone_idx;
compaction.c:	if (!wq_has_sleeper(&pgdat->kcompactd_wait))
compaction.c:	trace_mm_compaction_wakeup_kcompactd(pgdat->node_id, order,
compaction.c:	wake_up_interruptible(&pgdat->kcompactd_wait);
compaction.c:	const struct cpumask *cpumask = cpumask_of_node(pgdat->node_id);
compaction.c:	pgdat->kcompactd_max_order = 0;
compaction.c:	pgdat->kcompactd_classzone_idx = pgdat->nr_zones - 1;
compaction.c:		trace_mm_compaction_kcompactd_sleep(pgdat->node_id);
compaction.c:		wait_event_freezable(pgdat->kcompactd_wait,
compaction.c: * This kcompactd start function will be called by init and node-hot-add.
compaction.c: * On node-hot-add, kcompactd will moved to proper cpus if cpus are hot-added.
compaction.c:	if (pgdat->kcompactd)
compaction.c:	pgdat->kcompactd = kthread_run(kcompactd, pgdat, "kcompactd%d", nid);
compaction.c:	if (IS_ERR(pgdat->kcompactd)) {
compaction.c:		ret = PTR_ERR(pgdat->kcompactd);
compaction.c:		pgdat->kcompactd = NULL;
compaction.c:	struct task_struct *kcompactd = NODE_DATA(nid)->kcompactd;
compaction.c:		NODE_DATA(nid)->kcompactd = NULL;
compaction.c:		mask = cpumask_of_node(pgdat->node_id);
compaction.c:			set_cpus_allowed_ptr(pgdat->kcompactd, mask);
sparse.c:// SPDX-License-Identifier: GPL-2.0
sparse.c: * 1) mem_section	- memory sections, mem_map's for valid memory
sparse.c:		return -EEXIST;
sparse.c:		return -ENOMEM;
sparse.c:	return (root_nr * SECTIONS_PER_ROOT) + (ms - root);
sparse.c:	return (int)(ms - mem_section[0]);
sparse.c:	return (section->section_mem_map >> SECTION_NID_SHIFT);
sparse.c:	unsigned long max_sparsemem_pfn = 1UL << (MAX_PHYSMEM_BITS-PAGE_SHIFT);
sparse.c:	 * Sanity checks - do not allow an architecture to pass
sparse.c:			"Start of range %lu -> %lu exceeds SPARSEMEM max %lu\n",
sparse.c:			"End of range %lu -> %lu exceeds SPARSEMEM max %lu\n",
sparse.c:	ms->section_mem_map |= SECTION_MARKED_PRESENT;
sparse.c:	return -1;
sparse.c:	for (section_nr = next_present_section_nr(start-1);	\
sparse.c:		if (!ms->section_mem_map) {
sparse.c:			ms->section_mem_map = sparse_encode_early_nid(nid) |
sparse.c: * the identity pfn - section_mem_map will return the actual
sparse.c:	return (unsigned long)(mem_map - (section_nr_to_pfn(pnum)));
sparse.c:		return -EINVAL;
sparse.c:	ms->section_mem_map &= ~SECTION_MAP_MASK;
sparse.c:	ms->section_mem_map |= sparse_encode_mem_map(mem_map, pnum) |
sparse.c: 	ms->pageblock_flags = pageblock_bitmap;
sparse.c:	 * sections become inter-dependent. This allocates usemaps
sparse.c:	 * Some platforms allow un-removable section because they will just
sparse.c:	 * Just notify un-removable section's number here.
sparse.c:	return memblock_virt_alloc_node_nopanic(size, pgdat->node_id);
sparse.c:		ms->section_mem_map = 0;
sparse.c:	ms->section_mem_map = 0;
sparse.c: *  alloc_usemap_and_memmap - memory alloction for pageblock flags and vmemmap
sparse.c:		/* ok, we need to take cake of from pnum_begin to pnum - 1*/
sparse.c: * Allocate the accumulated non-linear sections, allocate a mem_map
sparse.c:		ms->section_mem_map |= SECTION_IS_ONLINE;
sparse.c:		ms->section_mem_map &= ~SECTION_IS_ONLINE;
sparse.c:		magic = (unsigned long) page->freelist;
sparse.c: * set.  If this is <=0, then that means that the passed-in
sparse.c:	ret = sparse_index_init(section_nr, pgdat->node_id);
sparse.c:	if (ret < 0 && ret != -EEXIST)
sparse.c:	memmap = kmalloc_section_memmap(section_nr, pgdat->node_id);
sparse.c:		return -ENOMEM;
sparse.c:		return -ENOMEM;
sparse.c:	if (ms->section_mem_map & SECTION_MARKED_PRESENT) {
sparse.c:		ret = -EEXIST;
sparse.c:	 * Check to see if allocation came from hot-plug-add
sparse.c:	struct pglist_data *pgdat = zone->zone_pgdat;
sparse.c:	if (ms->section_mem_map) {
sparse.c:		usemap = ms->pageblock_flags;
sparse.c:		memmap = sparse_decode_mem_map(ms->section_mem_map,
sparse.c:		ms->section_mem_map = 0;
sparse.c:		ms->pageblock_flags = NULL;
sparse.c:			PAGES_PER_SECTION - map_offset);
page_counter.c:// SPDX-License-Identifier: GPL-2.0
page_counter.c: * page_counter_cancel - take pages out of the local counter
page_counter.c:	new = atomic_long_sub_return(nr_pages, &counter->count);
page_counter.c: * page_counter_charge - hierarchically charge pages
page_counter.c:	for (c = counter; c; c = c->parent) {
page_counter.c:		new = atomic_long_add_return(nr_pages, &c->count);
page_counter.c:		if (new > c->watermark)
page_counter.c:			c->watermark = new;
page_counter.c: * page_counter_try_charge - try to hierarchically charge pages
page_counter.c:	for (c = counter; c; c = c->parent) {
page_counter.c:		new = atomic_long_add_return(nr_pages, &c->count);
page_counter.c:		if (new > c->limit) {
page_counter.c:			atomic_long_sub(nr_pages, &c->count);
page_counter.c:			c->failcnt++;
page_counter.c:		if (new > c->watermark)
page_counter.c:			c->watermark = new;
page_counter.c:	for (c = counter; c != *fail; c = c->parent)
page_counter.c: * page_counter_uncharge - hierarchically uncharge pages
page_counter.c:	for (c = counter; c; c = c->parent)
page_counter.c: * page_counter_limit - limit the number of pages allowed
page_counter.c: * Returns 0 on success, -EBUSY if the current number of pages on the
page_counter.c:		 * below the concurrently-changing counter value.
page_counter.c:		 * and after, so the read-swap-read is ordered and
page_counter.c:		count = atomic_long_read(&counter->count);
page_counter.c:			return -EBUSY;
page_counter.c:		old = xchg(&counter->limit, limit);
page_counter.c:		if (atomic_long_read(&counter->count) <= count)
page_counter.c:		counter->limit = old;
page_counter.c: * page_counter_memparse - memparse() for page counter limits
page_counter.c: * Returns -EINVAL, or 0 and @nr_pages on success.  @nr_pages will be
page_counter.c:		return -EINVAL;
hugetlb_cgroup.c:	return hugetlb_cgroup_from_css(h_cg->css.parent);
hugetlb_cgroup.c:		if (page_counter_read(&h_cg->hugepage[idx]))
hugetlb_cgroup.c:		struct page_counter *counter = &h_cgroup->hugepage[idx];
hugetlb_cgroup.c:			parent = &parent_h_cgroup->hugepage[idx];
hugetlb_cgroup.c:		return ERR_PTR(-ENOMEM);
hugetlb_cgroup.c:	return &h_cgroup->css;
hugetlb_cgroup.c:		page_counter_charge(&parent->hugepage[idx], nr_pages);
hugetlb_cgroup.c:	counter = &h_cg->hugepage[idx];
hugetlb_cgroup.c:			list_for_each_entry(page, &h->hugepage_activelist, lru)
hugetlb_cgroup.c:	if (!css_tryget_online(&h_cg->css)) {
hugetlb_cgroup.c:	if (!page_counter_try_charge(&h_cg->hugepage[idx], nr_pages, &counter))
hugetlb_cgroup.c:		ret = -ENOMEM;
hugetlb_cgroup.c:	css_put(&h_cg->css);
hugetlb_cgroup.c:	page_counter_uncharge(&h_cg->hugepage[idx], nr_pages);
hugetlb_cgroup.c:	page_counter_uncharge(&h_cg->hugepage[idx], nr_pages);
hugetlb_cgroup.c:	counter = &h_cg->hugepage[MEMFILE_IDX(cft->private)];
hugetlb_cgroup.c:	switch (MEMFILE_ATTR(cft->private)) {
hugetlb_cgroup.c:		return (u64)counter->limit * PAGE_SIZE;
hugetlb_cgroup.c:		return (u64)counter->watermark * PAGE_SIZE;
hugetlb_cgroup.c:		return counter->failcnt;
hugetlb_cgroup.c:		return -EINVAL;
hugetlb_cgroup.c:	ret = page_counter_memparse(buf, "-1", &nr_pages);
hugetlb_cgroup.c:	idx = MEMFILE_IDX(of_cft(of)->private);
hugetlb_cgroup.c:	switch (MEMFILE_ATTR(of_cft(of)->private)) {
hugetlb_cgroup.c:		ret = page_counter_limit(&h_cg->hugepage[idx], nr_pages);
hugetlb_cgroup.c:		ret = -EINVAL;
hugetlb_cgroup.c:	counter = &h_cg->hugepage[MEMFILE_IDX(of_cft(of)->private)];
hugetlb_cgroup.c:	switch (MEMFILE_ATTR(of_cft(of)->private)) {
hugetlb_cgroup.c:		counter->failcnt = 0;
hugetlb_cgroup.c:		ret = -EINVAL;
hugetlb_cgroup.c:	cft = &h->cgroup_files[0];
hugetlb_cgroup.c:	snprintf(cft->name, MAX_CFTYPE_NAME, "%s.limit_in_bytes", buf);
hugetlb_cgroup.c:	cft->private = MEMFILE_PRIVATE(idx, RES_LIMIT);
hugetlb_cgroup.c:	cft->read_u64 = hugetlb_cgroup_read_u64;
hugetlb_cgroup.c:	cft->write = hugetlb_cgroup_write;
hugetlb_cgroup.c:	cft = &h->cgroup_files[1];
hugetlb_cgroup.c:	snprintf(cft->name, MAX_CFTYPE_NAME, "%s.usage_in_bytes", buf);
hugetlb_cgroup.c:	cft->private = MEMFILE_PRIVATE(idx, RES_USAGE);
hugetlb_cgroup.c:	cft->read_u64 = hugetlb_cgroup_read_u64;
hugetlb_cgroup.c:	cft = &h->cgroup_files[2];
hugetlb_cgroup.c:	snprintf(cft->name, MAX_CFTYPE_NAME, "%s.max_usage_in_bytes", buf);
hugetlb_cgroup.c:	cft->private = MEMFILE_PRIVATE(idx, RES_MAX_USAGE);
hugetlb_cgroup.c:	cft->write = hugetlb_cgroup_reset;
hugetlb_cgroup.c:	cft->read_u64 = hugetlb_cgroup_read_u64;
hugetlb_cgroup.c:	cft = &h->cgroup_files[3];
hugetlb_cgroup.c:	snprintf(cft->name, MAX_CFTYPE_NAME, "%s.failcnt", buf);
hugetlb_cgroup.c:	cft->private  = MEMFILE_PRIVATE(idx, RES_FAILCNT);
hugetlb_cgroup.c:	cft->write = hugetlb_cgroup_reset;
hugetlb_cgroup.c:	cft->read_u64 = hugetlb_cgroup_read_u64;
hugetlb_cgroup.c:	cft = &h->cgroup_files[4];
hugetlb_cgroup.c:					  h->cgroup_files));
hugetlb_cgroup.c:	list_move(&newhpage->lru, &h->hugepage_activelist);
page_io.c:// SPDX-License-Identifier: GPL-2.0
page_io.c:		bio->bi_iter.bi_sector = map_swap_page(page, &bdev);
page_io.c:		bio->bi_iter.bi_sector <<= PAGE_SHIFT - 9;
page_io.c:		bio->bi_end_io = end_io;
page_io.c:		VM_BUG_ON(bio->bi_iter.bi_size != PAGE_SIZE * nr);
page_io.c:	struct page *page = bio->bi_io_vec[0].bv_page;
page_io.c:	if (bio->bi_status) {
page_io.c:		 * We failed to write the page out to swap-space.
page_io.c:		 * Re-dirty the page in order to avoid it being reclaimed.
page_io.c:		pr_alert("Write-error on swap-device (%u:%u:%llu)\n",
page_io.c:			 (unsigned long long)bio->bi_iter.bi_sector);
page_io.c:	 * There is no guarantee that the page is in swap cache - the software
page_io.c:	 * suspend code (at least) uses end_swap_bio_read() against a non-
page_io.c:	if (!(sis->flags & SWP_BLKDEV))
page_io.c:	 * reduce unnecessary I/O and enhance wear-leveling
page_io.c:	 * But if in-memory swap device (eg zram) is used,
page_io.c:	 * data in VM-owned memory and compressed data in
page_io.c:	 * zram-owned memory.  So let's free zram-owned memory
page_io.c:	 * and make the VM-owned decompressed page *dirty*,
page_io.c:	disk = sis->bdev->bd_disk;
page_io.c:	if (disk->fops->swap_slot_free_notify) {
page_io.c:		disk->fops->swap_slot_free_notify(sis->bdev,
page_io.c:	struct page *page = bio->bi_io_vec[0].bv_page;
page_io.c:	struct task_struct *waiter = bio->bi_private;
page_io.c:	if (bio->bi_status) {
page_io.c:		pr_alert("Read-error on swap-device (%u:%u:%llu)\n",
page_io.c:			 (unsigned long long)bio->bi_iter.bi_sector);
page_io.c:	WRITE_ONCE(bio->bi_private, NULL);
page_io.c:	struct address_space *mapping = swap_file->f_mapping;
page_io.c:	struct inode *inode = mapping->host;
page_io.c:	sector_t lowest_block = -1;
page_io.c:	blkbits = inode->i_blkbits;
page_io.c:			page_no < sis->max) {
page_io.c:		 * It must be PAGE_SIZE aligned on-disk
page_io.c:		if (first_block & (blocks_per_page - 1)) {
page_io.c:		first_block >>= (PAGE_SHIFT - blkbits);
page_io.c:		 * We found a PAGE_SIZE-length, PAGE_SIZE-aligned run of blocks
page_io.c:	*span = 1 + highest_block - lowest_block;
page_io.c:	sis->max = page_no;
page_io.c:	sis->pages = page_no - 1;
page_io.c:	sis->highest_bit = page_no - 1;
page_io.c:	ret = -EINVAL;
page_io.c:	return (sector_t)__page_file_index(page) << (PAGE_SHIFT - 9);
page_io.c:	if (sis->flags & SWP_FILE) {
page_io.c:		struct file *swap_file = sis->swap_file;
page_io.c:		struct address_space *mapping = swap_file->f_mapping;
page_io.c:		ret = mapping->a_ops->direct_IO(&kiocb, &from);
page_io.c:			 * In the case of swap-over-nfs, this can be a
page_io.c:			 * rotate_reclaimable_page but rate-limit the
page_io.c:			 * the normal direct-to-bio case as it could
page_io.c:	ret = bdev_write_page(sis->bdev, swap_page_sector(page), page, wbc);
page_io.c:		ret = -ENOMEM;
page_io.c:	bio->bi_opf = REQ_OP_WRITE | wbc_to_write_flags(wbc);
page_io.c:	if (sis->flags & SWP_FILE) {
page_io.c:		struct file *swap_file = sis->swap_file;
page_io.c:		struct address_space *mapping = swap_file->f_mapping;
page_io.c:		ret = mapping->a_ops->readpage(swap_file, page);
page_io.c:	ret = bdev_read_page(sis->bdev, swap_page_sector(page), page);
page_io.c:		ret = -ENOMEM;
page_io.c:	disk = bio->bi_disk;
page_io.c:	bio->bi_private = current;
page_io.c:		if (!READ_ONCE(bio->bi_private))
page_io.c:		if (!blk_mq_poll(disk->queue, qc))
page_io.c:	if (sis->flags & SWP_FILE) {
page_io.c:		struct address_space *mapping = sis->swap_file->f_mapping;
page_io.c:		return mapping->a_ops->set_page_dirty(page);
Kconfig.debug:	---help---
Kconfig.debug:	---help---
Kconfig.debug:	---help---
Kconfig.debug:	---help---
Kconfig.debug:	---help---
Kconfig.debug:	---help---
Kconfig.debug:	---help---
Kconfig.debug:    bool "Testcase for the marking rodata read-only"
Kconfig.debug:    ---help---
Kconfig.debug:      This option enables a testcase for the setting rodata read-only.
page_poison.c:// SPDX-License-Identifier: GPL-2.0
page_poison.c:		return -EINVAL;
page_poison.c:	return error && !(error & (error - 1));
page_poison.c:	for (end = mem + bytes - 1; end > start; end--) {
page_poison.c:			end - start + 1, 1);
percpu-stats.c: * mm/percpu-debug.c
percpu-stats.c:#include "percpu-internal.h"
percpu-stats.c:	seq_printf(m, "  %-20s: %12lld\n", X, (long long int)Y)
percpu-stats.c:	return *(int *)a - *(int *)b;
percpu-stats.c:			max_nr_alloc = max(max_nr_alloc, chunk->nr_alloc);
percpu-stats.c:	last_alloc = find_last_bit(chunk->alloc_map,
percpu-stats.c:				   pcpu_chunk_map_bits(chunk) -
percpu-stats.c:				   chunk->end_offset / PCPU_MIN_ALLOC_SIZE - 1);
percpu-stats.c:	last_alloc = test_bit(last_alloc, chunk->alloc_map) ?
percpu-stats.c:	start = chunk->start_offset / PCPU_MIN_ALLOC_SIZE;
percpu-stats.c:		if (test_bit(start, chunk->alloc_map)) {
percpu-stats.c:			end = find_next_bit(chunk->bound_map, last_alloc,
percpu-stats.c:			end = find_next_bit(chunk->alloc_map, last_alloc,
percpu-stats.c:			alloc_sizes[as_len] = -1;
percpu-stats.c:		alloc_sizes[as_len++] *= (end - start) * PCPU_MIN_ALLOC_SIZE;
percpu-stats.c:			sum_frag -= *p;
percpu-stats.c:			max_frag = max(max_frag, -1 * (*p));
percpu-stats.c:		cur_med_alloc = alloc_sizes[(i + as_len - 1) / 2];
percpu-stats.c:		cur_max_alloc = alloc_sizes[as_len - 1];
percpu-stats.c:	P("nr_alloc", chunk->nr_alloc);
percpu-stats.c:	P("max_alloc_size", chunk->max_alloc_size);
percpu-stats.c:	P("empty_pop_pages", chunk->nr_empty_pop_pages);
percpu-stats.c:	P("first_bit", chunk->first_bit);
percpu-stats.c:	P("free_bytes", chunk->free_bytes);
percpu-stats.c:	P("contig_bytes", chunk->contig_bits * PCPU_MIN_ALLOC_SIZE);
percpu-stats.c:		return -ENOMEM;
percpu-stats.c:	seq_printf(m, "  %-20s: %12lld\n", #X, (long long int)pcpu_stats_ai.X)
percpu-stats.c:			"----------------------------------------\n");
percpu-stats.c:	seq_printf(m, "  %-20s: %12llu\n", #X, (unsigned long long)pcpu_stats.X)
percpu-stats.c:			"----------------------------------------\n");
percpu-stats.c:			"----------------------------------------\n");
percpu-stats.c:		seq_puts(m, "Chunk: <- Reserved Chunk\n");
percpu-stats.c:				seq_puts(m, "Chunk: <- First Chunk\n");
workingset.c:// SPDX-License-Identifier: GPL-2.0
workingset.c: *   fault ------------------------+
workingset.c: *              +--------------+   |            +-------------+
workingset.c: *   reclaim <- |   inactive   | <-+-- demotion |    active   | <--+
workingset.c: *              +--------------+                +-------------+    |
workingset.c: *                     +-------------- promotion ------------------+
workingset.c: * done - the thrashing set could never fit into memory under any
workingset.c: * active pages - which may be used more, hopefully less frequently:
workingset.c: *      +-memory available to cache-+
workingset.c: *      +-inactive------+-active----+
workingset.c: *      +---------------+-----------+
workingset.c: * Approximating inactive page access frequency - Observations:
workingset.c: * access the refault, we combine the in-cache distance with the
workingset.c: * out-of-cache distance to get the complete minimum access distance
workingset.c: *      NR_inactive + (R - E)
workingset.c: *   NR_inactive + (R - E) <= NR_inactive + NR_active
workingset.c: *   (R - E) <= NR_active
workingset.c: * Put into words, the refault distance (out-of-cache) can be seen as
workingset.c: * a deficit in inactive list space (in-cache).  If the inactive list
workingset.c: * had (R - E) more page slots, the page would not have been evicted
workingset.c: * So when a refault distance of (R - E) is observed and there are at
workingset.c: * least (R - E) active pages, the refaulting page is activated
workingset.c: * optimistically in the hope that (R - E) active pages are actually
workingset.c: * used less frequently than the refaulting page - or even not used at
workingset.c: * and activations is maintained (node->inactive_age).
workingset.c:	eviction = (eviction << NODES_SHIFT) | pgdat->node_id;
workingset.c:	nid = entry & ((1UL << NODES_SHIFT) - 1);
workingset.c:	memcgid = entry & ((1UL << MEM_CGROUP_ID_SHIFT) - 1);
workingset.c: * workingset_eviction - note the eviction of a page from memory
workingset.c: * Returns a shadow entry to be stored in @mapping->page_tree in place
workingset.c:	/* Page is fully exclusive and pins page->mem_cgroup */
workingset.c:	eviction = atomic_long_inc_return(&lruvec->inactive_age);
workingset.c: * workingset_refault - evaluate the refault of a previously evicted page
workingset.c:	refault = atomic_long_read(&lruvec->inactive_age);
workingset.c:	refault_distance = (refault - eviction) & EVICTION_MASK;
workingset.c: * workingset_activation - note a page activation
workingset.c:	 * Filter non-memcg pages here, e.g. unmap can call
workingset.c:	 * XXX: See workingset_refault() - this should return
workingset.c:	atomic_long_inc(&lruvec->inactive_age);
workingset.c:	 * Track non-empty nodes that contain only shadow entries;
workingset.c:	 * as node->private_list is protected by &mapping->tree_lock.
workingset.c:	if (node->count && node->count == node->exceptional) {
workingset.c:		if (list_empty(&node->private_list))
workingset.c:			list_lru_add(&shadow_nodes, &node->private_list);
workingset.c:		if (!list_empty(&node->private_list))
workingset.c:			list_lru_del(&shadow_nodes, &node->private_list);
workingset.c:	/* list_lru lock nests inside IRQ-safe mapping->tree_lock */
workingset.c:	 * worst-case density of 1/8th. Below that, not all eligible
workingset.c:	 * On 64-bit with 7 radix_tree_nodes per page and 64 slots
workingset.c:	if (sc->memcg) {
workingset.c:		cache = mem_cgroup_node_nr_lru_pages(sc->memcg, sc->nid,
workingset.c:		cache = node_page_state(NODE_DATA(sc->nid), NR_ACTIVE_FILE) +
workingset.c:			node_page_state(NODE_DATA(sc->nid), NR_INACTIVE_FILE);
workingset.c:	max_nodes = cache >> (RADIX_TREE_MAP_SHIFT - 3);
workingset.c:	return nodes - max_nodes;
workingset.c:	 * the shadow node LRU under the mapping->tree_lock and the
workingset.c:	 * We can then safely transition to the mapping->tree_lock to
workingset.c:	 * to reclaim, take the node off-LRU, and drop the lru_lock.
workingset.c:	mapping = container_of(node->root, struct address_space, page_tree);
workingset.c:	if (!spin_trylock(&mapping->tree_lock)) {
workingset.c:	if (WARN_ON_ONCE(!node->exceptional))
workingset.c:	if (WARN_ON_ONCE(node->count != node->exceptional))
workingset.c:		if (node->slots[i]) {
workingset.c:			if (WARN_ON_ONCE(!radix_tree_exceptional_entry(node->slots[i])))
workingset.c:			if (WARN_ON_ONCE(!node->exceptional))
workingset.c:			if (WARN_ON_ONCE(!mapping->nrexceptional))
workingset.c:			node->slots[i] = NULL;
workingset.c:			node->exceptional--;
workingset.c:			node->count--;
workingset.c:			mapping->nrexceptional--;
workingset.c:	if (WARN_ON_ONCE(node->exceptional))
workingset.c:	__radix_tree_delete_node(&mapping->page_tree, node,
workingset.c:	spin_unlock(&mapping->tree_lock);
workingset.c:	/* list_lru lock nests inside IRQ-safe mapping->tree_lock */
workingset.c: * Our list_lru->lock is IRQ-safe as it nests inside the IRQ-safe
workingset.c: * mapping->tree_lock.
workingset.c:	 * double the initial memory by using totalram_pages as-is.
workingset.c:	timestamp_bits = BITS_PER_LONG - EVICTION_SHIFT;
workingset.c:	max_order = fls_long(totalram_pages - 1);
workingset.c:		bucket_order = max_order - timestamp_bits;
cleancache.c: * Copyright (C) 2009-2010 Oracle Corp. All rights reserved.
cleancache.c:	switch (sb->cleancache_poolid) {
cleancache.c:		return -EBUSY;
cleancache.c:	 * handle such a scenario, here we call ->init_fs or ->init_shared_fs
cleancache.c:	 * shared filesystems, we temporarily initialize sb->cleancache_poolid
cleancache.c:	 *    ->kill_sb
cleancache.c:	 *    ->mount yet, it waits until it is finished
cleancache.c:	 * c) cleancache_init_fs is called from ->mount and
cleancache.c:	 *    cleancache_invalidate_fs is called from ->kill_sb
cleancache.c:	 * ->mount by itself according to c). This proves that we call
cleancache.c:	 * ->init_fs at least once for each active super block.
cleancache.c:	 * block that has already started ->init_fs, it will wait until ->mount
cleancache.c:	 * and hence ->init_fs has finished, then check cleancache_poolid, see
cleancache.c:	 * that we call ->init_fs no more than once for each super block.
cleancache.c:	 * until the corresponding ->init_fs has been actually called and
cleancache.c:/* Called by a cleancache-enabled filesystem at time of mount */
cleancache.c:		pool_id = cleancache_ops->init_fs(PAGE_SIZE);
cleancache.c:	sb->cleancache_poolid = pool_id;
cleancache.c:/* Called by a cleancache-enabled clustered filesystem at time of mount */
cleancache.c:		pool_id = cleancache_ops->init_shared_fs(&sb->s_uuid, PAGE_SIZE);
cleancache.c:	sb->cleancache_poolid = pool_id;
cleancache.c:	struct super_block *sb = inode->i_sb;
cleancache.c:	key->u.ino = inode->i_ino;
cleancache.c:	if (sb->s_export_op != NULL) {
cleancache.c:		fhfn = sb->s_export_op->encode_fh;
cleancache.c:			len = (*fhfn)(inode, &key->u.fh[0], &maxlen, NULL);
cleancache.c:				return -1;
cleancache.c:				return -1;
cleancache.c: * The pageframe is unchanged and returns -1 if the get fails.
cleancache.c: * The function has two checks before any action is taken - whether
cleancache.c: * a backend is registered and whether the sb->cleancache_poolid
cleancache.c:	int ret = -1;
cleancache.c:	pool_id = page->mapping->host->i_sb->cleancache_poolid;
cleancache.c:	if (cleancache_get_key(page->mapping->host, &key) < 0)
cleancache.c:	ret = cleancache_ops->get_page(pool_id, key, page->index, page);
cleancache.c: * (previously-obtained per-filesystem) poolid and the page's,
cleancache.c: * The function has two checks before any action is taken - whether
cleancache.c: * a backend is registered and whether the sb->cleancache_poolid
cleancache.c:	pool_id = page->mapping->host->i_sb->cleancache_poolid;
cleancache.c:		cleancache_get_key(page->mapping->host, &key) >= 0) {
cleancache.c:		cleancache_ops->put_page(pool_id, key, page->index, page);
cleancache.c: * The function has two checks before any action is taken - whether
cleancache.c: * a backend is registered and whether the sb->cleancache_poolid
cleancache.c:	/* careful... page->mapping is NULL sometimes when this is called */
cleancache.c:	int pool_id = mapping->host->i_sb->cleancache_poolid;
cleancache.c:		if (cleancache_get_key(mapping->host, &key) >= 0) {
cleancache.c:			cleancache_ops->invalidate_page(pool_id,
cleancache.c:					key, page->index);
cleancache.c: * The function has two checks before any action is taken - whether
cleancache.c: * a backend is registered and whether the sb->cleancache_poolid
cleancache.c:	int pool_id = mapping->host->i_sb->cleancache_poolid;
cleancache.c:	if (pool_id >= 0 && cleancache_get_key(mapping->host, &key) >= 0)
cleancache.c:		cleancache_ops->invalidate_inode(pool_id, key);
cleancache.c: * Called by any cleancache-enabled filesystem at time of unmount;
cleancache.c:	pool_id = sb->cleancache_poolid;
cleancache.c:	sb->cleancache_poolid = CLEANCACHE_NO_POOL;
cleancache.c:		cleancache_ops->invalidate_fs(pool_id);
cleancache.c:		return -ENXIO;
huge_memory.c: *  the COPYING file in the top-level directory.
huge_memory.c:	if (test_bit(MMF_HUGE_ZERO_PAGE, &mm->flags))
huge_memory.c:	if (test_and_set_bit(MMF_HUGE_ZERO_PAGE, &mm->flags))
huge_memory.c:	if (test_bit(MMF_HUGE_ZERO_PAGE, &mm->flags))
huge_memory.c:		    min(sizeof("always")-1, count))) {
huge_memory.c:			   min(sizeof("madvise")-1, count))) {
huge_memory.c:			   min(sizeof("never")-1, count))) {
huge_memory.c:		ret = -EINVAL;
huge_memory.c:		return -EINVAL;
huge_memory.c:		    min(sizeof("always")-1, count))) {
huge_memory.c:		    min(sizeof("defer+madvise")-1, count))) {
huge_memory.c:		    min(sizeof("defer")-1, count))) {
huge_memory.c:			   min(sizeof("madvise")-1, count))) {
huge_memory.c:			   min(sizeof("never")-1, count))) {
huge_memory.c:		return -EINVAL;
huge_memory.c:		return -ENOMEM;
huge_memory.c:		return -EINVAL;
huge_memory.c:	 * we use page->mapping and page->index in second tail page
huge_memory.c:	if (totalram_pages < (512 << (20 - PAGE_SHIFT))) {
huge_memory.c:	if (likely(vma->vm_flags & VM_WRITE))
huge_memory.c:	 * ->lru in the tail pages is occupied by compound_head.
huge_memory.c:	 * Let's use ->mapping + ->index in the second tail page as list_head.
huge_memory.c:	 * we use page->mapping and page->indexlru in second tail page
huge_memory.c:	if (off_end <= off_align || (off_end - off_align) < size)
huge_memory.c:	addr = current->mm->get_unmapped_area(filp, 0, len_pad,
huge_memory.c:	addr += (off - addr) & (size - 1);
huge_memory.c:	if (!IS_DAX(filp->f_mapping->host) || !IS_ENABLED(CONFIG_FS_DAX_PMD))
huge_memory.c:	return current->mm->get_unmapped_area(filp, addr, len, pgoff, flags);
huge_memory.c:	struct vm_area_struct *vma = vmf->vma;
huge_memory.c:	unsigned long haddr = vmf->address & HPAGE_PMD_MASK;
huge_memory.c:	if (mem_cgroup_try_charge(page, vma->vm_mm, gfp | __GFP_NORETRY, &memcg,
huge_memory.c:	pgtable = pte_alloc_one(vma->vm_mm, haddr);
huge_memory.c:	clear_huge_page(page, vmf->address, HPAGE_PMD_NR);
huge_memory.c:	vmf->ptl = pmd_lock(vma->vm_mm, vmf->pmd);
huge_memory.c:	if (unlikely(!pmd_none(*vmf->pmd))) {
huge_memory.c:		ret = check_stable_address_space(vma->vm_mm);
huge_memory.c:			spin_unlock(vmf->ptl);
huge_memory.c:			pte_free(vma->vm_mm, pgtable);
huge_memory.c:		entry = mk_huge_pmd(page, vma->vm_page_prot);
huge_memory.c:		pgtable_trans_huge_deposit(vma->vm_mm, vmf->pmd, pgtable);
huge_memory.c:		set_pmd_at(vma->vm_mm, haddr, vmf->pmd, entry);
huge_memory.c:		add_mm_counter(vma->vm_mm, MM_ANONPAGES, HPAGE_PMD_NR);
huge_memory.c:		atomic_long_inc(&vma->vm_mm->nr_ptes);
huge_memory.c:		spin_unlock(vmf->ptl);
huge_memory.c:	spin_unlock(vmf->ptl);
huge_memory.c:		pte_free(vma->vm_mm, pgtable);
huge_memory.c:	const bool vma_madvised = !!(vma->vm_flags & VM_HUGEPAGE);
huge_memory.c:	entry = mk_pmd(zero_page, vma->vm_page_prot);
huge_memory.c:	atomic_long_inc(&mm->nr_ptes);
huge_memory.c:	struct vm_area_struct *vma = vmf->vma;
huge_memory.c:	unsigned long haddr = vmf->address & HPAGE_PMD_MASK;
huge_memory.c:	if (haddr < vma->vm_start || haddr + HPAGE_PMD_SIZE > vma->vm_end)
huge_memory.c:	if (unlikely(khugepaged_enter(vma, vma->vm_flags)))
huge_memory.c:	if (!(vmf->flags & FAULT_FLAG_WRITE) &&
huge_memory.c:			!mm_forbids_zeropage(vma->vm_mm) &&
huge_memory.c:		pgtable = pte_alloc_one(vma->vm_mm, haddr);
huge_memory.c:		zero_page = mm_get_huge_zero_page(vma->vm_mm);
huge_memory.c:			pte_free(vma->vm_mm, pgtable);
huge_memory.c:		vmf->ptl = pmd_lock(vma->vm_mm, vmf->pmd);
huge_memory.c:		if (pmd_none(*vmf->pmd)) {
huge_memory.c:			ret = check_stable_address_space(vma->vm_mm);
huge_memory.c:				spin_unlock(vmf->ptl);
huge_memory.c:				spin_unlock(vmf->ptl);
huge_memory.c:				set_huge_zero_page(pgtable, vma->vm_mm, vma,
huge_memory.c:						   haddr, vmf->pmd, zero_page);
huge_memory.c:				spin_unlock(vmf->ptl);
huge_memory.c:			spin_unlock(vmf->ptl);
huge_memory.c:			pte_free(vma->vm_mm, pgtable);
huge_memory.c:	struct mm_struct *mm = vma->vm_mm;
huge_memory.c:		atomic_long_inc(&mm->nr_ptes);
huge_memory.c:	pgprot_t pgprot = vma->vm_page_prot;
huge_memory.c:	BUG_ON(!(vma->vm_flags & (VM_PFNMAP|VM_MIXEDMAP)));
huge_memory.c:	BUG_ON((vma->vm_flags & (VM_PFNMAP|VM_MIXEDMAP)) ==
huge_memory.c:	BUG_ON((vma->vm_flags & VM_PFNMAP) && is_cow_mapping(vma->vm_flags));
huge_memory.c:	if (addr < vma->vm_start || addr >= vma->vm_end)
huge_memory.c:		pgtable = pte_alloc_one(vma->vm_mm, addr);
huge_memory.c:	if (likely(vma->vm_flags & VM_WRITE))
huge_memory.c:	struct mm_struct *mm = vma->vm_mm;
huge_memory.c:	pgprot_t pgprot = vma->vm_page_prot;
huge_memory.c:	BUG_ON(!(vma->vm_flags & (VM_PFNMAP|VM_MIXEDMAP)));
huge_memory.c:	BUG_ON((vma->vm_flags & (VM_PFNMAP|VM_MIXEDMAP)) ==
huge_memory.c:	BUG_ON((vma->vm_flags & VM_PFNMAP) && is_cow_mapping(vma->vm_flags));
huge_memory.c:	if (addr < vma->vm_start || addr >= vma->vm_end)
huge_memory.c:	struct mm_struct *mm = vma->vm_mm;
huge_memory.c:		return ERR_PTR(-EEXIST);
huge_memory.c:		return ERR_PTR(-EFAULT);
huge_memory.c:	int ret = -ENOMEM;
huge_memory.c:	/* Skip if can be re-fill on fault */
huge_memory.c:	ret = -EAGAIN;
huge_memory.c:		atomic_long_inc(&dst_mm->nr_ptes);
huge_memory.c:	atomic_long_inc(&dst_mm->nr_ptes);
huge_memory.c:	struct mm_struct *mm = vma->vm_mm;
huge_memory.c:		return ERR_PTR(-EEXIST);
huge_memory.c:		return ERR_PTR(-EFAULT);
huge_memory.c:	ret = -EAGAIN;
huge_memory.c:	bool write = vmf->flags & FAULT_FLAG_WRITE;
huge_memory.c:	vmf->ptl = pud_lock(vmf->vma->vm_mm, vmf->pud);
huge_memory.c:	if (unlikely(!pud_same(*vmf->pud, orig_pud)))
huge_memory.c:	haddr = vmf->address & HPAGE_PUD_MASK;
huge_memory.c:	if (pudp_set_access_flags(vmf->vma, haddr, vmf->pud, entry, write))
huge_memory.c:		update_mmu_cache_pud(vmf->vma, vmf->address, vmf->pud);
huge_memory.c:	spin_unlock(vmf->ptl);
huge_memory.c:	bool write = vmf->flags & FAULT_FLAG_WRITE;
huge_memory.c:	vmf->ptl = pmd_lock(vmf->vma->vm_mm, vmf->pmd);
huge_memory.c:	if (unlikely(!pmd_same(*vmf->pmd, orig_pmd)))
huge_memory.c:	haddr = vmf->address & HPAGE_PMD_MASK;
huge_memory.c:	if (pmdp_set_access_flags(vmf->vma, haddr, vmf->pmd, entry, write))
huge_memory.c:		update_mmu_cache_pmd(vmf->vma, vmf->address, vmf->pmd);
huge_memory.c:	spin_unlock(vmf->ptl);
huge_memory.c:	struct vm_area_struct *vma = vmf->vma;
huge_memory.c:	unsigned long haddr = vmf->address & HPAGE_PMD_MASK;
huge_memory.c:					       vmf->address, page_to_nid(page));
huge_memory.c:			     mem_cgroup_try_charge(pages[i], vma->vm_mm,
huge_memory.c:			while (--i >= 0) {
huge_memory.c:	mmu_notifier_invalidate_range_start(vma->vm_mm, mmun_start, mmun_end);
huge_memory.c:	vmf->ptl = pmd_lock(vma->vm_mm, vmf->pmd);
huge_memory.c:	if (unlikely(!pmd_same(*vmf->pmd, orig_pmd)))
huge_memory.c:	pmdp_huge_clear_flush_notify(vma, haddr, vmf->pmd);
huge_memory.c:	pgtable = pgtable_trans_huge_withdraw(vma->vm_mm, vmf->pmd);
huge_memory.c:	pmd_populate(vma->vm_mm, &_pmd, pgtable);
huge_memory.c:		entry = mk_pte(pages[i], vma->vm_page_prot);
huge_memory.c:		page_add_new_anon_rmap(pages[i], vmf->vma, haddr, false);
huge_memory.c:		vmf->pte = pte_offset_map(&_pmd, haddr);
huge_memory.c:		VM_BUG_ON(!pte_none(*vmf->pte));
huge_memory.c:		set_pte_at(vma->vm_mm, haddr, vmf->pte, entry);
huge_memory.c:		pte_unmap(vmf->pte);
huge_memory.c:	pmd_populate(vma->vm_mm, vmf->pmd, pgtable);
huge_memory.c:	spin_unlock(vmf->ptl);
huge_memory.c:	mmu_notifier_invalidate_range_end(vma->vm_mm, mmun_start, mmun_end);
huge_memory.c:	spin_unlock(vmf->ptl);
huge_memory.c:	mmu_notifier_invalidate_range_end(vma->vm_mm, mmun_start, mmun_end);
huge_memory.c:	struct vm_area_struct *vma = vmf->vma;
huge_memory.c:	unsigned long haddr = vmf->address & HPAGE_PMD_MASK;
huge_memory.c:	vmf->ptl = pmd_lockptr(vma->vm_mm, vmf->pmd);
huge_memory.c:	VM_BUG_ON_VMA(!vma->anon_vma, vma);
huge_memory.c:	spin_lock(vmf->ptl);
huge_memory.c:	if (unlikely(!pmd_same(*vmf->pmd, orig_pmd)))
huge_memory.c:		spin_unlock(vmf->ptl);
huge_memory.c:		spin_lock(vmf->ptl);
huge_memory.c:		if (unlikely(!pmd_same(*vmf->pmd, orig_pmd))) {
huge_memory.c:		if (pmdp_set_access_flags(vma, haddr, vmf->pmd, entry,  1))
huge_memory.c:			update_mmu_cache_pmd(vma, vmf->address, vmf->pmd);
huge_memory.c:	spin_unlock(vmf->ptl);
huge_memory.c:			split_huge_pmd(vma, vmf->pmd, vmf->address);
huge_memory.c:				split_huge_pmd(vma, vmf->pmd, vmf->address);
huge_memory.c:	if (unlikely(mem_cgroup_try_charge(new_page, vma->vm_mm,
huge_memory.c:		split_huge_pmd(vma, vmf->pmd, vmf->address);
huge_memory.c:		clear_huge_page(new_page, vmf->address, HPAGE_PMD_NR);
huge_memory.c:	mmu_notifier_invalidate_range_start(vma->vm_mm, mmun_start, mmun_end);
huge_memory.c:	spin_lock(vmf->ptl);
huge_memory.c:	if (unlikely(!pmd_same(*vmf->pmd, orig_pmd))) {
huge_memory.c:		spin_unlock(vmf->ptl);
huge_memory.c:		entry = mk_huge_pmd(new_page, vma->vm_page_prot);
huge_memory.c:		pmdp_huge_clear_flush_notify(vma, haddr, vmf->pmd);
huge_memory.c:		set_pmd_at(vma->vm_mm, haddr, vmf->pmd, entry);
huge_memory.c:		update_mmu_cache_pmd(vma, vmf->address, vmf->pmd);
huge_memory.c:			add_mm_counter(vma->vm_mm, MM_ANONPAGES, HPAGE_PMD_NR);
huge_memory.c:	spin_unlock(vmf->ptl);
huge_memory.c:	mmu_notifier_invalidate_range_end(vma->vm_mm, mmun_start, mmun_end);
huge_memory.c:	spin_unlock(vmf->ptl);
huge_memory.c:	struct mm_struct *mm = vma->vm_mm;
huge_memory.c:		return ERR_PTR(-EFAULT);
huge_memory.c:	if ((flags & FOLL_MLOCK) && (vma->vm_flags & VM_LOCKED)) {
huge_memory.c:		 * We don't mlock() pte-mapped THPs. This way we can avoid
huge_memory.c:		 * leaking mlocked pages into non-VM_LOCKED VMAs.
huge_memory.c:		 * break COW for the mlock() -- see gup_flags |= FOLL_WRITE for
huge_memory.c:		 * mlocking read-only mapping shared over fork(). We skip
huge_memory.c:		if (PageDoubleMap(page) || !page->mapping)
huge_memory.c:		if (page->mapping && !PageDoubleMap(page))
huge_memory.c:	struct vm_area_struct *vma = vmf->vma;
huge_memory.c:	unsigned long haddr = vmf->address & HPAGE_PMD_MASK;
huge_memory.c:	int page_nid = -1, this_nid = numa_node_id();
huge_memory.c:	int target_nid, last_cpupid = -1;
huge_memory.c:	vmf->ptl = pmd_lock(vma->vm_mm, vmf->pmd);
huge_memory.c:	if (unlikely(!pmd_same(pmd, *vmf->pmd)))
huge_memory.c:	if (unlikely(pmd_trans_migrating(*vmf->pmd))) {
huge_memory.c:		page = pmd_page(*vmf->pmd);
huge_memory.c:		spin_unlock(vmf->ptl);
huge_memory.c:	if (target_nid == -1) {
huge_memory.c:		page_nid = -1;
huge_memory.c:		spin_unlock(vmf->ptl);
huge_memory.c:	spin_unlock(vmf->ptl);
huge_memory.c:	spin_lock(vmf->ptl);
huge_memory.c:	if (unlikely(!pmd_same(pmd, *vmf->pmd))) {
huge_memory.c:		page_nid = -1;
huge_memory.c:		page_nid = -1;
huge_memory.c:	if (mm_tlb_flush_pending(vma->vm_mm))
huge_memory.c:	spin_unlock(vmf->ptl);
huge_memory.c:	migrated = migrate_misplaced_transhuge_page(vma->vm_mm, vma,
huge_memory.c:				vmf->pmd, pmd, vmf->address, page, target_nid);
huge_memory.c:	pmd = pmd_modify(pmd, vma->vm_page_prot);
huge_memory.c:	set_pmd_at(vma->vm_mm, haddr, vmf->pmd, pmd);
huge_memory.c:	update_mmu_cache_pmd(vma, vmf->address, vmf->pmd);
huge_memory.c:	spin_unlock(vmf->ptl);
huge_memory.c:	if (page_nid != -1)
huge_memory.c:	struct mm_struct *mm = tlb->mm;
huge_memory.c:	 * If user want to discard part-pages of THP, split it so MADV_FREE
huge_memory.c:	if (next - addr != HPAGE_PMD_SIZE) {
huge_memory.c:	atomic_long_dec(&mm->nr_ptes);
huge_memory.c:	orig_pmd = pmdp_huge_get_and_clear_full(tlb->mm, addr, pmd,
huge_memory.c:			tlb->fullmm);
huge_memory.c:			zap_deposited_table(tlb->mm, pmd);
huge_memory.c:		zap_deposited_table(tlb->mm, pmd);
huge_memory.c:			zap_deposited_table(tlb->mm, pmd);
huge_memory.c:			add_mm_counter(tlb->mm, MM_ANONPAGES, -HPAGE_PMD_NR);
huge_memory.c:				zap_deposited_table(tlb->mm, pmd);
huge_memory.c:			add_mm_counter(tlb->mm, MM_FILEPAGES, -HPAGE_PMD_NR);
huge_memory.c:	struct mm_struct *mm = vma->vm_mm;
huge_memory.c:	    old_end - old_addr < HPAGE_PMD_SIZE)
huge_memory.c: *  - 0 if PMD could not be locked
huge_memory.c: *  - 1 if PMD was locked but protections unchange and TLB flush unnecessary
huge_memory.c: *  - HPAGE_PMD_NR is protections changed and TLB flush necessary
huge_memory.c:	struct mm_struct *mm = vma->vm_mm;
huge_memory.c:	 * Avoid trapping faults against the zero page. The read-only
huge_memory.c:	 * data is likely to be read-cached on the local CPU and
huge_memory.c:	 *				 // pmd is re-established
huge_memory.c:	ptl = pmd_lock(vma->vm_mm, pmd);
huge_memory.c:	ptl = pud_lock(vma->vm_mm, pud);
huge_memory.c:	orig_pud = pudp_huge_get_and_clear_full(tlb->mm, addr, pud,
huge_memory.c:			tlb->fullmm);
huge_memory.c:	VM_BUG_ON_VMA(vma->vm_start > haddr, vma);
huge_memory.c:	VM_BUG_ON_VMA(vma->vm_end < haddr + HPAGE_PUD_SIZE, vma);
huge_memory.c:	struct mm_struct *mm = vma->vm_mm;
huge_memory.c:	struct mm_struct *mm = vma->vm_mm;
huge_memory.c:		entry = pfn_pte(my_zero_pfn(haddr), vma->vm_page_prot);
huge_memory.c:	struct mm_struct *mm = vma->vm_mm;
huge_memory.c:	VM_BUG_ON_VMA(vma->vm_start > haddr, vma);
huge_memory.c:	VM_BUG_ON_VMA(vma->vm_end < haddr + HPAGE_PMD_SIZE, vma);
huge_memory.c:		add_mm_counter(mm, MM_FILEPAGES, -HPAGE_PMD_NR);
huge_memory.c:	page_ref_add(page, HPAGE_PMD_NR - 1);
huge_memory.c:			entry = mk_pte(page + i, READ_ONCE(vma->vm_page_prot));
huge_memory.c:	 * false-negative page_mapped().
huge_memory.c:	if (atomic_add_negative(-1, compound_mapcount_ptr(page))) {
huge_memory.c:	 * place). If we overwrite the pmd with the not-huge version pointing
huge_memory.c:	 * and finally we write the non-huge version of the pmd entry with
huge_memory.c:	struct mm_struct *mm = vma->vm_mm;
huge_memory.c:	pgd = pgd_offset(vma->vm_mm, address);
huge_memory.c:	    (start & HPAGE_PMD_MASK) >= vma->vm_start &&
huge_memory.c:	    (start & HPAGE_PMD_MASK) + HPAGE_PMD_SIZE <= vma->vm_end)
huge_memory.c:	    (end & HPAGE_PMD_MASK) >= vma->vm_start &&
huge_memory.c:	    (end & HPAGE_PMD_MASK) + HPAGE_PMD_SIZE <= vma->vm_end)
huge_memory.c:	 * If we're also updating the vma->vm_next->vm_start, if the new
huge_memory.c:	 * vm_next->vm_start isn't page aligned and it could previously
huge_memory.c:		struct vm_area_struct *next = vma->vm_next;
huge_memory.c:		unsigned long nstart = next->vm_start;
huge_memory.c:		    (nstart & HPAGE_PMD_MASK) >= next->vm_start &&
huge_memory.c:		    (nstart & HPAGE_PMD_MASK) + HPAGE_PMD_SIZE <= next->vm_end)
huge_memory.c:	VM_BUG_ON_PAGE(atomic_read(&page_tail->_mapcount) != -1, page_tail);
huge_memory.c:	 * tail_page->_refcount is zero and not changing from under us. But
huge_memory.c:	page_tail->flags &= ~PAGE_FLAGS_CHECK_AT_PREP;
huge_memory.c:	page_tail->flags |= (head->flags &
huge_memory.c:	 * Page flags also must be visible before we make the page non-compound.
huge_memory.c:	/* ->mapping in first tail page is compound_mapcount */
huge_memory.c:	VM_BUG_ON_PAGE(tail > 2 && page_tail->mapping != TAIL_MAPPING,
huge_memory.c:	page_tail->mapping = head->mapping;
huge_memory.c:	page_tail->index = head->index + tail;
huge_memory.c:	pgoff_t end = -1;
huge_memory.c:	lruvec = mem_cgroup_page_lruvec(head, zone->zone_pgdat);
huge_memory.c:		end = DIV_ROUND_UP(i_size_read(head->mapping->host), PAGE_SIZE);
huge_memory.c:	for (i = HPAGE_PMD_NR - 1; i >= 1; i--) {
huge_memory.c:				shmem_uncharge(head->mapping->host, 1);
huge_memory.c:		spin_unlock(&head->mapping->tree_lock);
huge_memory.c:		return atomic_read(&page->_mapcount) + 1;
huge_memory.c:		return ret - compound * HPAGE_PMD_NR;
huge_memory.c:		ret -= HPAGE_PMD_NR;
huge_memory.c: * accuracy is primarily needed to know if copy-on-write faults can
huge_memory.c: * reuse the page and change the mapping to read-write instead of
huge_memory.c: * anon_vma of the transparent hugepage can become the vma->anon_vma
huge_memory.c: * page_trans_huge_mapcount() in the copy-on-write faults where we
huge_memory.c:		mapcount = atomic_read(&page->_mapcount) + 1;
huge_memory.c:		ret -= 1;
huge_memory.c:		_total_mapcount -= HPAGE_PMD_NR;
huge_memory.c:	return total_mapcount(page) == page_count(page) - extra_pins - 1;
huge_memory.c: * Only caller must hold pin on the @page, otherwise split fails with -EBUSY.
huge_memory.c: * Returns -EBUSY if the page is pinned or if anon_vma disappeared from under
huge_memory.c:		return -EBUSY;
huge_memory.c:			ret = -EBUSY;
huge_memory.c:		mapping = head->mapping;
huge_memory.c:			ret = -EBUSY;
huge_memory.c:		ret = -EBUSY;
huge_memory.c:	/* Make sure the page is not on per-CPU pagevec as it takes pin */
huge_memory.c:		spin_lock(&mapping->tree_lock);
huge_memory.c:		pslot = radix_tree_lookup_slot(&mapping->page_tree,
huge_memory.c:					&mapping->tree_lock) != head)
huge_memory.c:	/* Prevent deferred_split_scan() touching ->_refcount */
huge_memory.c:	spin_lock(&pgdata->split_queue_lock);
huge_memory.c:			pgdata->split_queue_len--;
huge_memory.c:		spin_unlock(&pgdata->split_queue_lock);
huge_memory.c:		spin_unlock(&pgdata->split_queue_lock);
huge_memory.c:			spin_unlock(&mapping->tree_lock);
huge_memory.c:		ret = -EBUSY;
huge_memory.c:	spin_lock_irqsave(&pgdata->split_queue_lock, flags);
huge_memory.c:		pgdata->split_queue_len--;
huge_memory.c:	spin_unlock_irqrestore(&pgdata->split_queue_lock, flags);
huge_memory.c:	spin_lock_irqsave(&pgdata->split_queue_lock, flags);
huge_memory.c:		list_add_tail(page_deferred_list(page), &pgdata->split_queue);
huge_memory.c:		pgdata->split_queue_len++;
huge_memory.c:	spin_unlock_irqrestore(&pgdata->split_queue_lock, flags);
huge_memory.c:	struct pglist_data *pgdata = NODE_DATA(sc->nid);
huge_memory.c:	return ACCESS_ONCE(pgdata->split_queue_len);
huge_memory.c:	struct pglist_data *pgdata = NODE_DATA(sc->nid);
huge_memory.c:	spin_lock_irqsave(&pgdata->split_queue_lock, flags);
huge_memory.c:	list_for_each_safe(pos, next, &pgdata->split_queue) {
huge_memory.c:			pgdata->split_queue_len--;
huge_memory.c:		if (!--sc->nr_to_scan)
huge_memory.c:	spin_unlock_irqrestore(&pgdata->split_queue_lock, flags);
huge_memory.c:	spin_lock_irqsave(&pgdata->split_queue_lock, flags);
huge_memory.c:	list_splice_tail(&list, &pgdata->split_queue);
huge_memory.c:	spin_unlock_irqrestore(&pgdata->split_queue_lock, flags);
huge_memory.c:	if (!split && list_empty(&pgdata->split_queue))
huge_memory.c:		return -EINVAL;
huge_memory.c:		for (pfn = zone->zone_start_pfn; pfn < max_zone_pfn; pfn++) {
huge_memory.c:	struct vm_area_struct *vma = pvmw->vma;
huge_memory.c:	struct mm_struct *mm = vma->vm_mm;
huge_memory.c:	unsigned long address = pvmw->address;
huge_memory.c:	if (!(pvmw->pmd && !pvmw->pte))
huge_memory.c:	pmdval = *pvmw->pmd;
huge_memory.c:	pmdp_invalidate(vma, address, pvmw->pmd);
huge_memory.c:	set_pmd_at(mm, address, pvmw->pmd, pmdswp);
huge_memory.c:	struct vm_area_struct *vma = pvmw->vma;
huge_memory.c:	struct mm_struct *mm = vma->vm_mm;
huge_memory.c:	unsigned long address = pvmw->address;
huge_memory.c:	if (!(pvmw->pmd && !pvmw->pte))
huge_memory.c:	entry = pmd_to_swp_entry(*pvmw->pmd);
huge_memory.c:	pmde = pmd_mkold(mk_huge_pmd(new, vma->vm_page_prot));
huge_memory.c:	if (pmd_swp_soft_dirty(*pvmw->pmd))
huge_memory.c:	set_pmd_at(mm, mmun_start, pvmw->pmd, pmde);
huge_memory.c:	if (vma->vm_flags & VM_LOCKED)
huge_memory.c:	update_mmu_cache_pmd(vma, address, pvmw->pmd);
frame_vector.c:// SPDX-License-Identifier: GPL-2.0
frame_vector.c: * get_vaddr_frames() - map virtual addresses to pfns
frame_vector.c:	struct mm_struct *mm = current->mm;
frame_vector.c:	if (WARN_ON_ONCE(nr_frames > vec->nr_allocated))
frame_vector.c:		nr_frames = vec->nr_allocated;
frame_vector.c:	down_read(&mm->mmap_sem);
frame_vector.c:		ret = -EFAULT;
frame_vector.c:	 * get_user_pages_longterm() and disallow it for filesystem-dax
frame_vector.c:		ret = -EOPNOTSUPP;
frame_vector.c:	if (!(vma->vm_flags & (VM_IO | VM_PFNMAP))) {
frame_vector.c:		vec->got_ref = true;
frame_vector.c:		vec->is_pfns = false;
frame_vector.c:			gup_flags, (struct page **)(vec->ptrs), &locked);
frame_vector.c:	vec->got_ref = false;
frame_vector.c:	vec->is_pfns = true;
frame_vector.c:		while (ret < nr_frames && start + PAGE_SIZE <= vma->vm_end) {
frame_vector.c:		if (ret >= nr_frames || start < vma->vm_end)
frame_vector.c:	} while (vma && vma->vm_flags & (VM_IO | VM_PFNMAP));
frame_vector.c:		up_read(&mm->mmap_sem);
frame_vector.c:		ret = -EFAULT;
frame_vector.c:		vec->nr_frames = ret;
frame_vector.c: * put_vaddr_frames() - drop references to pages if get_vaddr_frames() acquired
frame_vector.c:	if (!vec->got_ref)
frame_vector.c:	for (i = 0; i < vec->nr_frames; i++)
frame_vector.c:	vec->got_ref = false;
frame_vector.c:	vec->nr_frames = 0;
frame_vector.c: * frame_vector_to_pages - convert frame vector to contain page pointers
frame_vector.c:	if (!vec->is_pfns)
frame_vector.c:	for (i = 0; i < vec->nr_frames; i++)
frame_vector.c:			return -EINVAL;
frame_vector.c:	for (i = 0; i < vec->nr_frames; i++)
frame_vector.c:	vec->is_pfns = false;
frame_vector.c: * frame_vector_to_pfns - convert frame vector to contain pfns
frame_vector.c:	if (vec->is_pfns)
frame_vector.c:	pages = (struct page **)(vec->ptrs);
frame_vector.c:	for (i = 0; i < vec->nr_frames; i++)
frame_vector.c:	vec->is_pfns = true;
frame_vector.c: * frame_vector_create() - allocate & initialize structure for pinned pfns
frame_vector.c:	vec->nr_allocated = nr_frames;
frame_vector.c:	vec->nr_frames = 0;
frame_vector.c: * frame_vector_destroy() - free memory allocated to carry frame vector
frame_vector.c:	VM_BUG_ON(vec->nr_frames > 0);
memblock.c:	return *size = min(*size, (phys_addr_t)ULLONG_MAX - base);
memblock.c:	for (i = 0; i < type->cnt; i++)
memblock.c:		if (memblock_addrs_overlap(base, size, type->regions[i].base,
memblock.c:					   type->regions[i].size))
memblock.c:	return i < type->cnt;
memblock.c: * __memblock_find_range_bottom_up - find free area utility in bottom-up
memblock.c: * Utility called from memblock_find_in_range_node(), find free area bottom-up.
memblock.c:		if (cand < this_end && this_end - cand >= size)
memblock.c: * __memblock_find_range_top_down - find free area utility, in top-down
memblock.c: * Utility called from memblock_find_in_range_node(), find free area top-down.
memblock.c:		cand = round_down(this_end - size, align);
memblock.c: * memblock_find_in_range_node - find free area in given range and node
memblock.c: * When allocation direction is bottom-up, the @start should be greater
memblock.c: * reason is that we want the bottom-up allocation just near the kernel
memblock.c: * If bottom-up allocation failed, will try to allocate memory top-down.
memblock.c:	 * try bottom-up allocation only when bottom-up mode
memblock.c:		/* ok, try bottom-up allocation first */
memblock.c:		 * we always limit bottom-up allocation above the kernel,
memblock.c:		 * but top-down allocation doesn't have the limit, so
memblock.c:		 * retrying top-down allocation may succeed when bottom-up
memblock.c:		 * bottom-up allocation is expected to be fail very rarely,
memblock.c:		WARN_ONCE(1, "memblock: bottom-up allocation failed, memory hotunplug may be affected\n");
memblock.c: * memblock_find_in_range - find free area in given range
memblock.c:	type->total_size -= type->regions[r].size;
memblock.c:	memmove(&type->regions[r], &type->regions[r + 1],
memblock.c:		(type->cnt - (r + 1)) * sizeof(type->regions[r]));
memblock.c:	type->cnt--;
memblock.c:	if (type->cnt == 0) {
memblock.c:		WARN_ON(type->total_size != 0);
memblock.c:		type->cnt = 1;
memblock.c:		type->regions[0].base = 0;
memblock.c:		type->regions[0].size = 0;
memblock.c:		type->regions[0].flags = 0;
memblock.c:		memblock_set_region_node(&type->regions[0], MAX_NUMNODES);
memblock.c: * memblock_double_array - double the size of the memblock regions array
memblock.c: * 0 on success, -1 on failure.
memblock.c:		return -1;
memblock.c:	old_size = type->max * sizeof(struct memblock_region);
memblock.c:		       type->name, type->max, type->max * 2);
memblock.c:		return -1;
memblock.c:	memblock_dbg("memblock: %s is doubled to %ld at [%#010llx-%#010llx]",
memblock.c:			type->name, type->max * 2, (u64)addr,
memblock.c:			(u64)addr + new_size - 1);
memblock.c:	memcpy(new_array, type->regions, old_size);
memblock.c:	memset(new_array + type->max, 0, old_size);
memblock.c:	old_array = type->regions;
memblock.c:	type->regions = new_array;
memblock.c:	type->max <<= 1;
memblock.c: * memblock_merge_regions - merge neighboring compatible regions
memblock.c:	while (i < type->cnt - 1) {
memblock.c:		struct memblock_region *this = &type->regions[i];
memblock.c:		struct memblock_region *next = &type->regions[i + 1];
memblock.c:		if (this->base + this->size != next->base ||
memblock.c:		    this->flags != next->flags) {
memblock.c:			BUG_ON(this->base + this->size > next->base);
memblock.c:		this->size += next->size;
memblock.c:		memmove(next, next + 1, (type->cnt - (i + 2)) * sizeof(*next));
memblock.c:		type->cnt--;
memblock.c: * memblock_insert_region - insert new memblock region
memblock.c:	struct memblock_region *rgn = &type->regions[idx];
memblock.c:	BUG_ON(type->cnt >= type->max);
memblock.c:	memmove(rgn + 1, rgn, (type->cnt - idx) * sizeof(*rgn));
memblock.c:	rgn->base = base;
memblock.c:	rgn->size = size;
memblock.c:	rgn->flags = flags;
memblock.c:	type->cnt++;
memblock.c:	type->total_size += size;
memblock.c: * memblock_add_range - add new memblock region
memblock.c: * is allowed to overlap with existing ones - overlaps don't affect already
memblock.c: * 0 on success, -errno on failure.
memblock.c:	if (type->regions[0].size == 0) {
memblock.c:		WARN_ON(type->cnt != 1 || type->total_size);
memblock.c:		type->regions[0].base = base;
memblock.c:		type->regions[0].size = size;
memblock.c:		type->regions[0].flags = flags;
memblock.c:		memblock_set_region_node(&type->regions[0], nid);
memblock.c:		type->total_size = size;
memblock.c:		phys_addr_t rbase = rgn->base;
memblock.c:		phys_addr_t rend = rbase + rgn->size;
memblock.c:			WARN_ON(flags != rgn->flags);
memblock.c:						       rbase - base, nid,
memblock.c:			memblock_insert_region(type, idx, base, end - base,
memblock.c:		while (type->cnt + nr_new > type->max)
memblock.c:				return -ENOMEM;
memblock.c:	phys_addr_t end = base + size - 1;
memblock.c:	memblock_dbg("memblock_add: [%pa-%pa] %pF\n",
memblock.c: * memblock_isolate_range - isolate given range into disjoint memblocks
memblock.c: * 0 on success, -errno on failure.
memblock.c:	while (type->cnt + 2 > type->max)
memblock.c:			return -ENOMEM;
memblock.c:		phys_addr_t rbase = rgn->base;
memblock.c:		phys_addr_t rend = rbase + rgn->size;
memblock.c:			 * to process the next region - the new top half.
memblock.c:			rgn->base = base;
memblock.c:			rgn->size -= base - rbase;
memblock.c:			type->total_size -= base - rbase;
memblock.c:			memblock_insert_region(type, idx, rbase, base - rbase,
memblock.c:					       rgn->flags);
memblock.c:			 * current region - the new bottom half.
memblock.c:			rgn->base = end;
memblock.c:			rgn->size -= end - rbase;
memblock.c:			type->total_size -= end - rbase;
memblock.c:			memblock_insert_region(type, idx--, rbase, end - rbase,
memblock.c:					       rgn->flags);
memblock.c:	for (i = end_rgn - 1; i >= start_rgn; i--)
memblock.c:	phys_addr_t end = base + size - 1;
memblock.c:	memblock_dbg("   memblock_free: [%pa-%pa] %pF\n",
memblock.c:	phys_addr_t end = base + size - 1;
memblock.c:	memblock_dbg("memblock_reserve: [%pa-%pa] %pF\n",
memblock.c: * Return 0 on success, -errno on failure.
memblock.c:			memblock_set_region_flags(&type->regions[i], flag);
memblock.c:			memblock_clear_region_flags(&type->regions[i], flag);
memblock.c: * memblock_mark_hotplug - Mark hotpluggable memory with flag MEMBLOCK_HOTPLUG.
memblock.c: * Return 0 on success, -errno on failure.
memblock.c: * memblock_clear_hotplug - Clear flag MEMBLOCK_HOTPLUG for a specified region.
memblock.c: * Return 0 on success, -errno on failure.
memblock.c: * memblock_mark_mirror - Mark mirrored memory with flag MEMBLOCK_MIRROR.
memblock.c: * Return 0 on success, -errno on failure.
memblock.c: * memblock_mark_nomap - Mark a memory region with flag MEMBLOCK_NOMAP.
memblock.c: * Return 0 on success, -errno on failure.
memblock.c: * memblock_clear_nomap - Clear flag MEMBLOCK_NOMAP for a specified region.
memblock.c: * Return 0 on success, -errno on failure.
memblock.c: * __next_reserved_mem_region - next function for for_each_reserved_region()
memblock.c:	if (*idx < type->cnt) {
memblock.c:		struct memblock_region *r = &type->regions[*idx];
memblock.c:		phys_addr_t base = r->base;
memblock.c:		phys_addr_t size = r->size;
memblock.c:			*out_end = base + size - 1;
memblock.c: * __next__mem_range - next function for for_each_free_mem_range() etc.
memblock.c: *	0:[0-16), 1:[32-48), 2:[128-130)
memblock.c: *	0:[0-0), 1:[16-32), 2:[48-128), 3:[130-MAX)
memblock.c:	for (; idx_a < type_a->cnt; idx_a++) {
memblock.c:		struct memblock_region *m = &type_a->regions[idx_a];
memblock.c:		phys_addr_t m_start = m->base;
memblock.c:		phys_addr_t m_end = m->base + m->size;
memblock.c:		/* if we want mirror memory skip non-mirror memory regions */
memblock.c:		for (; idx_b < type_b->cnt + 1; idx_b++) {
memblock.c:			r = &type_b->regions[idx_b];
memblock.c:			r_start = idx_b ? r[-1].base + r[-1].size : 0;
memblock.c:			r_end = idx_b < type_b->cnt ?
memblock.c:				r->base : ULLONG_MAX;
memblock.c: * __next_mem_range_rev - generic next function for for_each_*_range_rev()
memblock.c:		idx_a = type_a->cnt - 1;
memblock.c:			idx_b = type_b->cnt;
memblock.c:	for (; idx_a >= 0; idx_a--) {
memblock.c:		struct memblock_region *m = &type_a->regions[idx_a];
memblock.c:		phys_addr_t m_start = m->base;
memblock.c:		phys_addr_t m_end = m->base + m->size;
memblock.c:		/* if we want mirror memory skip non-mirror memory regions */
memblock.c:			idx_a--;
memblock.c:		for (; idx_b >= 0; idx_b--) {
memblock.c:			r = &type_b->regions[idx_b];
memblock.c:			r_start = idx_b ? r[-1].base + r[-1].size : 0;
memblock.c:			r_end = idx_b < type_b->cnt ?
memblock.c:				r->base : ULLONG_MAX;
memblock.c:					idx_a--;
memblock.c:					idx_b--;
memblock.c:	while (++*idx < type->cnt) {
memblock.c:		r = &type->regions[*idx];
memblock.c:		if (PFN_UP(r->base) >= PFN_DOWN(r->base + r->size))
memblock.c:		if (nid == MAX_NUMNODES || nid == r->nid)
memblock.c:	if (*idx >= type->cnt) {
memblock.c:		*idx = -1;
memblock.c:		*out_start_pfn = PFN_UP(r->base);
memblock.c:		*out_end_pfn = PFN_DOWN(r->base + r->size);
memblock.c:		*out_nid = r->nid;
memblock.c: * memblock_set_node - set node ID on memblock regions
memblock.c: * 0 on success, -errno on failure.
memblock.c:		memblock_set_region_node(&type->regions[i], nid);
memblock.c: * memblock_virt_alloc_internal - allocate boot memory block
memblock.c: * memblock_virt_alloc_try_nid_nopanic - allocate boot memory block
memblock.c: * memblock_virt_alloc_try_nid - allocate boot memory block with panicking
memblock.c: * __memblock_free_early - free boot memory block
memblock.c:	memblock_dbg("%s: [%#016llx-%#016llx] %pF\n",
memblock.c:		     __func__, (u64)base, (u64)base + size - 1,
memblock.c: * __memblock_free_late - free bootmem block pages directly to buddy allocator
memblock.c:	memblock_dbg("%s: [%#016llx-%#016llx] %pF\n",
memblock.c:		     __func__, (u64)base, (u64)base + size - 1,
memblock.c:		pages += end_pfn - start_pfn;
memblock.c:	int idx = memblock.memory.cnt - 1;
memblock.c:		if (limit <= r->size) {
memblock.c:			max_addr = r->base + limit;
memblock.c:		limit -= r->size;
memblock.c:	for (i = memblock.memory.cnt - 1; i >= end_rgn; i--)
memblock.c:	for (i = start_rgn - 1; i >= 0; i--)
memblock.c:	unsigned int left = 0, right = type->cnt;
memblock.c:		if (addr < type->regions[mid].base)
memblock.c:		else if (addr >= (type->regions[mid].base +
memblock.c:				  type->regions[mid].size))
memblock.c:	return -1;
memblock.c:	return memblock_search(&memblock.reserved, addr) != -1;
memblock.c:	return memblock_search(&memblock.memory, addr) != -1;
memblock.c:	if (i == -1)
memblock.c:	if (mid == -1)
memblock.c:		return -1;
memblock.c:	*start_pfn = PFN_DOWN(type->regions[mid].base);
memblock.c:	*end_pfn = PFN_DOWN(type->regions[mid].base + type->regions[mid].size);
memblock.c:	return type->regions[mid].nid;
memblock.c: * memblock_is_region_memory - check if a region is a subset of memory
memblock.c: * 0 if false, non-zero if true
memblock.c:	if (idx == -1)
memblock.c: * memblock_is_region_reserved - check if a region intersects reserved memory
memblock.c:		orig_start = r->base;
memblock.c:		orig_end = r->base + r->size;
memblock.c:			r->base = start;
memblock.c:			r->size = end - start;
memblock.c:					       r - memblock.memory.regions);
memblock.c:			r--;
memblock.c:	pr_info(" %s.cnt  = 0x%lx\n", type->name, type->cnt);
memblock.c:		base = rgn->base;
memblock.c:		size = rgn->size;
memblock.c:		end = base + size - 1;
memblock.c:		flags = rgn->flags;
memblock.c:		pr_info(" %s[%#x]\t[%pa-%pa], %pa bytes%s flags: %#lx\n",
memblock.c:			type->name, idx, &base, &end, &size, nid_buf, flags);
memblock.c:		if (rgn->base + rgn->size < start_addr)
memblock.c:		if (rgn->base > end_addr)
memblock.c:		start = rgn->base;
memblock.c:		end = start + rgn->size;
memblock.c:		size += end - start;
memblock.c:	struct memblock_type *type = m->private;
memblock.c:	for (i = 0; i < type->cnt; i++) {
memblock.c:		reg = &type->regions[i];
memblock.c:		end = reg->base + reg->size - 1;
memblock.c:		seq_printf(m, "%pa..%pa\n", &reg->base, &end);
memblock.c:	return single_open(file, memblock_debug_show, inode->i_private);
memblock.c:		return -ENXIO;
interval_tree.c: * mm/interval_tree.c - interval tree for mapping->i_mmap
interval_tree.c:	return v->vm_pgoff;
interval_tree.c:	return v->vm_pgoff + ((v->vm_end - v->vm_start) >> PAGE_SHIFT) - 1;
interval_tree.c:	if (!prev->shared.rb.rb_right) {
interval_tree.c:		link = &prev->shared.rb.rb_right;
interval_tree.c:		parent = rb_entry(prev->shared.rb.rb_right,
interval_tree.c:		if (parent->shared.rb_subtree_last < last)
interval_tree.c:			parent->shared.rb_subtree_last = last;
interval_tree.c:		while (parent->shared.rb.rb_left) {
interval_tree.c:			parent = rb_entry(parent->shared.rb.rb_left,
interval_tree.c:			if (parent->shared.rb_subtree_last < last)
interval_tree.c:				parent->shared.rb_subtree_last = last;
interval_tree.c:		link = &parent->shared.rb.rb_left;
interval_tree.c:	node->shared.rb_subtree_last = last;
interval_tree.c:	rb_link_node(&node->shared.rb, &parent->shared.rb, link);
interval_tree.c:	rb_insert_augmented(&node->shared.rb, &root->rb_root,
interval_tree.c:	return vma_start_pgoff(avc->vma);
interval_tree.c:	return vma_last_pgoff(avc->vma);
interval_tree.c:	node->cached_vma_start = avc_start_pgoff(node);
interval_tree.c:	node->cached_vma_last = avc_last_pgoff(node);
interval_tree.c:	WARN_ON_ONCE(node->cached_vma_start != avc_start_pgoff(node));
interval_tree.c:	WARN_ON_ONCE(node->cached_vma_last != avc_last_pgoff(node));
z3fold.c: * struct z3fold_header - z3fold page metadata occupying first chunks of each
z3fold.c: * @page_lock:		per-page lock
z3fold.c:#define CHUNK_SHIFT	(PAGE_SHIFT - NCHUNKS_ORDER)
z3fold.c:#define NCHUNKS		((PAGE_SIZE - ZHDR_SIZE_ALIGNED) >> CHUNK_SHIFT)
z3fold.c: * struct z3fold_pool - stores metadata for each z3fold pool
z3fold.c: * @unbuddied:	per-cpu array of lists tracking z3fold pages that contain 2-
z3fold.c:	return (size + CHUNK_SIZE - 1) >> CHUNK_SHIFT;
z3fold.c:	INIT_LIST_HEAD(&page->lru);
z3fold.c:	clear_bit(PAGE_HEADLESS, &page->private);
z3fold.c:	clear_bit(MIDDLE_CHUNK_MAPPED, &page->private);
z3fold.c:	clear_bit(NEEDS_COMPACTING, &page->private);
z3fold.c:	clear_bit(PAGE_STALE, &page->private);
z3fold.c:	clear_bit(UNDER_RECLAIM, &page->private);
z3fold.c:	spin_lock_init(&zhdr->page_lock);
z3fold.c:	kref_init(&zhdr->refcount);
z3fold.c:	zhdr->first_chunks = 0;
z3fold.c:	zhdr->middle_chunks = 0;
z3fold.c:	zhdr->last_chunks = 0;
z3fold.c:	zhdr->first_num = 0;
z3fold.c:	zhdr->start_middle = 0;
z3fold.c:	zhdr->cpu = -1;
z3fold.c:	zhdr->pool = pool;
z3fold.c:	INIT_LIST_HEAD(&zhdr->buddy);
z3fold.c:	INIT_WORK(&zhdr->work, compact_page_work);
z3fold.c:	spin_lock(&zhdr->page_lock);
z3fold.c:	return spin_trylock(&zhdr->page_lock);
z3fold.c:	spin_unlock(&zhdr->page_lock);
z3fold.c:		handle += (bud + zhdr->first_num) & BUDDY_MASK;
z3fold.c: * (handle & BUDDY_MASK) < zhdr->first_num is possible in encode_handle
z3fold.c:	return (handle - zhdr->first_num) & BUDDY_MASK;
z3fold.c:	struct z3fold_pool *pool = zhdr->pool;
z3fold.c:	WARN_ON(!list_empty(&zhdr->buddy));
z3fold.c:	set_bit(PAGE_STALE, &page->private);
z3fold.c:	clear_bit(NEEDS_COMPACTING, &page->private);
z3fold.c:	spin_lock(&pool->lock);
z3fold.c:	if (!list_empty(&page->lru))
z3fold.c:		list_del(&page->lru);
z3fold.c:	spin_unlock(&pool->lock);
z3fold.c:	spin_lock(&pool->stale_lock);
z3fold.c:	list_add(&zhdr->buddy, &pool->stale);
z3fold.c:	queue_work(pool->release_wq, &pool->work);
z3fold.c:	spin_unlock(&pool->stale_lock);
z3fold.c:	spin_lock(&zhdr->pool->lock);
z3fold.c:	list_del_init(&zhdr->buddy);
z3fold.c:	spin_unlock(&zhdr->pool->lock);
z3fold.c:	spin_lock(&pool->stale_lock);
z3fold.c:	while (!list_empty(&pool->stale)) {
z3fold.c:		struct z3fold_header *zhdr = list_first_entry(&pool->stale,
z3fold.c:		list_del(&zhdr->buddy);
z3fold.c:		if (WARN_ON(!test_bit(PAGE_STALE, &page->private)))
z3fold.c:		spin_unlock(&pool->stale_lock);
z3fold.c:		cancel_work_sync(&zhdr->work);
z3fold.c:		spin_lock(&pool->stale_lock);
z3fold.c:	spin_unlock(&pool->stale_lock);
z3fold.c:	if (zhdr->middle_chunks != 0) {
z3fold.c:		int nfree_before = zhdr->first_chunks ?
z3fold.c:			0 : zhdr->start_middle - ZHDR_CHUNKS;
z3fold.c:		int nfree_after = zhdr->last_chunks ?
z3fold.c:			0 : TOTAL_CHUNKS -
z3fold.c:				(zhdr->start_middle + zhdr->middle_chunks);
z3fold.c:		nfree = NCHUNKS - zhdr->first_chunks - zhdr->last_chunks;
z3fold.c:		       beg + (zhdr->start_middle << CHUNK_SHIFT),
z3fold.c:		       zhdr->middle_chunks << CHUNK_SHIFT);
z3fold.c:	if (test_bit(MIDDLE_CHUNK_MAPPED, &page->private))
z3fold.c:	if (zhdr->middle_chunks == 0)
z3fold.c:	if (zhdr->first_chunks == 0 && zhdr->last_chunks == 0) {
z3fold.c:		zhdr->first_chunks = zhdr->middle_chunks;
z3fold.c:		zhdr->middle_chunks = 0;
z3fold.c:		zhdr->start_middle = 0;
z3fold.c:		zhdr->first_num++;
z3fold.c:	if (zhdr->first_chunks != 0 && zhdr->last_chunks == 0 &&
z3fold.c:	    zhdr->start_middle - (zhdr->first_chunks + ZHDR_CHUNKS) >=
z3fold.c:		mchunk_memmove(zhdr, zhdr->first_chunks + ZHDR_CHUNKS);
z3fold.c:		zhdr->start_middle = zhdr->first_chunks + ZHDR_CHUNKS;
z3fold.c:	} else if (zhdr->last_chunks != 0 && zhdr->first_chunks == 0 &&
z3fold.c:		   TOTAL_CHUNKS - (zhdr->last_chunks + zhdr->start_middle
z3fold.c:					+ zhdr->middle_chunks) >=
z3fold.c:		unsigned short new_start = TOTAL_CHUNKS - zhdr->last_chunks -
z3fold.c:			zhdr->middle_chunks;
z3fold.c:		zhdr->start_middle = new_start;
z3fold.c:	struct z3fold_pool *pool = zhdr->pool;
z3fold.c:	if (WARN_ON(!test_and_clear_bit(NEEDS_COMPACTING, &page->private))) {
z3fold.c:	spin_lock(&pool->lock);
z3fold.c:	list_del_init(&zhdr->buddy);
z3fold.c:	spin_unlock(&pool->lock);
z3fold.c:	if (kref_put(&zhdr->refcount, release_z3fold_page_locked)) {
z3fold.c:		atomic64_dec(&pool->pages_nr);
z3fold.c:	unbuddied = get_cpu_ptr(pool->unbuddied);
z3fold.c:	    (!zhdr->first_chunks || !zhdr->middle_chunks ||
z3fold.c:			!zhdr->last_chunks)) {
z3fold.c:		spin_lock(&pool->lock);
z3fold.c:		list_add(&zhdr->buddy, &unbuddied[fchunks]);
z3fold.c:		spin_unlock(&pool->lock);
z3fold.c:		zhdr->cpu = smp_processor_id();
z3fold.c:	put_cpu_ptr(pool->unbuddied);
z3fold.c: * z3fold_create_pool() - create a new z3fold pool
z3fold.c: * @ops:	user-defined operations for the z3fold pool
z3fold.c:	spin_lock_init(&pool->lock);
z3fold.c:	spin_lock_init(&pool->stale_lock);
z3fold.c:	pool->unbuddied = __alloc_percpu(sizeof(struct list_head)*NCHUNKS, 2);
z3fold.c:	if (!pool->unbuddied)
z3fold.c:				per_cpu_ptr(pool->unbuddied, cpu);
z3fold.c:	INIT_LIST_HEAD(&pool->lru);
z3fold.c:	INIT_LIST_HEAD(&pool->stale);
z3fold.c:	atomic64_set(&pool->pages_nr, 0);
z3fold.c:	pool->name = name;
z3fold.c:	pool->compact_wq = create_singlethread_workqueue(pool->name);
z3fold.c:	if (!pool->compact_wq)
z3fold.c:	pool->release_wq = create_singlethread_workqueue(pool->name);
z3fold.c:	if (!pool->release_wq)
z3fold.c:	INIT_WORK(&pool->work, free_pages_work);
z3fold.c:	pool->ops = ops;
z3fold.c:	destroy_workqueue(pool->compact_wq);
z3fold.c:	free_percpu(pool->unbuddied);
z3fold.c: * z3fold_destroy_pool() - destroys an existing z3fold pool
z3fold.c:	destroy_workqueue(pool->release_wq);
z3fold.c:	destroy_workqueue(pool->compact_wq);
z3fold.c: * z3fold_alloc() - allocates a region of a given size
z3fold.c: * Return: 0 if success and handle is set, otherwise -EINVAL if the size or
z3fold.c: * gfp arguments are invalid or -ENOMEM if the pool was unable to allocate
z3fold.c:		return -EINVAL;
z3fold.c:		return -ENOSPC;
z3fold.c:	if (size > PAGE_SIZE - ZHDR_SIZE_ALIGNED - CHUNK_SIZE)
z3fold.c:		unbuddied = get_cpu_ptr(pool->unbuddied);
z3fold.c:			/* Re-check under lock. */
z3fold.c:			spin_lock(&pool->lock);
z3fold.c:				spin_unlock(&pool->lock);
z3fold.c:				put_cpu_ptr(pool->unbuddied);
z3fold.c:			list_del_init(&zhdr->buddy);
z3fold.c:			zhdr->cpu = -1;
z3fold.c:			spin_unlock(&pool->lock);
z3fold.c:			if (test_bit(NEEDS_COMPACTING, &page->private)) {
z3fold.c:				put_cpu_ptr(pool->unbuddied);
z3fold.c:			kref_get(&zhdr->refcount);
z3fold.c:		put_cpu_ptr(pool->unbuddied);
z3fold.c:			if (zhdr->first_chunks == 0) {
z3fold.c:				if (zhdr->middle_chunks != 0 &&
z3fold.c:				    chunks >= zhdr->start_middle)
z3fold.c:			} else if (zhdr->last_chunks == 0)
z3fold.c:			else if (zhdr->middle_chunks == 0)
z3fold.c:				if (kref_put(&zhdr->refcount,
z3fold.c:					atomic64_dec(&pool->pages_nr);
z3fold.c:	spin_lock(&pool->stale_lock);
z3fold.c:	zhdr = list_first_entry_or_null(&pool->stale,
z3fold.c:	if (zhdr && (can_sleep || !work_pending(&zhdr->work))) {
z3fold.c:		list_del(&zhdr->buddy);
z3fold.c:		spin_unlock(&pool->stale_lock);
z3fold.c:			cancel_work_sync(&zhdr->work);
z3fold.c:		spin_unlock(&pool->stale_lock);
z3fold.c:		return -ENOMEM;
z3fold.c:	atomic64_inc(&pool->pages_nr);
z3fold.c:		set_bit(PAGE_HEADLESS, &page->private);
z3fold.c:		zhdr->first_chunks = chunks;
z3fold.c:		zhdr->last_chunks = chunks;
z3fold.c:		zhdr->middle_chunks = chunks;
z3fold.c:		zhdr->start_middle = zhdr->first_chunks + ZHDR_CHUNKS;
z3fold.c:	if (zhdr->first_chunks == 0 || zhdr->last_chunks == 0 ||
z3fold.c:			zhdr->middle_chunks == 0) {
z3fold.c:		struct list_head *unbuddied = get_cpu_ptr(pool->unbuddied);
z3fold.c:		spin_lock(&pool->lock);
z3fold.c:		list_add(&zhdr->buddy, &unbuddied[freechunks]);
z3fold.c:		spin_unlock(&pool->lock);
z3fold.c:		zhdr->cpu = smp_processor_id();
z3fold.c:		put_cpu_ptr(pool->unbuddied);
z3fold.c:	spin_lock(&pool->lock);
z3fold.c:	if (!list_empty(&page->lru))
z3fold.c:		list_del(&page->lru);
z3fold.c:	list_add(&page->lru, &pool->lru);
z3fold.c:	spin_unlock(&pool->lock);
z3fold.c: * z3fold_free() - frees the allocation associated with the given handle
z3fold.c:	if (test_bit(PAGE_HEADLESS, &page->private)) {
z3fold.c:			zhdr->first_chunks = 0;
z3fold.c:			zhdr->middle_chunks = 0;
z3fold.c:			zhdr->start_middle = 0;
z3fold.c:			zhdr->last_chunks = 0;
z3fold.c:		spin_lock(&pool->lock);
z3fold.c:		list_del(&page->lru);
z3fold.c:		spin_unlock(&pool->lock);
z3fold.c:		atomic64_dec(&pool->pages_nr);
z3fold.c:	if (kref_put(&zhdr->refcount, release_z3fold_page_locked_list)) {
z3fold.c:		atomic64_dec(&pool->pages_nr);
z3fold.c:	if (test_bit(UNDER_RECLAIM, &page->private)) {
z3fold.c:	if (test_and_set_bit(NEEDS_COMPACTING, &page->private)) {
z3fold.c:	if (zhdr->cpu < 0 || !cpu_online(zhdr->cpu)) {
z3fold.c:		spin_lock(&pool->lock);
z3fold.c:		list_del_init(&zhdr->buddy);
z3fold.c:		spin_unlock(&pool->lock);
z3fold.c:		zhdr->cpu = -1;
z3fold.c:		kref_get(&zhdr->refcount);
z3fold.c:	kref_get(&zhdr->refcount);
z3fold.c:	queue_work_on(zhdr->cpu, pool->compact_wq, &zhdr->work);
z3fold.c: * z3fold_reclaim_page() - evicts allocations from a pool page and frees it
z3fold.c: * call the user-defined eviction handler with the pool and handle as
z3fold.c: * non-zero. z3fold_reclaim_page() will add the z3fold page back to the
z3fold.c: * Returns: 0 if page is successfully freed, otherwise -EINVAL if there are
z3fold.c: * no pages to evict or an eviction handler is not registered, -EAGAIN if
z3fold.c:	spin_lock(&pool->lock);
z3fold.c:	if (!pool->ops || !pool->ops->evict || retries == 0) {
z3fold.c:		spin_unlock(&pool->lock);
z3fold.c:		return -EINVAL;
z3fold.c:		if (list_empty(&pool->lru)) {
z3fold.c:			spin_unlock(&pool->lock);
z3fold.c:			return -EINVAL;
z3fold.c:		list_for_each_prev(pos, &pool->lru) {
z3fold.c:			if (test_bit(PAGE_HEADLESS, &page->private))
z3fold.c:			kref_get(&zhdr->refcount);
z3fold.c:			list_del_init(&zhdr->buddy);
z3fold.c:			zhdr->cpu = -1;
z3fold.c:			set_bit(UNDER_RECLAIM, &page->private);
z3fold.c:		list_del_init(&page->lru);
z3fold.c:		spin_unlock(&pool->lock);
z3fold.c:		if (!test_bit(PAGE_HEADLESS, &page->private)) {
z3fold.c:			if (zhdr->first_chunks)
z3fold.c:			if (zhdr->middle_chunks)
z3fold.c:			if (zhdr->last_chunks)
z3fold.c:			ret = pool->ops->evict(pool, middle_handle);
z3fold.c:			ret = pool->ops->evict(pool, first_handle);
z3fold.c:			ret = pool->ops->evict(pool, last_handle);
z3fold.c:		if (test_bit(PAGE_HEADLESS, &page->private)) {
z3fold.c:			spin_lock(&pool->lock);
z3fold.c:			list_add(&page->lru, &pool->lru);
z3fold.c:			spin_unlock(&pool->lock);
z3fold.c:			clear_bit(UNDER_RECLAIM, &page->private);
z3fold.c:			if (kref_put(&zhdr->refcount,
z3fold.c:				atomic64_dec(&pool->pages_nr);
z3fold.c:			spin_lock(&pool->lock);
z3fold.c:			list_add(&page->lru, &pool->lru);
z3fold.c:			spin_unlock(&pool->lock);
z3fold.c:		spin_lock(&pool->lock);
z3fold.c:	spin_unlock(&pool->lock);
z3fold.c:	return -EAGAIN;
z3fold.c: * z3fold_map() - maps the allocation associated with the given handle
z3fold.c:	if (test_bit(PAGE_HEADLESS, &page->private))
z3fold.c:		addr += zhdr->start_middle << CHUNK_SHIFT;
z3fold.c:		set_bit(MIDDLE_CHUNK_MAPPED, &page->private);
z3fold.c:		addr += PAGE_SIZE - (zhdr->last_chunks << CHUNK_SHIFT);
z3fold.c: * z3fold_unmap() - unmaps the allocation associated with the given handle
z3fold.c:	if (test_bit(PAGE_HEADLESS, &page->private))
z3fold.c:		clear_bit(MIDDLE_CHUNK_MAPPED, &page->private);
z3fold.c: * z3fold_get_pool_size() - gets the z3fold pool size in pages
z3fold.c:	return atomic64_read(&pool->pages_nr);
z3fold.c:	if (pool->zpool && pool->zpool_ops && pool->zpool_ops->evict)
z3fold.c:		return pool->zpool_ops->evict(pool->zpool, handle);
z3fold.c:		return -ENOENT;
z3fold.c:		pool->zpool = zpool;
z3fold.c:		pool->zpool_ops = zpool_ops;
z3fold.c:	int ret = -EINVAL;
z3fold.c:MODULE_ALIAS("zpool-z3fold");
z3fold.c:MODULE_DESCRIPTION("3-Fold Allocator for Compressed Pages");
readahead.c: * mm/readahead.c - address_space-level file readahead.
readahead.c:#include <linux/backing-dev.h>
readahead.c:	ra->ra_pages = inode_to_bdi(mapping->host)->ra_pages;
readahead.c:	ra->prev_pos = -1;
readahead.c: * - the caller of read_cache_pages() may have set PG_private or PG_fscache
readahead.c:		page->mapping = mapping;
readahead.c:		page->mapping = NULL;
readahead.c:		list_del(&victim->lru);
readahead.c: * read_cache_pages - populate an address space with some pages & start reads against them
readahead.c: *   pages have their ->index populated and are otherwise uninitialised.
readahead.c:		list_del(&page->lru);
readahead.c:		if (add_to_page_cache_lru(page, mapping, page->index,
readahead.c:	if (mapping->a_ops->readpages) {
readahead.c:		ret = mapping->a_ops->readpages(filp, mapping, pages, nr_pages);
readahead.c:		list_del(&page->lru);
readahead.c:		if (!add_to_page_cache_lru(page, mapping, page->index, gfp))
readahead.c:			mapping->a_ops->readpage(filp, page);
readahead.c:	struct inode *inode = mapping->host;
readahead.c:	end_index = ((isize - 1) >> PAGE_SHIFT);
readahead.c:		page = radix_tree_lookup(&mapping->page_tree, page_offset);
readahead.c:		page->index = page_offset;
readahead.c:		list_add(&page->lru, &page_pool);
readahead.c:		if (page_idx == nr_to_read - lookahead_size)
readahead.c:	 * Now start the IO.  We ignore I/O errors - if the page is not
readahead.c:	struct backing_dev_info *bdi = inode_to_bdi(mapping->host);
readahead.c:	struct file_ra_state *ra = &filp->f_ra;
readahead.c:	if (unlikely(!mapping->a_ops->readpage && !mapping->a_ops->readpages))
readahead.c:		return -EINVAL;
readahead.c:	max_pages = max_t(unsigned long, bdi->io_pages, ra->ra_pages);
readahead.c:		nr_to_read -= this_chunk;
readahead.c: * 1-8 page = 32k initial, > 8 page = 128k initial
readahead.c:	unsigned long cur = ra->size;
readahead.c: * On-demand readahead design.
readahead.c: * The fields in struct file_ra_state represent the most-recently-executed
readahead.c: *                        |<----- async_size ---------|
readahead.c: *     |------------------- size -------------------->|
readahead.c: * page at (start+size-async_size) with PG_readahead, and use it as readahead
readahead.c: * readahead-for-nothing fuss, saving pointless page cache lookups.
readahead.c: * There is a special-case: if the first page which the application tries to
readahead.c: * Count contiguously cached pages from @offset-1 to @offset-@max,
readahead.c: * 	- length of the sequential read sequence, or
readahead.c: * 	- thrashing threshold in memory tight systems
readahead.c:	head = page_cache_prev_hole(mapping, offset - 1, max);
readahead.c:	return offset - 1 - head;
readahead.c: * page cache context based read-ahead
readahead.c:	 * it is a strong indication of long-run stream (or whole-file-read)
readahead.c:	ra->start = offset;
readahead.c:	ra->size = min(size + req_size, max);
readahead.c:	ra->async_size = 1;
readahead.c:	struct backing_dev_info *bdi = inode_to_bdi(mapping->host);
readahead.c:	unsigned long max_pages = ra->ra_pages;
readahead.c:	if (req_size > max_pages && bdi->io_pages > max_pages)
readahead.c:		max_pages = min(req_size, bdi->io_pages);
readahead.c:	if ((offset == (ra->start + ra->size - ra->async_size) ||
readahead.c:	     offset == (ra->start + ra->size))) {
readahead.c:		ra->start += ra->size;
readahead.c:		ra->size = get_next_ra_size(ra, max_pages);
readahead.c:		ra->async_size = ra->size;
readahead.c:		if (!start || start - offset > max_pages)
readahead.c:		ra->start = start;
readahead.c:		ra->size = start - offset;	/* old async_size */
readahead.c:		ra->size += req_size;
readahead.c:		ra->size = get_next_ra_size(ra, max_pages);
readahead.c:		ra->async_size = ra->size;
readahead.c:	 * trivial case: (offset - prev_offset) == 1
readahead.c:	 * unaligned reads: (offset - prev_offset) == 0
readahead.c:	prev_offset = (unsigned long long)ra->prev_pos >> PAGE_SHIFT;
readahead.c:	if (offset - prev_offset <= 1UL)
readahead.c:	ra->start = offset;
readahead.c:	ra->size = get_init_ra_size(req_size, max_pages);
readahead.c:	ra->async_size = ra->size > req_size ? ra->size - req_size : ra->size;
readahead.c:	if (offset == ra->start && ra->size == ra->async_size) {
readahead.c:		ra->async_size = get_next_ra_size(ra, max_pages);
readahead.c:		ra->size += ra->async_size;
readahead.c: * page_cache_sync_readahead - generic file readahead
readahead.c: * @filp: passed on to ->readpage() and ->readpages()
readahead.c: * @offset: start offset into @mapping, in pagecache page-sized units
readahead.c:	/* no read-ahead */
readahead.c:	if (!ra->ra_pages)
readahead.c:	if (filp && (filp->f_mode & FMODE_RANDOM)) {
readahead.c:	/* do read-ahead */
readahead.c: * page_cache_async_readahead - file readahead for marked pages
readahead.c: * @filp: passed on to ->readpage() and ->readpages()
readahead.c: * @offset: start offset into @mapping, in pagecache page-sized units
readahead.c:	/* no read-ahead */
readahead.c:	if (!ra->ra_pages)
readahead.c:	 * Defer asynchronous read-ahead on IO congestion.
readahead.c:	if (inode_read_congested(mapping->host))
readahead.c:	/* do read-ahead */
readahead.c:	if (!mapping || !mapping->a_ops)
readahead.c:		return -EINVAL;
readahead.c:	ret = -EBADF;
readahead.c:		if (f.file->f_mode & FMODE_READ) {
readahead.c:			struct address_space *mapping = f.file->f_mapping;
readahead.c:			pgoff_t end = (offset + count - 1) >> PAGE_SHIFT;
readahead.c:			unsigned long len = end - start + 1;
debug.c:// SPDX-License-Identifier: GPL-2.0
debug.c:	 * page->_mapcount space in struct page is used by sl[aou]b pages to
debug.c:		  page->mapping, page_to_pgoff(page));
debug.c:	pr_emerg("flags: %#lx(%pGp)\n", page->flags, &page->flags);
debug.c:	if (page->mem_cgroup)
debug.c:		pr_alert("page->mem_cgroup:%p\n", page->mem_cgroup);
debug.c:		vma, (void *)vma->vm_start, (void *)vma->vm_end, vma->vm_next,
debug.c:		vma->vm_prev, vma->vm_mm,
debug.c:		(unsigned long)pgprot_val(vma->vm_page_prot),
debug.c:		vma->anon_vma, vma->vm_ops, vma->vm_pgoff,
debug.c:		vma->vm_file, vma->vm_private_data,
debug.c:		vma->vm_flags, &vma->vm_flags);
debug.c:		mm, mm->mmap, mm->vmacache_seqnum, mm->task_size,
debug.c:		mm->get_unmapped_area,
debug.c:		mm->mmap_base, mm->mmap_legacy_base, mm->highest_vm_end,
debug.c:		mm->pgd, atomic_read(&mm->mm_users),
debug.c:		atomic_read(&mm->mm_count),
debug.c:		atomic_long_read((atomic_long_t *)&mm->nr_ptes),
debug.c:		mm->map_count,
debug.c:		mm->hiwater_rss, mm->hiwater_vm, mm->total_vm, mm->locked_vm,
debug.c:		mm->pinned_vm, mm->data_vm, mm->exec_vm, mm->stack_vm,
debug.c:		mm->start_code, mm->end_code, mm->start_data, mm->end_data,
debug.c:		mm->start_brk, mm->brk, mm->start_stack,
debug.c:		mm->arg_start, mm->arg_end, mm->env_start, mm->env_end,
debug.c:		mm->binfmt, mm->flags, mm->core_state,
debug.c:		mm->ioctx_table,
debug.c:		mm->owner,
debug.c:		mm->exe_file,
debug.c:		mm->mmu_notifier_mm,
debug.c:		mm->numa_next_scan, mm->numa_scan_offset, mm->numa_scan_seq,
debug.c:		atomic_read(&mm->tlb_flush_pending),
debug.c:		mm->def_flags, &mm->def_flags
migrate.c:// SPDX-License-Identifier: GPL-2.0
migrate.c: * Memory Migration functionality - linux/mm/migrate.c
migrate.c:#include <linux/backing-dev.h>
migrate.c:	 * being (wrongly) re-isolated while it is under migration,
migrate.c:	if (!mapping->a_ops->isolate_page(page, mode))
migrate.c:	/* Driver shouldn't use PG_isolated bit of page->flags */
migrate.c:	return -EBUSY;
migrate.c:	mapping->a_ops->putback_page(page);
migrate.c:		list_del(&page->lru);
migrate.c:		 * We isolated non-lru movable page so here we can use
migrate.c:					page_is_file_cache(page), -hpage_nr_pages(page));
migrate.c:			new = page - pvmw.page->index +
migrate.c:		/* PMD-mapped THP migration entry */
migrate.c:		pte = pte_mkold(mk_pte(new, READ_ONCE(vma->vm_page_prot)));
migrate.c:			set_huge_pte_at(vma->vm_mm, pvmw.address, pvmw.pte, pte);
migrate.c:			set_pte_at(vma->vm_mm, pvmw.address, pvmw.pte, pte);
migrate.c:		if (vma->vm_flags & VM_LOCKED && !PageTransCompound(new))
migrate.c:		/* No need to invalidate - it was non-present before */
migrate.c:	 * Once radix-tree replacement of page migration started, page_count
migrate.c:			bh = bh->b_this_page;
migrate.c:				bh = bh->b_this_page;
migrate.c:		bh = bh->b_this_page;
migrate.c:			return -EAGAIN;
migrate.c:		newpage->index = page->index;
migrate.c:		newpage->mapping = page->mapping;
migrate.c:	spin_lock_irq(&mapping->tree_lock);
migrate.c:	pslot = radix_tree_lookup_slot(&mapping->page_tree,
migrate.c:		radix_tree_deref_slot_protected(pslot, &mapping->tree_lock) != page) {
migrate.c:		spin_unlock_irq(&mapping->tree_lock);
migrate.c:		return -EAGAIN;
migrate.c:		spin_unlock_irq(&mapping->tree_lock);
migrate.c:		return -EAGAIN;
migrate.c:		spin_unlock_irq(&mapping->tree_lock);
migrate.c:		return -EAGAIN;
migrate.c:	newpage->index = page->index;
migrate.c:	newpage->mapping = page->mapping;
migrate.c:	radix_tree_replace_slot(&mapping->page_tree, pslot, newpage);
migrate.c:	page_ref_unfreeze(page, expected_count - 1);
migrate.c:	spin_unlock(&mapping->tree_lock);
migrate.c:		__dec_node_state(oldzone->zone_pgdat, NR_FILE_PAGES);
migrate.c:		__inc_node_state(newzone->zone_pgdat, NR_FILE_PAGES);
migrate.c:			__dec_node_state(oldzone->zone_pgdat, NR_SHMEM);
migrate.c:			__inc_node_state(newzone->zone_pgdat, NR_SHMEM);
migrate.c:			__dec_node_state(oldzone->zone_pgdat, NR_FILE_DIRTY);
migrate.c:			__inc_node_state(newzone->zone_pgdat, NR_FILE_DIRTY);
migrate.c:	spin_lock_irq(&mapping->tree_lock);
migrate.c:	pslot = radix_tree_lookup_slot(&mapping->page_tree,
migrate.c:		radix_tree_deref_slot_protected(pslot, &mapping->tree_lock) != page) {
migrate.c:		spin_unlock_irq(&mapping->tree_lock);
migrate.c:		return -EAGAIN;
migrate.c:		spin_unlock_irq(&mapping->tree_lock);
migrate.c:		return -EAGAIN;
migrate.c:	newpage->index = page->index;
migrate.c:	newpage->mapping = page->mapping;
migrate.c:	radix_tree_replace_slot(&mapping->page_tree, pslot, newpage);
migrate.c:	page_ref_unfreeze(page, expected_count - 1);
migrate.c:	spin_unlock_irq(&mapping->tree_lock);
migrate.c:	 * Copy NUMA information to the new page, to prevent over-eager
migrate.c:	cpupid = page_cpupid_xchg_last(page, -1);
migrate.c:	 * with an IRQ-safe spinlock held. In the sync case, the buffers
migrate.c:		bh = bh->b_this_page;
migrate.c:		bh = bh->b_this_page;
migrate.c:	if (!mapping->a_ops->writepage)
migrate.c:		return -EINVAL;
migrate.c:		return -EAGAIN;
migrate.c:	rc = mapping->a_ops->writepage(page, &wbc);
migrate.c:	return (rc < 0) ? -EIO : -EAGAIN;
migrate.c:			return -EBUSY;
migrate.c:		return -EAGAIN;
migrate.c: *   < 0 - error code
migrate.c: *  MIGRATEPAGE_SUCCESS - success
migrate.c:	int rc = -EAGAIN;
migrate.c:		else if (mapping->a_ops->migratepage)
migrate.c:			rc = mapping->a_ops->migratepage(mapping, newpage,
migrate.c:		 * In case of non-lru page, it could be released after
migrate.c:		rc = mapping->a_ops->migratepage(mapping, newpage,
migrate.c:	 * When successful, old pagecache page->mapping must be cleared before
migrate.c:		 * Anonymous and movable page->mapping will be cleard by
migrate.c:			page->mapping = NULL;
migrate.c:	int rc = -EAGAIN;
migrate.c:		if (current->flags & PF_MEMALLOC)
migrate.c:		 * the retry loop is too short and in the sync-light case,
migrate.c:			rc = -EBUSY;
migrate.c:	 * By try_to_unmap(), page->mapcount goes down to 0 here. In this case,
migrate.c:	 * 1. When a new swap-cache page is read into, it is added to the LRU
migrate.c:	 * Calling try_to_unmap() against a page->mapping==NULL page will
migrate.c:	 * fs-private metadata. The page can be picked up due to memory
migrate.c:	if (!page->mapping) {
migrate.c:		return -ENOMEM;
migrate.c:	if (rc != -EAGAIN) {
migrate.c:		list_del(&page->lru);
migrate.c:		 * Compaction can migrate also non-LRU pages which are
migrate.c:					page_is_file_cache(page), -hpage_nr_pages(page));
migrate.c:		if (rc != -EAGAIN) {
migrate.c:	int rc = -EAGAIN;
migrate.c:	 * tables or check whether the hugepage is pmd-based or not before
migrate.c:		return -ENOSYS;
migrate.c:		return -ENOMEM;
migrate.c:	if (rc != -EAGAIN)
migrate.c: * migrate_pages - migrate the pages specified in a list, to the free pages
migrate.c:	int swapwrite = current->flags & PF_SWAPWRITE;
migrate.c:		current->flags |= PF_SWAPWRITE;
migrate.c:			case -ENOMEM:
migrate.c:			case -EAGAIN:
migrate.c:				 * Permanent failure (-EBUSY, -ENOSYS, etc.):
migrate.c:				 * unlike -EAGAIN case, the failed page is
migrate.c:		current->flags &= ~PF_SWAPWRITE;
migrate.c:	while (pm->node != MAX_NUMNODES && pm->page != p)
migrate.c:	if (pm->node == MAX_NUMNODES)
migrate.c:	*result = &pm->status;
migrate.c:					pm->node);
migrate.c:		thp = alloc_pages_node(pm->node,
migrate.c:		return __alloc_pages_node(pm->node,
migrate.c:	down_read(&mm->mmap_sem);
migrate.c:	for (pp = pm; pp->node != MAX_NUMNODES; pp++) {
migrate.c:		err = -EFAULT;
migrate.c:		vma = find_vma(mm, pp->addr);
migrate.c:		if (!vma || pp->addr < vma->vm_start || !vma_migratable(vma))
migrate.c:		page = follow_page(vma, pp->addr, follflags);
migrate.c:		err = -ENOENT;
migrate.c:		if (err == pp->node)
migrate.c:		err = -EACCES;
migrate.c:				pp->page = page;
migrate.c:		pp->page = compound_head(page);
migrate.c:			list_add_tail(&head->lru, &pagelist);
migrate.c:		pp->status = err;
migrate.c:	up_read(&mm->mmap_sem);
migrate.c:	err = -ENOMEM;
migrate.c:	chunk_nr_pages = (PAGE_SIZE / sizeof(struct page_to_node)) - 1;
migrate.c:			chunk_nr_pages = nr_pages - chunk_start;
migrate.c:		/* fill the chunk pm with addrs and nodes from user-space */
migrate.c:			err = -EFAULT;
migrate.c:			err = -ENODEV;
migrate.c:			err = -EACCES;
migrate.c:				err = -EFAULT;
migrate.c:	down_read(&mm->mmap_sem);
migrate.c:		int err = -EFAULT;
migrate.c:		if (!vma || addr < vma->vm_start)
migrate.c:		err = page ? page_to_nid(page) : -ENOENT;
migrate.c:	up_read(&mm->mmap_sem);
migrate.c:		nr_pages -= chunk_nr;
migrate.c:	return nr_pages ? -EFAULT : 0;
migrate.c:		return -EINVAL;
migrate.c:		return -EPERM;
migrate.c:		return -ESRCH;
migrate.c:		err = -EPERM;
migrate.c:		return -EINVAL;
migrate.c:	for (z = pgdat->nr_zones - 1; z >= 0; z--) {
migrate.c:		struct zone *zone = pgdat->node_zones + z;
migrate.c:static unsigned int ratelimit_pages __read_mostly = 128 << (20 - PAGE_SHIFT);
migrate.c:/* Returns true if the node is migrate rate-limited after the update */
migrate.c:	 * Rate-limit the amount of data that is being migrated to a node.
migrate.c:	if (time_after(jiffies, pgdat->numabalancing_migrate_next_window)) {
migrate.c:		spin_lock(&pgdat->numabalancing_migrate_lock);
migrate.c:		pgdat->numabalancing_migrate_nr_pages = 0;
migrate.c:		pgdat->numabalancing_migrate_next_window = jiffies +
migrate.c:		spin_unlock(&pgdat->numabalancing_migrate_lock);
migrate.c:	if (pgdat->numabalancing_migrate_nr_pages > ratelimit_pages) {
migrate.c:		trace_mm_numa_migrate_ratelimit(current, pgdat->node_id,
migrate.c:	 * This is an unlocked non-atomic update so errors are possible.
migrate.c:	 * a problem, it can be converted to a per-cpu counter.
migrate.c:	pgdat->numabalancing_migrate_nr_pages += nr_pages;
migrate.c:	    (vma->vm_flags & VM_EXEC))
migrate.c:	 * Rate-limit the amount of data that is being migrated to a node.
migrate.c:	list_add(&page->lru, &migratepages);
migrate.c:			list_del(&page->lru);
migrate.c:	 * Rate-limit the amount of data that is being migrated to a node.
migrate.c:	/* anon mapping, we can simply copy page->mapping to the new page: */
migrate.c:	new_page->mapping = page->mapping;
migrate.c:	new_page->index = page->index;
migrate.c:			 NR_ISOLATED_ANON + page_lru, -HPAGE_PMD_NR);
migrate.c:	entry = mk_huge_pmd(new_page, vma->vm_page_prot);
migrate.c:			-HPAGE_PMD_NR);
migrate.c:		entry = pmd_modify(entry, vma->vm_page_prot);
migrate.c:	struct migrate_vma *migrate = walk->private;
migrate.c:		migrate->src[migrate->npages] = MIGRATE_PFN_MIGRATE;
migrate.c:		migrate->dst[migrate->npages] = 0;
migrate.c:		migrate->npages++;
migrate.c:		migrate->cpages++;
migrate.c:	struct migrate_vma *migrate = walk->private;
migrate.c:		migrate->dst[migrate->npages] = 0;
migrate.c:		migrate->src[migrate->npages++] = 0;
migrate.c:	struct migrate_vma *migrate = walk->private;
migrate.c:	struct vm_area_struct *vma = walk->vma;
migrate.c:	struct mm_struct *mm = vma->vm_mm;
migrate.c:			migrate->cpages++;
migrate.c:				migrate->cpages++;
migrate.c:			page = _vm_normal_page(migrate->vma, addr, pte, true);
migrate.c:		if (!page || !page->mapping || PageTransCompound(page)) {
migrate.c:		migrate->cpages++;
migrate.c:		migrate->dst[migrate->npages] = 0;
migrate.c:		migrate->src[migrate->npages++] = mpfn;
migrate.c:	pte_unmap_unlock(ptep - 1, ptl);
migrate.c:		flush_tlb_range(walk->vma, start, end);
migrate.c: * migrate_vma_collect() - collect pages over a range of virtual addresses
migrate.c:	mm_walk.vma = migrate->vma;
migrate.c:	mm_walk.mm = migrate->vma->vm_mm;
migrate.c:					    migrate->start,
migrate.c:					    migrate->end);
migrate.c:	walk_page_range(migrate->start, migrate->end, &mm_walk);
migrate.c:					  migrate->start,
migrate.c:					  migrate->end);
migrate.c:	migrate->end = migrate->start + (migrate->npages << PAGE_SHIFT);
migrate.c: * migrate_vma_check_page() - check if page is pinned or not
migrate.c:	if ((page_count(page) - extra) > page_mapcount(page))
migrate.c: * migrate_vma_prepare() - lock pages and isolate them from the lru
migrate.c: * page is locked it is isolated from the lru (for non-device pages). Finally,
migrate.c:	const unsigned long npages = migrate->npages;
migrate.c:	const unsigned long start = migrate->start;
migrate.c:	for (i = 0; (i < npages) && migrate->cpages; i++) {
migrate.c:		struct page *page = migrate_pfn_to_page(migrate->src[i]);
migrate.c:		if (!(migrate->src[i] & MIGRATE_PFN_LOCKED)) {
migrate.c:				migrate->src[i] = 0;
migrate.c:				migrate->cpages--;
migrate.c:			migrate->src[i] |= MIGRATE_PFN_LOCKED;
migrate.c:					migrate->src[i] &= ~MIGRATE_PFN_MIGRATE;
migrate.c:					migrate->cpages--;
migrate.c:					migrate->src[i] = 0;
migrate.c:					migrate->cpages--;
migrate.c:				migrate->src[i] &= ~MIGRATE_PFN_MIGRATE;
migrate.c:				migrate->cpages--;
migrate.c:				migrate->src[i] = 0;
migrate.c:				migrate->cpages--;
migrate.c:		struct page *page = migrate_pfn_to_page(migrate->src[i]);
migrate.c:		if (!page || (migrate->src[i] & MIGRATE_PFN_MIGRATE))
migrate.c:		remove_migration_pte(page, migrate->vma, addr, page);
migrate.c:		migrate->src[i] = 0;
migrate.c:		restore--;
migrate.c: * migrate_vma_unmap() - replace page mapping with special migration pte entry
migrate.c:	const unsigned long npages = migrate->npages;
migrate.c:	const unsigned long start = migrate->start;
migrate.c:		struct page *page = migrate_pfn_to_page(migrate->src[i]);
migrate.c:		if (!page || !(migrate->src[i] & MIGRATE_PFN_MIGRATE))
migrate.c:		migrate->src[i] &= ~MIGRATE_PFN_MIGRATE;
migrate.c:		migrate->cpages--;
migrate.c:		struct page *page = migrate_pfn_to_page(migrate->src[i]);
migrate.c:		if (!page || (migrate->src[i] & MIGRATE_PFN_MIGRATE))
migrate.c:		migrate->src[i] = 0;
migrate.c:		restore--;
migrate.c:	struct vm_area_struct *vma = migrate->vma;
migrate.c:	struct mm_struct *mm = vma->vm_mm;
migrate.c:	if (mem_cgroup_try_charge(page, vma->vm_mm, GFP_KERNEL, &memcg, false))
migrate.c:			swp_entry = make_device_private_entry(page, vma->vm_flags & VM_WRITE);
migrate.c:			entry = pte_mkold(mk_pte(page, READ_ONCE(vma->vm_page_prot)));
migrate.c:			if (vma->vm_flags & VM_WRITE)
migrate.c:		entry = mk_pte(page, vma->vm_page_prot);
migrate.c:		if (vma->vm_flags & VM_WRITE)
migrate.c:		/* No need to invalidate - it was non-present before */
migrate.c: * migrate_vma_pages() - migrate meta-data from src page to dst page
migrate.c: * This migrates struct page meta-data from source struct page to destination
migrate.c:	const unsigned long npages = migrate->npages;
migrate.c:	const unsigned long start = migrate->start;
migrate.c:	struct vm_area_struct *vma = migrate->vma;
migrate.c:	struct mm_struct *mm = vma->vm_mm;
migrate.c:		struct page *newpage = migrate_pfn_to_page(migrate->dst[i]);
migrate.c:		struct page *page = migrate_pfn_to_page(migrate->src[i]);
migrate.c:			migrate->src[i] &= ~MIGRATE_PFN_MIGRATE;
migrate.c:			if (!(migrate->src[i] & MIGRATE_PFN_MIGRATE)) {
migrate.c:								migrate->end);
migrate.c:						&migrate->src[i],
migrate.c:						&migrate->dst[i]);
migrate.c:				 * migrating to un-addressable device memory.
migrate.c:					migrate->src[i] &= ~MIGRATE_PFN_MIGRATE;
migrate.c:				migrate->src[i] &= ~MIGRATE_PFN_MIGRATE;
migrate.c:			migrate->src[i] &= ~MIGRATE_PFN_MIGRATE;
migrate.c:						  migrate->end);
migrate.c: * migrate_vma_finalize() - restore CPU page table entry
migrate.c:	const unsigned long npages = migrate->npages;
migrate.c:		struct page *newpage = migrate_pfn_to_page(migrate->dst[i]);
migrate.c:		struct page *page = migrate_pfn_to_page(migrate->src[i]);
migrate.c:		if (!(migrate->src[i] & MIGRATE_PFN_MIGRATE) || !newpage) {
migrate.c:		migrate->cpages--;
migrate.c: * migrate_vma() - migrate a range of memory inside vma
migrate.c: * Both src and dst array must be big enough for (end - start) >> PAGE_SHIFT
migrate.c:	if (!vma || is_vm_hugetlb_page(vma) || (vma->vm_flags & VM_SPECIAL))
migrate.c:		return -EINVAL;
migrate.c:	if (start < vma->vm_start || start >= vma->vm_end)
migrate.c:		return -EINVAL;
migrate.c:	if (end <= vma->vm_start || end > vma->vm_end)
migrate.c:		return -EINVAL;
migrate.c:		return -EINVAL;
migrate.c:	memset(src, 0, sizeof(*src) * ((end - start) >> PAGE_SHIFT));
migrate.c:	ops->alloc_and_copy(vma, src, dst, start, end, private);
migrate.c:	ops->finalize_and_map(vma, src, dst, start, end, private);
slab.c:// SPDX-License-Identifier: GPL-2.0
slab.c: * kmem_cache_destroy() + some cleanup - 1999 Andrea Arcangeli
slab.c: * Major cleanup, different bufctl logic, per-cpu arrays
slab.c: *	Pub: Prentice Hall	ISBN 0-13-101908-2
slab.c: *	The Slab Allocator: An Object-Caching Kernel Memory Allocator
slab.c: * Each cache has a short per-cpu head array, most allocs
slab.c: * The c_cpuarray may not be read with enabled local interrupts -
slab.c: *  The per-cpu arrays are never accessed from the wrong cpu, no locking,
slab.c: *  	and local interrupts are disabled so slab code is preempt-safe.
slab.c: *  The non-constant members are protected with a per-cache irq spinlock.
slab.c: * Many thanks to Mark Hemment, who wrote another per-cpu slab patch
slab.c: * in 2000 - many ideas in the current implementation are derived from
slab.c: * 11 April '97.  Started multi-threading - markhe
slab.c: *	The global cache-chain is protected by the mutex 'slab_mutex'.
slab.c: *	The sem is only needed when accessing/extending the cache-chain, which
slab.c:#include	<linux/fault-inject.h>
slab.c: * DEBUG	- 1 for kmem_cache_create() to honour; SLAB_RED_ZONE & SLAB_POISON.
slab.c: * STATS	- 1 to collect stats for /proc/slabinfo.
slab.c: * FORCED_DEBUG	- 1 enables SLAB_RED_ZONE and SLAB_POISON (if possible)
slab.c:#define SLAB_OBJ_MAX_NUM ((1 << sizeof(freelist_idx_t) * BITS_PER_BYTE) - 1)
slab.c: * - LIFO ordering, to hand out cache-warm objects from _alloc
slab.c: * - reduce the number of linked list operations
slab.c: * - reduce spinlock operations
slab.c: * The limit is stored in the per-cpu structure to reduce the data cache
slab.c:	INIT_LIST_HEAD(&parent->slabs_full);
slab.c:	INIT_LIST_HEAD(&parent->slabs_partial);
slab.c:	INIT_LIST_HEAD(&parent->slabs_free);
slab.c:	parent->total_slabs = 0;
slab.c:	parent->free_slabs = 0;
slab.c:	parent->shared = NULL;
slab.c:	parent->alien = NULL;
slab.c:	parent->colour_next = 0;
slab.c:	spin_lock_init(&parent->list_lock);
slab.c:	parent->free_objects = 0;
slab.c:	parent->free_touched = 0;
slab.c:		list_splice(&get_node(cachep, nodeid)->slab, listp);	\
slab.c:	MAKE_LIST((cachep), (&(ptr)->slabs_full), slabs_full, nodeid);	\
slab.c:	MAKE_LIST((cachep), (&(ptr)->slabs_partial), slabs_partial, nodeid); \
slab.c:	MAKE_LIST((cachep), (&(ptr)->slabs_free), slabs_free, nodeid);	\
slab.c:#define	OBJFREELIST_SLAB(x)	((x)->flags & CFLGS_OBJFREELIST_SLAB)
slab.c:#define	OFF_SLAB(x)	((x)->flags & CFLGS_OFF_SLAB)
slab.c:#define	STATS_INC_ACTIVE(x)	((x)->num_active++)
slab.c:#define	STATS_DEC_ACTIVE(x)	((x)->num_active--)
slab.c:#define	STATS_INC_ALLOCED(x)	((x)->num_allocations++)
slab.c:#define	STATS_INC_GROWN(x)	((x)->grown++)
slab.c:#define	STATS_ADD_REAPED(x,y)	((x)->reaped += (y))
slab.c:		if ((x)->num_active > (x)->high_mark)			\
slab.c:			(x)->high_mark = (x)->num_active;		\
slab.c:#define	STATS_INC_ERR(x)	((x)->errors++)
slab.c:#define	STATS_INC_NODEALLOCS(x)	((x)->node_allocs++)
slab.c:#define	STATS_INC_NODEFREES(x)	((x)->node_frees++)
slab.c:#define STATS_INC_ACOVERFLOW(x)   ((x)->node_overflow++)
slab.c:		if ((x)->max_freeable < i)				\
slab.c:			(x)->max_freeable = i;				\
slab.c:#define STATS_INC_ALLOCHIT(x)	atomic_inc(&(x)->allochit)
slab.c:#define STATS_INC_ALLOCMISS(x)	atomic_inc(&(x)->allocmiss)
slab.c:#define STATS_INC_FREEHIT(x)	atomic_inc(&(x)->freehit)
slab.c:#define STATS_INC_FREEMISS(x)	atomic_inc(&(x)->freemiss)
slab.c: * 0 .. cachep->obj_offset - BYTES_PER_WORD - 1: padding. This ensures that
slab.c: * cachep->obj_offset - BYTES_PER_WORD .. cachep->obj_offset - 1:
slab.c: * cachep->obj_offset: The real object.
slab.c: * cachep->size - 2* BYTES_PER_WORD: redzone word [BYTES_PER_WORD long]
slab.c: * cachep->size - 1* BYTES_PER_WORD: last caller address
slab.c:	return cachep->obj_offset;
slab.c:	BUG_ON(!(cachep->flags & SLAB_RED_ZONE));
slab.c:	return (unsigned long long*) (objp + obj_offset(cachep) -
slab.c:	BUG_ON(!(cachep->flags & SLAB_RED_ZONE));
slab.c:	if (cachep->flags & SLAB_STORE_USER)
slab.c:		return (unsigned long long *)(objp + cachep->size -
slab.c:					      sizeof(unsigned long long) -
slab.c:	return (unsigned long long *) (objp + cachep->size -
slab.c:	BUG_ON(!(cachep->flags & SLAB_STORE_USER));
slab.c:	return (void **)(objp + cachep->size - BYTES_PER_WORD);
slab.c:	return atomic_read(&cachep->store_user_clean) == 1;
slab.c:	atomic_set(&cachep->store_user_clean, 1);
slab.c:		atomic_set(&cachep->store_user_clean, 0);
slab.c:	return page->slab_cache;
slab.c:	return page->s_mem + cache->size * idx;
slab.c: * We want to avoid an expensive divide : (offset / cache->size)
slab.c: *   we can replace (offset / cache->size) by
slab.c: *   reciprocal_divide(offset, cache->reciprocal_buffer_size)
slab.c:	u32 offset = (obj - page->s_mem);
slab.c:	return reciprocal_divide(offset, cache->reciprocal_buffer_size);
slab.c:	return this_cpu_ptr(cachep->cpu_cache);
slab.c: * Calculate the number of objects and left-over bytes for a given buffer size.
slab.c:	 * - @buffer_size bytes for each object
slab.c:	 * - One freelist_idx_t for each object
slab.c:	       function, cachep->name, msg);
slab.c:				min(slab_max_order, MAX_ORDER - 1);
slab.c:	if (reap_work->work.func == NULL) {
slab.c:		ac->avail = 0;
slab.c:		ac->limit = limit;
slab.c:		ac->batchcount = batch;
slab.c:		ac->touched = 0;
slab.c:	spin_lock(&n->list_lock);
slab.c:	spin_unlock(&n->list_lock);
slab.c:	int nr = min3(from->avail, max, to->limit - to->avail);
slab.c:	memcpy(to->entry + to->avail, from->entry + from->avail -nr,
slab.c:	from->avail -= nr;
slab.c:	to->avail += nr;
slab.c:	init_arraycache(&alc->ac, entries, batch);
slab.c:	spin_lock_init(&alc->lock);
slab.c:			for (i--; i >= 0; i--)
slab.c:	if (ac->avail) {
slab.c:		spin_lock(&n->list_lock);
slab.c:		if (n->shared)
slab.c:			transfer_objects(n->shared, ac, ac->limit);
slab.c:		free_block(cachep, ac->entry, ac->avail, node, list);
slab.c:		ac->avail = 0;
slab.c:		spin_unlock(&n->list_lock);
slab.c:	if (n->alien) {
slab.c:		struct alien_cache *alc = n->alien[node];
slab.c:			ac = &alc->ac;
slab.c:			if (ac->avail && spin_trylock_irq(&alc->lock)) {
slab.c:				spin_unlock_irq(&alc->lock);
slab.c:			ac = &alc->ac;
slab.c:			spin_lock_irqsave(&alc->lock, flags);
slab.c:			spin_unlock_irqrestore(&alc->lock, flags);
slab.c:	if (n->alien && n->alien[page_node]) {
slab.c:		alien = n->alien[page_node];
slab.c:		ac = &alien->ac;
slab.c:		spin_lock(&alien->lock);
slab.c:		if (unlikely(ac->avail == ac->limit)) {
slab.c:		ac->entry[ac->avail++] = objp;
slab.c:		spin_unlock(&alien->lock);
slab.c:		spin_lock(&n->list_lock);
slab.c:		spin_unlock(&n->list_lock);
slab.c:		spin_lock_irq(&n->list_lock);
slab.c:		n->free_limit = (1 + nr_cpus_node(node)) * cachep->batchcount +
slab.c:				cachep->num;
slab.c:		spin_unlock_irq(&n->list_lock);
slab.c:		return -ENOMEM;
slab.c:	n->next_reap = jiffies + REAPTIMEOUT_NODE +
slab.c:	n->free_limit =
slab.c:		(1 + nr_cpus_node(node)) * cachep->batchcount + cachep->num;
slab.c:	cachep->node[node] = n;
slab.c: * either memory or cpu hotplug.  If memory is being hot-added, the kmem_cache_node
slab.c: * will be allocated off-node since memory is not yet online for the new node.
slab.c:	int ret = -ENOMEM;
slab.c:		new_alien = alloc_alien_cache(node, cachep->limit, gfp);
slab.c:	if (cachep->shared) {
slab.c:			cachep->shared * cachep->batchcount, 0xbaadf00d, gfp);
slab.c:	spin_lock_irq(&n->list_lock);
slab.c:	if (n->shared && force_change) {
slab.c:		free_block(cachep, n->shared->entry,
slab.c:				n->shared->avail, node, &list);
slab.c:		n->shared->avail = 0;
slab.c:	if (!n->shared || force_change) {
slab.c:		old_shared = n->shared;
slab.c:		n->shared = new_shared;
slab.c:	if (!n->alien) {
slab.c:		n->alien = new_alien;
slab.c:	spin_unlock_irq(&n->list_lock);
slab.c:	 * To protect lockless access to n->shared during irq disabled context.
slab.c:	 * If n->shared isn't NULL in irq disabled context, accessing to it is
slab.c:	 * guaranteed to be valid until irq is re-enabled, because it will be
slab.c:		spin_lock_irq(&n->list_lock);
slab.c:		n->free_limit -= cachep->batchcount;
slab.c:		nc = per_cpu_ptr(cachep->cpu_cache, cpu);
slab.c:			free_block(cachep, nc->entry, nc->avail, node, &list);
slab.c:			nc->avail = 0;
slab.c:			spin_unlock_irq(&n->list_lock);
slab.c:		shared = n->shared;
slab.c:			free_block(cachep, shared->entry,
slab.c:				   shared->avail, node, &list);
slab.c:			n->shared = NULL;
slab.c:		alien = n->alien;
slab.c:		n->alien = NULL;
slab.c:		spin_unlock_irq(&n->list_lock);
slab.c:	return -ENOMEM;
slab.c: * Drains freelist for a node on each slab cache, used for memory hot-remove.
slab.c: * Returns -EBUSY if all objects cannot be drained so that the node is not
slab.c:		if (!list_empty(&n->slabs_full) ||
slab.c:		    !list_empty(&n->slabs_partial)) {
slab.c:			ret = -EBUSY;
slab.c:	nid = mnb->status_change_nid;
slab.c:	spin_lock_init(&ptr->list_lock);
slab.c:	cachep->node[nodeid] = ptr;
slab.c:		cachep->node[node] = &init_kmem_cache_node[index + node];
slab.c:		cachep->node[node]->next_reap = jiffies +
slab.c:	BUILD_BUG_ON(sizeof(((struct page *)NULL)->lru) <
slab.c:	 * Fragmentation resistance on low memory - only use bigger
slab.c:	list_add(&kmem_cache->list, &slab_caches);
slab.c:		cachep->name, cachep->size, cachep->gfporder);
slab.c:		spin_lock_irqsave(&n->list_lock, flags);
slab.c:		total_slabs = n->total_slabs;
slab.c:		free_slabs = n->free_slabs;
slab.c:		free_objs = n->free_objects;
slab.c:		spin_unlock_irqrestore(&n->list_lock, flags);
slab.c:			node, total_slabs - free_slabs, total_slabs,
slab.c:			(total_slabs * cachep->num) - free_objs,
slab.c:			total_slabs * cachep->num);
slab.c: * kmem_cache_node ->list_lock.
slab.c:	flags |= cachep->allocflags;
slab.c:	if (cachep->flags & SLAB_RECLAIM_ACCOUNT)
slab.c:	page = __alloc_pages_node(nodeid, flags, cachep->gfporder);
slab.c:	if (memcg_charge_slab(page, flags, cachep->gfporder, cachep)) {
slab.c:		__free_pages(page, cachep->gfporder);
slab.c:	nr_pages = (1 << cachep->gfporder);
slab.c:	if (cachep->flags & SLAB_RECLAIM_ACCOUNT)
slab.c:	int order = cachep->gfporder;
slab.c:	if (cachep->flags & SLAB_RECLAIM_ACCOUNT)
slab.c:		mod_lruvec_page_state(page, NR_SLAB_RECLAIMABLE, -nr_freed);
slab.c:		mod_lruvec_page_state(page, NR_SLAB_UNRECLAIMABLE, -nr_freed);
slab.c:	page->mapping = NULL;
slab.c:	if (current->reclaim_state)
slab.c:		current->reclaim_state->reclaimed_slab += nr_freed;
slab.c:	cachep = page->slab_cache;
slab.c:		(cachep->size % PAGE_SIZE) == 0)
slab.c:	int size = cachep->object_size;
slab.c:	size -= 3 * sizeof(unsigned long);
slab.c:				size -= sizeof(unsigned long);
slab.c:	kernel_map_pages(virt_to_page(objp), cachep->size / PAGE_SIZE, map);
slab.c:	int size = cachep->object_size;
slab.c:	*(unsigned char *)(addr + size - 1) = POISON_END;
slab.c:		if (!(error & (error - 1))) {
slab.c:	if (cachep->flags & SLAB_RED_ZONE) {
slab.c:	if (cachep->flags & SLAB_STORE_USER) {
slab.c:	size = cachep->object_size;
slab.c:	for (i = 0; i < size && lines; i += 16, lines--) {
slab.c:			limit = size - i;
slab.c:	size = cachep->object_size;
slab.c:		if (i == size - 1)
slab.c:				       print_tainted(), cachep->name,
slab.c:				limit = size - i;
slab.c:			objp = index_to_obj(cachep, page, objnr - 1);
slab.c:		if (objnr + 1 < cachep->num) {
slab.c:	if (OBJFREELIST_SLAB(cachep) && cachep->flags & SLAB_POISON) {
slab.c:		poison_obj(cachep, page->freelist - obj_offset(cachep),
slab.c:	for (i = 0; i < cachep->num; i++) {
slab.c:		if (cachep->flags & SLAB_POISON) {
slab.c:		if (cachep->flags & SLAB_RED_ZONE) {
slab.c: * slab_destroy - destroy and release all objects in a slab
slab.c: * kmem_cache_node ->list_lock is not held/needed.
slab.c:	freelist = page->freelist;
slab.c:	if (unlikely(cachep->flags & SLAB_TYPESAFE_BY_RCU))
slab.c:		call_rcu(&page->rcu_head, kmem_rcu_free);
slab.c:		kmem_cache_free(cachep->freelist_cache, freelist);
slab.c:		list_del(&page->lru);
slab.c: * calculate_slab_order - calculate size (page order) of slabs
slab.c: * towards high-order requests, this should be changed.
slab.c:			if (freelist_cache->size > cachep->size / 2)
slab.c:		/* Found something acceptable - save it away */
slab.c:		cachep->num = num;
slab.c:		cachep->gfporder = gfporder;
slab.c:		 * A VFS-reclaimable slab tends to have most allocations
slab.c:		 * higher-order pages when we are unable to shrink dcache.
slab.c:	cachep->cpu_cache = alloc_kmem_cache_cpus(cachep, 1, 1);
slab.c:	if (!cachep->cpu_cache)
slab.c:			cachep->node[node] = kmalloc_node(
slab.c:			BUG_ON(!cachep->node[node]);
slab.c:			kmem_cache_node_init(cachep->node[node]);
slab.c:	cachep->node[numa_mem_id()]->next_reap =
slab.c:	cpu_cache_get(cachep)->avail = 0;
slab.c:	cpu_cache_get(cachep)->limit = BOOT_CPUCACHE_ENTRIES;
slab.c:	cpu_cache_get(cachep)->batchcount = 1;
slab.c:	cpu_cache_get(cachep)->touched = 0;
slab.c:	cachep->batchcount = 1;
slab.c:	cachep->limit = BOOT_CPUCACHE_ENTRIES;
slab.c:		cachep->refcount++;
slab.c:		cachep->object_size = max_t(int, cachep->object_size, size);
slab.c:	cachep->num = 0;
slab.c:	if (cachep->ctor || flags & SLAB_TYPESAFE_BY_RCU)
slab.c:	if (!cachep->num)
slab.c:	if (cachep->num * sizeof(freelist_idx_t) > cachep->object_size)
slab.c:	cachep->colour = left / cachep->colour_off;
slab.c:	cachep->num = 0;
slab.c:	 * Always use on-slab management when SLAB_NOLEAKTRACE
slab.c:	 * off-slab (should allow better packing of objs).
slab.c:	if (!cachep->num)
slab.c:	 * If the slab has been placed off-slab, and we have enough space then
slab.c:	 * move it on-slab. This is at the expense of any extra colouring.
slab.c:	if (left >= cachep->num * sizeof(freelist_idx_t))
slab.c:	cachep->colour = left / cachep->colour_off;
slab.c:	cachep->num = 0;
slab.c:	if (!cachep->num)
slab.c:	cachep->colour = left / cachep->colour_off;
slab.c: * __kmem_cache_create - Create a cache.
slab.c: * %SLAB_POISON - Poison the slab with a known test pattern (a5a5a5a5)
slab.c: * %SLAB_RED_ZONE - Insert `Red' zones around the allocated memory to check
slab.c: * %SLAB_HWCACHE_ALIGN - Align the objects in this cache to a hardware
slab.c:	size_t size = cachep->size;
slab.c:	if (size < 4096 || fls(size - 1) == fls(size-1 + REDZONE_ALIGN +
slab.c:	 * sure any on-slab bufctl's are also correctly aligned.
slab.c:	if (ralign < cachep->align) {
slab.c:		ralign = cachep->align;
slab.c:	cachep->align = ralign;
slab.c:	cachep->colour_off = cache_line_size();
slab.c:	if (cachep->colour_off < cachep->align)
slab.c:		cachep->colour_off = cachep->align;
slab.c:	 * Both debugging options require word-alignment which is calculated
slab.c:		cachep->obj_offset += sizeof(unsigned long long);
slab.c:	size = ALIGN(size, cachep->align);
slab.c:		size = ALIGN(SLAB_OBJ_MIN_SIZE, cachep->align);
slab.c:	 * To activate debug pagealloc, off-slab management is necessary
slab.c:		size >= 256 && cachep->object_size > cache_line_size()) {
slab.c:				cachep->obj_offset += tmp_size - size;
slab.c:	return -E2BIG;
slab.c:	cachep->freelist_size = cachep->num * sizeof(freelist_idx_t);
slab.c:	cachep->flags = flags;
slab.c:	cachep->allocflags = __GFP_COMP;
slab.c:		cachep->allocflags |= GFP_DMA;
slab.c:	cachep->size = size;
slab.c:	cachep->reciprocal_buffer_size = reciprocal_value(size);
slab.c:		(cachep->flags & SLAB_POISON) &&
slab.c:		cachep->flags &= ~(SLAB_RED_ZONE | SLAB_STORE_USER);
slab.c:		cachep->freelist_cache =
slab.c:			kmalloc_slab(cachep->freelist_size, 0u);
slab.c:	assert_spin_locked(&get_node(cachep, numa_mem_id())->list_lock);
slab.c:	assert_spin_locked(&get_node(cachep, node)->list_lock);
slab.c:	if (!ac || !ac->avail)
slab.c:	tofree = free_all ? ac->avail : (ac->limit + 4) / 5;
slab.c:	if (tofree > ac->avail)
slab.c:		tofree = (ac->avail + 1) / 2;
slab.c:	free_block(cachep, ac->entry, tofree, node, list);
slab.c:	ac->avail -= tofree;
slab.c:	memmove(ac->entry, &(ac->entry[tofree]), sizeof(void *) * ac->avail);
slab.c:	spin_lock(&n->list_lock);
slab.c:	free_block(cachep, ac->entry, ac->avail, node, &list);
slab.c:	spin_unlock(&n->list_lock);
slab.c:	ac->avail = 0;
slab.c:		if (n->alien)
slab.c:			drain_alien_cache(cachep, n->alien);
slab.c:		spin_lock_irq(&n->list_lock);
slab.c:		drain_array_locked(cachep, n->shared, node, true, &list);
slab.c:		spin_unlock_irq(&n->list_lock);
slab.c:	while (nr_freed < tofree && !list_empty(&n->slabs_free)) {
slab.c:		spin_lock_irq(&n->list_lock);
slab.c:		p = n->slabs_free.prev;
slab.c:		if (p == &n->slabs_free) {
slab.c:			spin_unlock_irq(&n->list_lock);
slab.c:		list_del(&page->lru);
slab.c:		n->free_slabs--;
slab.c:		n->total_slabs--;
slab.c:		n->free_objects -= cache->num;
slab.c:		spin_unlock_irq(&n->list_lock);
slab.c:		ret += !list_empty(&n->slabs_full) ||
slab.c:			!list_empty(&n->slabs_partial);
slab.c:	free_percpu(cachep->cpu_cache);
slab.c:		kfree(n->shared);
slab.c:		free_alien_cache(n->alien);
slab.c:		cachep->node[i] = NULL;
slab.c: * For a slab cache when the slab descriptor is off-slab, the
slab.c: * in turn looks up in the kmalloc_{dma,}_caches for the disired-size one.
slab.c: * This is a "chicken-and-egg" problem.
slab.c: * So the off-slab slab descriptor shall come from the kmalloc_{dma,}_caches,
slab.c:	page->s_mem = addr + colour_off;
slab.c:	page->active = 0;
slab.c:		/* Slab management obj is off-slab. */
slab.c:		freelist = kmem_cache_alloc_node(cachep->freelist_cache,
slab.c:		freelist = addr + (PAGE_SIZE << cachep->gfporder) -
slab.c:				cachep->freelist_size;
slab.c:	return ((freelist_idx_t *)page->freelist)[idx];
slab.c:	((freelist_idx_t *)(page->freelist))[idx] = val;
slab.c:	for (i = 0; i < cachep->num; i++) {
slab.c:		if (cachep->flags & SLAB_STORE_USER)
slab.c:		if (cachep->flags & SLAB_RED_ZONE) {
slab.c:		if (cachep->ctor && !(cachep->flags & SLAB_POISON)) {
slab.c:			cachep->ctor(objp + obj_offset(cachep));
slab.c:		if (cachep->flags & SLAB_RED_ZONE) {
slab.c:		if (cachep->flags & SLAB_POISON) {
slab.c: * return true if the pre-computed list is available, false otherwize.
slab.c:	/* Use a random state if the pre-computed list is not available */
slab.c:	if (!cachep->random_seq) {
slab.c:		prandom_seed_state(&state->rnd_state, rand);
slab.c:		state->list = cachep->random_seq;
slab.c:		state->count = count;
slab.c:		state->pos = rand % count;
slab.c:	if (state->pos >= state->count)
slab.c:		state->pos = 0;
slab.c:	return state->list[state->pos++];
slab.c:	swap(((freelist_idx_t *)page->freelist)[a],
slab.c:		((freelist_idx_t *)page->freelist)[b]);
slab.c: * Shuffle the freelist initialization state based on pre-computed lists.
slab.c:	unsigned int objfreelist = 0, i, rand, count = cachep->num;
slab.c:			objfreelist = count - 1;
slab.c:		page->freelist = index_to_obj(cachep, page, objfreelist) +
slab.c:		count--;
slab.c:	 * Later use a pre-computed list for speed.
slab.c:		/* Fisher-Yates shuffle */
slab.c:		for (i = count - 1; i > 0; i--) {
slab.c:		set_free_obj(page, cachep->num - 1, objfreelist);
slab.c:		page->freelist = index_to_obj(cachep, page, cachep->num - 1) +
slab.c:	for (i = 0; i < cachep->num; i++) {
slab.c:		if (DEBUG == 0 && cachep->ctor) {
slab.c:			cachep->ctor(objp);
slab.c:	objp = index_to_obj(cachep, page, get_free_obj(page, page->active));
slab.c:	page->active++;
slab.c:	if (cachep->flags & SLAB_STORE_USER)
slab.c:	for (i = page->active; i < cachep->num; i++) {
slab.c:			       cachep->name, objp);
slab.c:	page->active--;
slab.c:	if (!page->freelist)
slab.c:		page->freelist = objp + obj_offset(cachep);
slab.c:	set_free_obj(page, page->active, objnr);
slab.c:	page->slab_cache = cache;
slab.c:	page->freelist = freelist;
slab.c:	n->colour_next++;
slab.c:	if (n->colour_next >= cachep->colour)
slab.c:		n->colour_next = 0;
slab.c:	offset = n->colour_next;
slab.c:	if (offset >= cachep->colour)
slab.c:	offset *= cachep->colour_off;
slab.c:	INIT_LIST_HEAD(&page->lru);
slab.c:	spin_lock(&n->list_lock);
slab.c:	n->total_slabs++;
slab.c:	if (!page->active) {
slab.c:		list_add_tail(&page->lru, &(n->slabs_free));
slab.c:		n->free_slabs++;
slab.c:	n->free_objects += cachep->num - page->active;
slab.c:	spin_unlock(&n->list_lock);
slab.c: * - detect bad pointers.
slab.c: * - POISON/RED_ZONE checking
slab.c:	objp -= obj_offset(cachep);
slab.c:	if (cachep->flags & SLAB_RED_ZONE) {
slab.c:	if (cachep->flags & SLAB_STORE_USER) {
slab.c:	BUG_ON(objnr >= cachep->num);
slab.c:	if (cachep->flags & SLAB_POISON) {
slab.c:		objp = next - obj_offset(cachep);
slab.c:	list_del(&page->lru);
slab.c:	if (page->active == cachep->num) {
slab.c:		list_add(&page->lru, &n->slabs_full);
slab.c:			if (cachep->flags & SLAB_POISON) {
slab.c:				void **objp = page->freelist;
slab.c:			page->freelist = NULL;
slab.c:		list_add(&page->lru, &n->slabs_partial);
slab.c:/* Try to find non-pfmemalloc slab if needed */
slab.c:	if (n->free_objects > n->free_limit) {
slab.c:	list_del(&page->lru);
slab.c:	if (!page->active) {
slab.c:		list_add_tail(&page->lru, &n->slabs_free);
slab.c:		n->free_slabs++;
slab.c:		list_add_tail(&page->lru, &n->slabs_partial);
slab.c:	list_for_each_entry(page, &n->slabs_partial, lru) {
slab.c:	n->free_touched = 1;
slab.c:	list_for_each_entry(page, &n->slabs_free, lru) {
slab.c:			n->free_slabs--;
slab.c:	assert_spin_locked(&n->list_lock);
slab.c:	page = list_first_entry_or_null(&n->slabs_partial, struct page, lru);
slab.c:		n->free_touched = 1;
slab.c:		page = list_first_entry_or_null(&n->slabs_free, struct page,
slab.c:			n->free_slabs--;
slab.c:	spin_lock(&n->list_lock);
slab.c:		spin_unlock(&n->list_lock);
slab.c:	n->free_objects--;
slab.c:	spin_unlock(&n->list_lock);
slab.c:	BUG_ON(page->active >= cachep->num);
slab.c:	while (page->active < cachep->num && batchcount--) {
slab.c:		ac->entry[ac->avail++] = slab_get_obj(cachep, page);
slab.c:	batchcount = ac->batchcount;
slab.c:	if (!ac->touched && batchcount > BATCHREFILL_LIMIT) {
slab.c:	BUG_ON(ac->avail > 0 || !n);
slab.c:	shared = READ_ONCE(n->shared);
slab.c:	if (!n->free_objects && (!shared || !shared->avail))
slab.c:	spin_lock(&n->list_lock);
slab.c:	shared = READ_ONCE(n->shared);
slab.c:		shared->touched = 1;
slab.c:	n->free_objects -= ac->avail;
slab.c:	spin_unlock(&n->list_lock);
slab.c:	if (unlikely(!ac->avail)) {
slab.c:		if (!ac->avail && page)
slab.c:		if (!ac->avail)
slab.c:	ac->touched = 1;
slab.c:	return ac->entry[--ac->avail];
slab.c:	if (cachep->flags & SLAB_POISON) {
slab.c:	if (cachep->flags & SLAB_STORE_USER)
slab.c:	if (cachep->flags & SLAB_RED_ZONE) {
slab.c:	if (cachep->ctor && cachep->flags & SLAB_POISON)
slab.c:		cachep->ctor(objp);
slab.c:	    ((unsigned long)objp & (ARCH_SLAB_MINALIGN-1))) {
slab.c:	if (likely(ac->avail)) {
slab.c:		ac->touched = 1;
slab.c:		objp = ac->entry[--ac->avail];
slab.c:	 * per-CPU caches is leaked, we need to make sure kmemleak doesn't
slab.c:		kmemleak_erase(&ac->entry[ac->avail]);
slab.c:	if (cpuset_do_slab_mem_spread() && (cachep->flags & SLAB_MEM_SPREAD))
slab.c:	else if (current->mempolicy)
slab.c:			get_node(cache, nid)->free_objects) {
slab.c:	spin_lock(&n->list_lock);
slab.c:	BUG_ON(page->active == cachep->num);
slab.c:	n->free_objects--;
slab.c:	spin_unlock(&n->list_lock);
slab.c:	spin_unlock(&n->list_lock);
slab.c:		memset(ptr, 0, cachep->object_size);
slab.c:	if (current->mempolicy || cpuset_do_slab_mem_spread()) {
slab.c:		memset(objp, 0, cachep->object_size);
slab.c:	n->free_objects += nr_objects;
slab.c:		list_del(&page->lru);
slab.c:		if (page->active == 0) {
slab.c:			list_add(&page->lru, &n->slabs_free);
slab.c:			n->free_slabs++;
slab.c:			 * partial list on free - maximum time for the
slab.c:			list_add_tail(&page->lru, &n->slabs_partial);
slab.c:	while (n->free_objects > n->free_limit && !list_empty(&n->slabs_free)) {
slab.c:		n->free_objects -= cachep->num;
slab.c:		page = list_last_entry(&n->slabs_free, struct page, lru);
slab.c:		list_move(&page->lru, list);
slab.c:		n->free_slabs--;
slab.c:		n->total_slabs--;
slab.c:	batchcount = ac->batchcount;
slab.c:	spin_lock(&n->list_lock);
slab.c:	if (n->shared) {
slab.c:		struct array_cache *shared_array = n->shared;
slab.c:		int max = shared_array->limit - shared_array->avail;
slab.c:			memcpy(&(shared_array->entry[shared_array->avail]),
slab.c:			       ac->entry, sizeof(void *) * batchcount);
slab.c:			shared_array->avail += batchcount;
slab.c:	free_block(cachep, ac->entry, batchcount, node, &list);
slab.c:		list_for_each_entry(page, &n->slabs_free, lru) {
slab.c:			BUG_ON(page->active);
slab.c:	spin_unlock(&n->list_lock);
slab.c:	ac->avail -= batchcount;
slab.c:	memmove(ac->entry, &(ac->entry[batchcount]), sizeof(void *)*ac->avail);
slab.c:	kmemleak_free_recursive(objp, cachep->flags);
slab.c:	if (ac->avail < ac->limit) {
slab.c:	ac->entry[ac->avail++] = objp;
slab.c: * kmem_cache_alloc - Allocate an object
slab.c:			       cachep->object_size, cachep->size, flags);
slab.c:			memset(p[i], 0, s->object_size);
slab.c:		      size, cachep->size, flags);
slab.c: * kmem_cache_alloc_node - Allocate an object on the specified node
slab.c:				    cachep->object_size, cachep->size,
slab.c:			   size, cachep->size,
slab.c: * __do_kmalloc - allocate memory
slab.c:		      size, cachep->size, flags);
slab.c: * kmem_cache_free - Deallocate an object
slab.c:	debug_check_no_locks_freed(objp, cachep->object_size);
slab.c:	if (!(cachep->flags & SLAB_DEBUG_OBJECTS))
slab.c:		debug_check_no_obj_freed(objp, cachep->object_size);
slab.c:		debug_check_no_locks_freed(objp, s->object_size);
slab.c:		if (!(s->flags & SLAB_DEBUG_OBJECTS))
slab.c:			debug_check_no_obj_freed(objp, s->object_size);
slab.c: * kfree - free previously allocated memory
slab.c:	debug_check_no_locks_freed(objp, c->object_size);
slab.c:	debug_check_no_obj_freed(objp, c->object_size);
slab.c:	if (!cachep->list.next) {
slab.c:		node--;
slab.c:				kfree(n->shared);
slab.c:				free_alien_cache(n->alien);
slab.c:				cachep->node[node] = NULL;
slab.c:			node--;
slab.c:	return -ENOMEM;
slab.c:		return -ENOMEM;
slab.c:	prev = cachep->cpu_cache;
slab.c:	cachep->cpu_cache = cpu_cache;
slab.c:	cachep->batchcount = batchcount;
slab.c:	cachep->limit = limit;
slab.c:	cachep->shared = shared;
slab.c:		spin_lock_irq(&n->list_lock);
slab.c:		free_block(cachep, ac->entry, ac->avail, node, &list);
slab.c:		spin_unlock_irq(&n->list_lock);
slab.c:	err = cache_random_seq_create(cachep, cachep->num, gfp);
slab.c:		limit = root->limit;
slab.c:		shared = root->shared;
slab.c:		batchcount = root->batchcount;
slab.c:	 * - create a LIFO ordering, i.e. return objects that are cache-warm
slab.c:	 * - reduce the number of spinlock operations.
slab.c:	 * - reduce the number of linked list operations on the slab and
slab.c:	 * The numbers are guessed, we should auto-tune as described by
slab.c:	if (cachep->size > 131072)
slab.c:	else if (cachep->size > PAGE_SIZE)
slab.c:	else if (cachep->size > 1024)
slab.c:	else if (cachep->size > 256)
slab.c:	if (cachep->size <= PAGE_SIZE && num_possible_cpus() > 1)
slab.c:		       cachep->name, -err);
slab.c:	/* ac from n->shared can be freed if we don't hold the slab_mutex. */
slab.c:	if (!ac || !ac->avail)
slab.c:	if (ac->touched) {
slab.c:		ac->touched = 0;
slab.c:	spin_lock_irq(&n->list_lock);
slab.c:	spin_unlock_irq(&n->list_lock);
slab.c: * cache_reap - Reclaim memory from caches.
slab.c: * - clear the per-cpu caches for this CPU.
slab.c: * - return freeable pages to the main free memory pool.
slab.c: * If we cannot acquire the cache chain mutex then just give up - we'll try
slab.c:		if (time_after(n->next_reap, jiffies))
slab.c:		n->next_reap = jiffies + REAPTIMEOUT_NODE;
slab.c:		drain_array(searchp, n, n->shared, node);
slab.c:		if (n->free_touched)
slab.c:			n->free_touched = 0;
slab.c:			freed = drain_freelist(searchp, n, (n->free_limit +
slab.c:				5 * searchp->num - 1) / (5 * searchp->num));
slab.c:		spin_lock_irq(&n->list_lock);
slab.c:		total_slabs += n->total_slabs;
slab.c:		free_slabs += n->free_slabs;
slab.c:		free_objs += n->free_objects;
slab.c:		if (n->shared)
slab.c:			shared_avail += n->shared->avail;
slab.c:		spin_unlock_irq(&n->list_lock);
slab.c:	num_objs = total_slabs * cachep->num;
slab.c:	active_slabs = total_slabs - free_slabs;
slab.c:	active_objs = num_objs - free_objs;
slab.c:	sinfo->active_objs = active_objs;
slab.c:	sinfo->num_objs = num_objs;
slab.c:	sinfo->active_slabs = active_slabs;
slab.c:	sinfo->num_slabs = total_slabs;
slab.c:	sinfo->shared_avail = shared_avail;
slab.c:	sinfo->limit = cachep->limit;
slab.c:	sinfo->batchcount = cachep->batchcount;
slab.c:	sinfo->shared = cachep->shared;
slab.c:	sinfo->objects_per_slab = cachep->num;
slab.c:	sinfo->cache_order = cachep->gfporder;
slab.c:		unsigned long high = cachep->high_mark;
slab.c:		unsigned long allocs = cachep->num_allocations;
slab.c:		unsigned long grown = cachep->grown;
slab.c:		unsigned long reaped = cachep->reaped;
slab.c:		unsigned long errors = cachep->errors;
slab.c:		unsigned long max_freeable = cachep->max_freeable;
slab.c:		unsigned long node_allocs = cachep->node_allocs;
slab.c:		unsigned long node_frees = cachep->node_frees;
slab.c:		unsigned long overflows = cachep->node_overflow;
slab.c:		unsigned long allochit = atomic_read(&cachep->allochit);
slab.c:		unsigned long allocmiss = atomic_read(&cachep->allocmiss);
slab.c:		unsigned long freehit = atomic_read(&cachep->freehit);
slab.c:		unsigned long freemiss = atomic_read(&cachep->freemiss);
slab.c: * slabinfo_write - Tuning for the slab allocator
slab.c:		return -EINVAL;
slab.c:		return -EFAULT;
slab.c:		return -EINVAL;
slab.c:		return -EINVAL;
slab.c:	res = -EINVAL;
slab.c:		if (!strcmp(cachep->name, kbuf)) {
slab.c:			l -= i + 1;
slab.c:	memmove(p + 2, p, n[1] * 2 * sizeof(unsigned long) - ((void *)p - (void *)n));
slab.c:	for (i = 0, p = page->s_mem; i < c->num; i++, p += c->size) {
slab.c:		for (j = page->active; j < c->num; j++) {
slab.c:	unsigned long *x = m->private;
slab.c:	if (!(cachep->flags & SLAB_STORE_USER))
slab.c:	if (!(cachep->flags & SLAB_RED_ZONE))
slab.c:			spin_lock_irq(&n->list_lock);
slab.c:			list_for_each_entry(page, &n->slabs_full, lru)
slab.c:			list_for_each_entry(page, &n->slabs_partial, lru)
slab.c:			spin_unlock_irq(&n->list_lock);
slab.c:	name = cachep->name;
slab.c:		m->private = kzalloc(x[0] * 4 * sizeof(unsigned long), GFP_KERNEL);
slab.c:		if (!m->private) {
slab.c:			m->private = x;
slab.c:			return -ENOMEM;
slab.c:		*(unsigned long *)m->private = x[0] * 2;
slab.c:		m->count = m->size;
slab.c:		return -ENOMEM;
slab.c:	cachep = page->slab_cache;
slab.c:	BUG_ON(objnr >= cachep->num);
slab.c:	offset = ptr - index_to_obj(cachep, page, objnr) - obj_offset(cachep);
slab.c:	if (offset <= cachep->object_size && n <= cachep->object_size - offset)
slab.c:	return cachep->name;
slab.c: * ksize - get the actual amount of memory allocated for a given object
slab.c:	size = virt_to_cache(objp)->object_size;
vmstat.c: *  Copyright (C) 2008-2014 Christoph Lameter
vmstat.c:#define NUMA_STATS_THRESHOLD (U16_MAX - 2)
vmstat.c:			ret[i] += this->event[i];
vmstat.c: * The result is unavoidably approximate - it can change
vmstat.c:		count_vm_events(i, fold_state->event[i]);
vmstat.c:		fold_state->event[i] = 0;
vmstat.c:	watermark_distance = low_wmark_pages(zone) - min_wmark_pages(zone);
vmstat.c:	 * ------------------------------------------------------------------
vmstat.c:	 * 8		1		1	0.9-1 GB	4
vmstat.c:	 * 16		2		2	0.9-1 GB	4
vmstat.c:	 * 20 		2		2	1-2 GB		5
vmstat.c:	 * 24		2		2	2-4 GB		6
vmstat.c:	 * 28		2		2	4-8 GB		7
vmstat.c:	 * 32		2		2	8-16 GB		8
vmstat.c:	 * 30		4		3	2-4 GB		5
vmstat.c:	 * 48		4		3	8-16 GB		8
vmstat.c:	 * 32		8		4	1-2 GB		4
vmstat.c:	 * 32		8		4	0.9-1GB		4
vmstat.c:	 * 70		64		7	2-4 GB		5
vmstat.c:	 * 84		64		7	4-8 GB		6
vmstat.c:	 * 108		512		9	4-8 GB		6
vmstat.c:	 * 125		1024		10	8-16 GB		8
vmstat.c:	 * 125		1024		10	16-32 GB	9
vmstat.c:	mem = zone->managed_pages >> (27 - PAGE_SHIFT);
vmstat.c:			per_cpu_ptr(pgdat->per_cpu_nodestats, cpu)->stat_threshold = 0;
vmstat.c:		struct pglist_data *pgdat = zone->zone_pgdat;
vmstat.c:			per_cpu_ptr(zone->pageset, cpu)->stat_threshold
vmstat.c:			pgdat_threshold = per_cpu_ptr(pgdat->per_cpu_nodestats, cpu)->stat_threshold;
vmstat.c:			per_cpu_ptr(pgdat->per_cpu_nodestats, cpu)->stat_threshold
vmstat.c:		tolerate_drift = low_wmark_pages(zone) - min_wmark_pages(zone);
vmstat.c:			zone->percpu_drift_mark = high_wmark_pages(zone) +
vmstat.c:	for (i = 0; i < pgdat->nr_zones; i++) {
vmstat.c:		zone = &pgdat->node_zones[i];
vmstat.c:		if (!zone->percpu_drift_mark)
vmstat.c:			per_cpu_ptr(zone->pageset, cpu)->stat_threshold
vmstat.c:	struct per_cpu_pageset __percpu *pcp = zone->pageset;
vmstat.c:	s8 __percpu *p = pcp->vm_stat_diff + item;
vmstat.c:	t = __this_cpu_read(pcp->stat_threshold);
vmstat.c:	if (unlikely(x > t || x < -t)) {
vmstat.c:	struct per_cpu_nodestat __percpu *pcp = pgdat->per_cpu_nodestats;
vmstat.c:	s8 __percpu *p = pcp->vm_node_stat_diff + item;
vmstat.c:	t = __this_cpu_read(pcp->stat_threshold);
vmstat.c:	if (unlikely(x > t || x < -t)) {
vmstat.c:	struct per_cpu_pageset __percpu *pcp = zone->pageset;
vmstat.c:	s8 __percpu *p = pcp->vm_stat_diff + item;
vmstat.c:	t = __this_cpu_read(pcp->stat_threshold);
vmstat.c:		__this_cpu_write(*p, -overstep);
vmstat.c:	struct per_cpu_nodestat __percpu *pcp = pgdat->per_cpu_nodestats;
vmstat.c:	s8 __percpu *p = pcp->vm_node_stat_diff + item;
vmstat.c:	t = __this_cpu_read(pcp->stat_threshold);
vmstat.c:		__this_cpu_write(*p, -overstep);
vmstat.c:	struct per_cpu_pageset __percpu *pcp = zone->pageset;
vmstat.c:	s8 __percpu *p = pcp->vm_stat_diff + item;
vmstat.c:	t = __this_cpu_read(pcp->stat_threshold);
vmstat.c:	if (unlikely(v < - t)) {
vmstat.c:		zone_page_state_add(v - overstep, zone, item);
vmstat.c:	struct per_cpu_nodestat __percpu *pcp = pgdat->per_cpu_nodestats;
vmstat.c:	s8 __percpu *p = pcp->vm_node_stat_diff + item;
vmstat.c:	t = __this_cpu_read(pcp->stat_threshold);
vmstat.c:	if (unlikely(v < - t)) {
vmstat.c:		node_page_state_add(v - overstep, pgdat, item);
vmstat.c: *     -1      Overstepping minus half of threshold
vmstat.c:	struct per_cpu_pageset __percpu *pcp = zone->pageset;
vmstat.c:	s8 __percpu *p = pcp->vm_stat_diff + item;
vmstat.c:		t = this_cpu_read(pcp->stat_threshold);
vmstat.c:		if (n > t || n < -t) {
vmstat.c:			n = -os;
vmstat.c:	mod_zone_state(page_zone(page), item, -1, -1);
vmstat.c:	struct per_cpu_nodestat __percpu *pcp = pgdat->per_cpu_nodestats;
vmstat.c:	s8 __percpu *p = pcp->vm_node_stat_diff + item;
vmstat.c:		t = this_cpu_read(pcp->stat_threshold);
vmstat.c:		if (n > t || n < -t) {
vmstat.c:			n = -os;
vmstat.c:	mod_node_state(page_pgdat(page), item, -1, -1);
vmstat.c:		struct per_cpu_pageset __percpu *p = zone->pageset;
vmstat.c:			v = this_cpu_xchg(p->vm_stat_diff[i], 0);
vmstat.c:				atomic_long_add(v, &zone->vm_stat[i]);
vmstat.c:				__this_cpu_write(p->expire, 3);
vmstat.c:			v = this_cpu_xchg(p->vm_numa_stat_diff[i], 0);
vmstat.c:				atomic_long_add(v, &zone->vm_numa_stat[i]);
vmstat.c:				__this_cpu_write(p->expire, 3);
vmstat.c:			if (!__this_cpu_read(p->expire) ||
vmstat.c:			       !__this_cpu_read(p->pcp.count))
vmstat.c:				__this_cpu_write(p->expire, 0);
vmstat.c:			if (__this_cpu_dec_return(p->expire))
vmstat.c:			if (__this_cpu_read(p->pcp.count)) {
vmstat.c:				drain_zone_pages(zone, this_cpu_ptr(&p->pcp));
vmstat.c:		struct per_cpu_nodestat __percpu *p = pgdat->per_cpu_nodestats;
vmstat.c:			v = this_cpu_xchg(p->vm_node_stat_diff[i], 0);
vmstat.c:				atomic_long_add(v, &pgdat->vm_stat[i]);
vmstat.c:		p = per_cpu_ptr(zone->pageset, cpu);
vmstat.c:			if (p->vm_stat_diff[i]) {
vmstat.c:				v = p->vm_stat_diff[i];
vmstat.c:				p->vm_stat_diff[i] = 0;
vmstat.c:				atomic_long_add(v, &zone->vm_stat[i]);
vmstat.c:			if (p->vm_numa_stat_diff[i]) {
vmstat.c:				v = p->vm_numa_stat_diff[i];
vmstat.c:				p->vm_numa_stat_diff[i] = 0;
vmstat.c:				atomic_long_add(v, &zone->vm_numa_stat[i]);
vmstat.c:		p = per_cpu_ptr(pgdat->per_cpu_nodestats, cpu);
vmstat.c:			if (p->vm_node_stat_diff[i]) {
vmstat.c:				v = p->vm_node_stat_diff[i];
vmstat.c:				p->vm_node_stat_diff[i] = 0;
vmstat.c:				atomic_long_add(v, &pgdat->vm_stat[i]);
vmstat.c: * pset->vm_stat_diff[] exsist.
vmstat.c:		if (pset->vm_stat_diff[i]) {
vmstat.c:			int v = pset->vm_stat_diff[i];
vmstat.c:			pset->vm_stat_diff[i] = 0;
vmstat.c:			atomic_long_add(v, &zone->vm_stat[i]);
vmstat.c:		if (pset->vm_numa_stat_diff[i]) {
vmstat.c:			int v = pset->vm_numa_stat_diff[i];
vmstat.c:			pset->vm_numa_stat_diff[i] = 0;
vmstat.c:			atomic_long_add(v, &zone->vm_numa_stat[i]);
vmstat.c:	struct per_cpu_pageset __percpu *pcp = zone->pageset;
vmstat.c:	u16 __percpu *p = pcp->vm_numa_stat_diff + item;
vmstat.c:	struct zone *zones = NODE_DATA(node)->node_zones;
vmstat.c:	struct zone *zones = NODE_DATA(node)->node_zones;
vmstat.c:	long x = atomic_long_read(&pgdat->vm_stat[item]);
vmstat.c:	info->free_pages = 0;
vmstat.c:	info->free_blocks_total = 0;
vmstat.c:	info->free_blocks_suitable = 0;
vmstat.c:		blocks = zone->free_area[order].nr_free;
vmstat.c:		info->free_blocks_total += blocks;
vmstat.c:		info->free_pages += blocks << order;
vmstat.c:			info->free_blocks_suitable += blocks <<
vmstat.c:						(order - suitable_order);
vmstat.c:	if (!info->free_blocks_total)
vmstat.c:	if (info->free_blocks_suitable)
vmstat.c:		return -1000;
vmstat.c:	return 1000 - div_u64( (1000+(div_u64(info->free_pages * 1000ULL, requested))), info->free_blocks_total);
vmstat.c:	/* Node-based counters */
vmstat.c:		--node;
vmstat.c:	struct zone *node_zones = pgdat->node_zones;
vmstat.c:	for (zone = node_zones; zone - node_zones < MAX_NR_ZONES; ++zone) {
vmstat.c:			spin_lock_irqsave(&zone->lock, flags);
vmstat.c:			spin_unlock_irqrestore(&zone->lock, flags);
vmstat.c:	seq_printf(m, "Node %d, zone %8s ", pgdat->node_id, zone->name);
vmstat.c:		seq_printf(m, "%6lu ", zone->free_area[order].nr_free);
vmstat.c:					pgdat->node_id,
vmstat.c:					zone->name,
vmstat.c:			area = &(zone->free_area[order]);
vmstat.c:			list_for_each(curr, &area->free_list[mtype])
vmstat.c:	seq_printf(m, "%-43s ", "Free pages count per migrate type at order");
vmstat.c:	unsigned long start_pfn = zone->zone_start_pfn;
vmstat.c:	seq_printf(m, "Node %d, zone %8s ", pgdat->node_id, zone->name);
vmstat.c:	seq_printf(m, "\n%-23s", "Number of blocks type ");
vmstat.c:	seq_printf(m, "\n%-23s", "Number of mixed blocks ");
vmstat.c:	if (!node_state(pgdat->node_id, N_MEMORY))
vmstat.c:		struct zone *compare = &pgdat->node_zones[zid];
vmstat.c:	seq_printf(m, "Node %d, zone %8s", pgdat->node_id, zone->name);
vmstat.c:		seq_printf(m, "\n  per-node stats");
vmstat.c:			seq_printf(m, "\n      %-12s %lu",
vmstat.c:		   zone->spanned_pages,
vmstat.c:		   zone->present_pages,
vmstat.c:		   zone->managed_pages);
vmstat.c:		   zone->lowmem_reserve[0]);
vmstat.c:	for (i = 1; i < ARRAY_SIZE(zone->lowmem_reserve); i++)
vmstat.c:		seq_printf(m, ", %ld", zone->lowmem_reserve[i]);
vmstat.c:		seq_printf(m, "\n      %-12s %lu", vmstat_text[i],
vmstat.c:		seq_printf(m, "\n      %-12s %lu",
vmstat.c:		pageset = per_cpu_ptr(zone->pageset, i);
vmstat.c:			   pageset->pcp.count,
vmstat.c:			   pageset->pcp.high,
vmstat.c:			   pageset->pcp.batch);
vmstat.c:				pageset->stat_threshold);
vmstat.c:		   pgdat->kswapd_failures >= MAX_RECLAIM_RETRIES,
vmstat.c:		   zone->zone_start_pfn,
vmstat.c:		   zone->zone_pgdat->inactive_ratio);
vmstat.c:	m->private = v;
vmstat.c:		return ERR_PTR(-ENOMEM);
vmstat.c:	v[PGPGIN] /= 2;		/* sectors -> kbytes */
vmstat.c:	return (unsigned long *)m->private + *pos;
vmstat.c:	return (unsigned long *)m->private + *pos;
vmstat.c:	unsigned long off = l - (unsigned long *)m->private;
vmstat.c:	kfree(m->private);
vmstat.c:	m->private = NULL;
vmstat.c:			err = -EINVAL;
vmstat.c:			err = -EINVAL;
vmstat.c:		struct per_cpu_pageset *p = per_cpu_ptr(zone->pageset, cpu);
vmstat.c:		BUILD_BUG_ON(sizeof(p->vm_stat_diff[0]) != 1);
vmstat.c:		BUILD_BUG_ON(sizeof(p->vm_numa_stat_diff[0]) != 2);
vmstat.c:		if (memchr_inv(p->vm_stat_diff, 0, NR_VM_ZONE_STAT_ITEMS))
vmstat.c:		if (memchr_inv(p->vm_numa_stat_diff, 0, NR_VM_NUMA_STAT_ITEMS))
vmstat.c:	if (info->free_pages == 0)
vmstat.c:	return div_u64((info->free_pages - (info->free_blocks_suitable << order)) * 1000ULL, info->free_pages);
vmstat.c:				pgdat->node_id,
vmstat.c:				zone->name);
vmstat.c:	if (!node_state(pgdat->node_id, N_MEMORY))
vmstat.c:				pgdat->node_id,
vmstat.c:				zone->name);
vmstat.c:		return -ENOMEM;
vmstat.c:	return -ENOMEM;
early_ioremap.c:// SPDX-License-Identifier: GPL-2.0
early_ioremap.c:		slot_virt[i] = __fix_to_virt(FIX_BTMAP_BEGIN - NR_FIX_BTMAPS*i);
early_ioremap.c:	slot = -1;
early_ioremap.c:	last_addr = phys_addr + size - 1;
early_ioremap.c:	 * Mappings have to be page-aligned
early_ioremap.c:	size = PAGE_ALIGN(last_addr + 1) - phys_addr;
early_ioremap.c:	idx = FIX_BTMAP_BEGIN - NR_FIX_BTMAPS*slot;
early_ioremap.c:		--idx;
early_ioremap.c:		--nrpages;
early_ioremap.c:	slot = -1;
early_ioremap.c:	idx = FIX_BTMAP_BEGIN - NR_FIX_BTMAPS*slot;
early_ioremap.c:		--idx;
early_ioremap.c:		--nrpages;
early_ioremap.c:		if (clen > MAX_MAP_CHUNK - slop)
early_ioremap.c:			clen = MAX_MAP_CHUNK - slop;
early_ioremap.c:		size -= clen;
swap_cgroup.c:// SPDX-License-Identifier: GPL-2.0
swap_cgroup.c: *  - we have no race in "exchange" when we're accessed via SwapCache because
swap_cgroup.c: *  - When called via swap_free(), there is no user of this entry and no race.
swap_cgroup.c:	for (idx = 0; idx < ctrl->length; idx++) {
swap_cgroup.c:		ctrl->map[idx] = page;
swap_cgroup.c:		__free_page(ctrl->map[idx]);
swap_cgroup.c:	return -ENOMEM;
swap_cgroup.c:	mappage = ctrl->map[offset / SC_PER_PAGE];
swap_cgroup.c: * swap_cgroup_cmpxchg - cmpxchg mem_cgroup's id for this swp_entry.
swap_cgroup.c:	spin_lock_irqsave(&ctrl->lock, flags);
swap_cgroup.c:	retval = sc->id;
swap_cgroup.c:		sc->id = new;
swap_cgroup.c:	spin_unlock_irqrestore(&ctrl->lock, flags);
swap_cgroup.c: * swap_cgroup_record - record mem_cgroup for a set of swap entries
swap_cgroup.c:	spin_lock_irqsave(&ctrl->lock, flags);
swap_cgroup.c:	old = sc->id;
swap_cgroup.c:		VM_BUG_ON(sc->id != old);
swap_cgroup.c:		sc->id = id;
swap_cgroup.c:	spin_unlock_irqrestore(&ctrl->lock, flags);
swap_cgroup.c: * lookup_swap_cgroup_id - lookup mem_cgroup id tied to swap entry
swap_cgroup.c:	return lookup_swap_cgroup(ent, NULL)->id;
swap_cgroup.c:	ctrl->length = length;
swap_cgroup.c:	ctrl->map = array;
swap_cgroup.c:	spin_lock_init(&ctrl->lock);
swap_cgroup.c:		ctrl->map = NULL;
swap_cgroup.c:		ctrl->length = 0;
swap_cgroup.c:	return -ENOMEM;
swap_cgroup.c:	map = ctrl->map;
swap_cgroup.c:	length = ctrl->length;
swap_cgroup.c:	ctrl->map = NULL;
swap_cgroup.c:	ctrl->length = 0;
page_vma_mapped.c:// SPDX-License-Identifier: GPL-2.0
page_vma_mapped.c:	pvmw->pte = pte_offset_map(pvmw->pmd, pvmw->address);
page_vma_mapped.c:	if (!(pvmw->flags & PVMW_SYNC)) {
page_vma_mapped.c:		if (pvmw->flags & PVMW_MIGRATION) {
page_vma_mapped.c:			if (!is_swap_pte(*pvmw->pte))
page_vma_mapped.c:			if (!pte_present(*pvmw->pte))
page_vma_mapped.c:	pvmw->ptl = pte_lockptr(pvmw->vma->vm_mm, pvmw->pmd);
page_vma_mapped.c:	spin_lock(pvmw->ptl);
page_vma_mapped.c: * check_pte - check if @pvmw->page is mapped at the @pvmw->pte
page_vma_mapped.c: * page_vma_mapped_walk() found a place where @pvmw->page is *potentially*
page_vma_mapped.c: * @pvmw->pte may point to empty PTE, swap PTE or PTE pointing to arbitrary
page_vma_mapped.c: * If PVMW_MIGRATION flag is set, returns true if @pvmw->pte contains migration
page_vma_mapped.c: * entry that points to @pvmw->page or any subpage in case of THP.
page_vma_mapped.c: * If PVMW_MIGRATION flag is not set, returns true if @pvmw->pte points to
page_vma_mapped.c: * @pvmw->page or any subpage in case of THP.
page_vma_mapped.c:	if (pvmw->flags & PVMW_MIGRATION) {
page_vma_mapped.c:		if (!is_swap_pte(*pvmw->pte))
page_vma_mapped.c:		entry = pte_to_swp_entry(*pvmw->pte);
page_vma_mapped.c:	} else if (is_swap_pte(*pvmw->pte)) {
page_vma_mapped.c:		/* Handle un-addressable ZONE_DEVICE memory */
page_vma_mapped.c:		entry = pte_to_swp_entry(*pvmw->pte);
page_vma_mapped.c:		if (!pte_present(*pvmw->pte))
page_vma_mapped.c:		pfn = pte_pfn(*pvmw->pte);
page_vma_mapped.c:	if (pfn < page_to_pfn(pvmw->page))
page_vma_mapped.c:	if (pfn - page_to_pfn(pvmw->page) >= hpage_nr_pages(pvmw->page))
page_vma_mapped.c: * page_vma_mapped_walk - check if @pvmw->page is mapped in @pvmw->vma at
page_vma_mapped.c: * @pvmw->address
page_vma_mapped.c: * Returns true if the page is mapped in the vma. @pvmw->pmd and @pvmw->pte point
page_vma_mapped.c: * to relevant page table entries. @pvmw->ptl is locked. @pvmw->address is
page_vma_mapped.c: * adjusted if needed (for PTE-mapped THPs).
page_vma_mapped.c: * If @pvmw->pmd is set but @pvmw->pte is not, you have found PMD-mapped page
page_vma_mapped.c: * (usually THP). For PTE-mapped THP, you should run page_vma_mapped_walk() in
page_vma_mapped.c: * For HugeTLB pages, @pvmw->pte is set to the relevant page table entry
page_vma_mapped.c: * regardless of which page table level the page is mapped at. @pvmw->pmd is
page_vma_mapped.c: * the vma. @pvmw->ptl is unlocked and @pvmw->pte is unmapped.
page_vma_mapped.c:	struct mm_struct *mm = pvmw->vma->vm_mm;
page_vma_mapped.c:	struct page *page = pvmw->page;
page_vma_mapped.c:	if (pvmw->pmd && !pvmw->pte)
page_vma_mapped.c:	if (pvmw->pte)
page_vma_mapped.c:	if (unlikely(PageHuge(pvmw->page))) {
page_vma_mapped.c:		pvmw->pte = huge_pte_offset(mm, pvmw->address,
page_vma_mapped.c:		if (!pvmw->pte)
page_vma_mapped.c:		pvmw->ptl = huge_pte_lockptr(page_hstate(page), mm, pvmw->pte);
page_vma_mapped.c:		spin_lock(pvmw->ptl);
page_vma_mapped.c:	pgd = pgd_offset(mm, pvmw->address);
page_vma_mapped.c:	p4d = p4d_offset(pgd, pvmw->address);
page_vma_mapped.c:	pud = pud_offset(p4d, pvmw->address);
page_vma_mapped.c:	pvmw->pmd = pmd_offset(pud, pvmw->address);
page_vma_mapped.c:	pmde = READ_ONCE(*pvmw->pmd);
page_vma_mapped.c:		pvmw->ptl = pmd_lock(mm, pvmw->pmd);
page_vma_mapped.c:		if (likely(pmd_trans_huge(*pvmw->pmd))) {
page_vma_mapped.c:			if (pvmw->flags & PVMW_MIGRATION)
page_vma_mapped.c:			if (pmd_page(*pvmw->pmd) != page)
page_vma_mapped.c:		} else if (!pmd_present(*pvmw->pmd)) {
page_vma_mapped.c:				if (!(pvmw->flags & PVMW_MIGRATION))
page_vma_mapped.c:				if (is_migration_entry(pmd_to_swp_entry(*pvmw->pmd))) {
page_vma_mapped.c:					swp_entry_t entry = pmd_to_swp_entry(*pvmw->pmd);
page_vma_mapped.c:			spin_unlock(pvmw->ptl);
page_vma_mapped.c:			pvmw->ptl = NULL;
page_vma_mapped.c:		if (!PageTransHuge(pvmw->page) || PageHuge(pvmw->page))
page_vma_mapped.c:			pvmw->address += PAGE_SIZE;
page_vma_mapped.c:			if (pvmw->address >= pvmw->vma->vm_end ||
page_vma_mapped.c:			    pvmw->address >=
page_vma_mapped.c:					__vma_address(pvmw->page, pvmw->vma) +
page_vma_mapped.c:					hpage_nr_pages(pvmw->page) * PAGE_SIZE)
page_vma_mapped.c:			if (pvmw->address % PMD_SIZE == 0) {
page_vma_mapped.c:				pte_unmap(pvmw->pte);
page_vma_mapped.c:				if (pvmw->ptl) {
page_vma_mapped.c:					spin_unlock(pvmw->ptl);
page_vma_mapped.c:					pvmw->ptl = NULL;
page_vma_mapped.c:				pvmw->pte++;
page_vma_mapped.c:		} while (pte_none(*pvmw->pte));
page_vma_mapped.c:		if (!pvmw->ptl) {
page_vma_mapped.c:			pvmw->ptl = pte_lockptr(mm, pvmw->pmd);
page_vma_mapped.c:			spin_lock(pvmw->ptl);
page_vma_mapped.c: * page_mapped_in_vma - check whether a page is really mapped in a VMA
page_vma_mapped.c:	end = start + PAGE_SIZE * (hpage_nr_pages(page) - 1);
page_vma_mapped.c:	if (unlikely(end < vma->vm_start || start >= vma->vm_end))
page_vma_mapped.c:	pvmw.address = max(start, vma->vm_start);
oom_kill.c: *  Since we won't call these routines often (on a well-configured
oom_kill.c: * has_intersects_mems_allowed() - check task eligiblity for kill
oom_kill.c: * The process p may have detached its own ->mm while exiting or through
oom_kill.c: * pointer.  Return p, or any of its subthreads with a valid ->mm, with
oom_kill.c:		if (likely(t->mm))
oom_kill.c: * order == -1 means the oom kill is required by sysrq, otherwise only
oom_kill.c:	return oc->order == -1;
oom_kill.c:	return oc->memcg != NULL;
oom_kill.c:	if (p->flags & PF_KTHREAD)
oom_kill.c: * oom_badness - heuristic function to determine which candidate task to kill
oom_kill.c:	adj = (long)p->signal->oom_score_adj;
oom_kill.c:			test_bit(MMF_OOM_SKIP, &p->mm->flags) ||
oom_kill.c:	points = get_mm_rss(p->mm) + get_mm_counter(p->mm, MM_SWAPENTS) +
oom_kill.c:		atomic_long_read(&p->mm->nr_ptes) + mm_nr_pmds(p->mm);
oom_kill.c:		points -= (points * 3) / 100;
oom_kill.c:	enum zone_type high_zoneidx = gfp_zone(oc->gfp_mask);
oom_kill.c:		oc->totalpages = mem_cgroup_get_limit(oc->memcg) ?: 1;
oom_kill.c:	oc->totalpages = totalram_pages + total_swap_pages;
oom_kill.c:	if (!oc->zonelist)
oom_kill.c:	if (oc->gfp_mask & __GFP_THISNODE)
oom_kill.c:	if (oc->nodemask &&
oom_kill.c:	    !nodes_subset(node_states[N_MEMORY], *oc->nodemask)) {
oom_kill.c:		oc->totalpages = total_swap_pages;
oom_kill.c:		for_each_node_mask(nid, *oc->nodemask)
oom_kill.c:			oc->totalpages += node_spanned_pages(nid);
oom_kill.c:	for_each_zone_zonelist_nodemask(zone, z, oc->zonelist,
oom_kill.c:			high_zoneidx, oc->nodemask)
oom_kill.c:		if (!cpuset_zone_allowed(zone, oc->gfp_mask))
oom_kill.c:		oc->totalpages = total_swap_pages;
oom_kill.c:			oc->totalpages += node_spanned_pages(nid);
oom_kill.c:	if (oom_unkillable_task(task, NULL, oc->nodemask))
oom_kill.c:		if (test_bit(MMF_OOM_SKIP, &task->signal->oom_mm->flags))
oom_kill.c:	points = oom_badness(task, NULL, oc->nodemask, oc->totalpages);
oom_kill.c:	if (!points || points < oc->chosen_points)
oom_kill.c:	if (points == oc->chosen_points && thread_group_leader(oc->chosen))
oom_kill.c:	if (oc->chosen)
oom_kill.c:		put_task_struct(oc->chosen);
oom_kill.c:	oc->chosen = task;
oom_kill.c:	oc->chosen_points = points;
oom_kill.c:	if (oc->chosen)
oom_kill.c:		put_task_struct(oc->chosen);
oom_kill.c:	oc->chosen = (void *)-1UL;
oom_kill.c: * 'points'. In case scan was aborted, oc->chosen is set to -1.
oom_kill.c:		mem_cgroup_scan_tasks(oc->memcg, oom_evaluate_task, oc);
oom_kill.c:	oc->chosen_points = oc->chosen_points * 1000 / oc->totalpages;
oom_kill.c: * dump_tasks - dump current memory state of all system tasks
oom_kill.c:			task->pid, from_kuid(&init_user_ns, task_uid(task)),
oom_kill.c:			task->tgid, task->mm->total_vm, get_mm_rss(task->mm),
oom_kill.c:			atomic_long_read(&task->mm->nr_ptes),
oom_kill.c:			mm_nr_pmds(task->mm),
oom_kill.c:			get_mm_counter(task->mm, MM_SWAPENTS),
oom_kill.c:			task->signal->oom_score_adj, task->comm);
oom_kill.c:	pr_warn("%s invoked oom-killer: gfp_mask=%#x(%pGg), nodemask=",
oom_kill.c:		current->comm, oc->gfp_mask, &oc->gfp_mask);
oom_kill.c:	if (oc->nodemask)
oom_kill.c:		pr_cont("%*pbl", nodemask_pr_args(oc->nodemask));
oom_kill.c:		oc->order, current->signal->oom_score_adj);
oom_kill.c:	if (!IS_ENABLED(CONFIG_COMPACTION) && oc->order)
oom_kill.c:	if (oc->memcg)
oom_kill.c:		mem_cgroup_print_oom_info(oc->memcg, p);
oom_kill.c:		show_mem(SHOW_MEM_FILTER_NODES, oc->nodemask);
oom_kill.c:		dump_tasks(oc->memcg, oc->nodemask);
oom_kill.c:#define K(x) ((x) << (PAGE_SHIFT-10))
oom_kill.c: * task->mm can be NULL if the task is the exited group leader.  So to
oom_kill.c:		struct mm_struct *t_mm = READ_ONCE(t->mm);
oom_kill.c:	set_bit(MMF_UNSTABLE, &mm->flags);
oom_kill.c:	for (vma = mm->mmap ; vma; vma = vma->vm_next) {
oom_kill.c:		if (vma_is_anonymous(vma) || !(vma->vm_flags & VM_SHARED)) {
oom_kill.c:			tlb_gather_mmu(&tlb, mm, vma->vm_start, vma->vm_end);
oom_kill.c:			unmap_page_range(&tlb, vma, vma->vm_start, vma->vm_end,
oom_kill.c:			tlb_finish_mmu(&tlb, vma->vm_start, vma->vm_end);
oom_kill.c:	if (!down_read_trylock(&mm->mmap_sem)) {
oom_kill.c:		trace_skip_task_reaping(tsk->pid);
oom_kill.c:		up_read(&mm->mmap_sem);
oom_kill.c:	if (test_bit(MMF_OOM_SKIP, &mm->flags)) {
oom_kill.c:		up_read(&mm->mmap_sem);
oom_kill.c:		trace_skip_task_reaping(tsk->pid);
oom_kill.c:	trace_start_task_reaping(tsk->pid);
oom_kill.c:	pr_info("oom_reaper: reaped process %d (%s), now anon-rss:%lukB, file-rss:%lukB, shmem-rss:%lukB\n",
oom_kill.c:			task_pid_nr(tsk), tsk->comm,
oom_kill.c:	up_read(&mm->mmap_sem);
oom_kill.c:	trace_finish_task_reaping(tsk->pid);
oom_kill.c:	struct mm_struct *mm = tsk->signal->oom_mm;
oom_kill.c:		task_pid_nr(tsk), tsk->comm);
oom_kill.c:	tsk->oom_reaper_list = NULL;
oom_kill.c:	set_bit(MMF_OOM_SKIP, &mm->flags);
oom_kill.c:			oom_reaper_list = tsk->oom_reaper_list;
oom_kill.c:	if (tsk == oom_reaper_list || tsk->oom_reaper_list)
oom_kill.c:	tsk->oom_reaper_list = oom_reaper_list;
oom_kill.c:	trace_wake_reaper(tsk->pid);
oom_kill.c: * mark_oom_victim - mark the given task as OOM victim
oom_kill.c: * tsk->mm has to be non NULL and caller has to guarantee it is stable (either
oom_kill.c:	struct mm_struct *mm = tsk->mm;
oom_kill.c:	if (!cmpxchg(&tsk->signal->oom_mm, NULL, mm)) {
oom_kill.c:		mmgrab(tsk->signal->oom_mm);
oom_kill.c:		set_bit(MMF_OOM_VICTIM, &mm->flags);
oom_kill.c:	trace_mark_victim(tsk->pid);
oom_kill.c: * exit_oom_victim - note the exit of an OOM victim
oom_kill.c: * oom_killer_enable - enable OOM killer
oom_kill.c: * oom_killer_disable - disable OOM killer
oom_kill.c:	struct signal_struct *sig = task->signal;
oom_kill.c:	if (sig->flags & SIGNAL_GROUP_COREDUMP)
oom_kill.c:	if (sig->flags & SIGNAL_GROUP_EXIT)
oom_kill.c:	if (thread_group_empty(task) && (task->flags & PF_EXITING))
oom_kill.c: * Caller has to make sure that task->mm is stable (hold task_lock or
oom_kill.c:	struct mm_struct *mm = task->mm;
oom_kill.c:	if (test_bit(MMF_OOM_SKIP, &mm->flags))
oom_kill.c:	if (atomic_read(&mm->mm_users) <= 1)
oom_kill.c:	struct task_struct *p = oc->chosen;
oom_kill.c:	unsigned int points = oc->chosen_points;
oom_kill.c:		message, task_pid_nr(p), p->comm, points);
oom_kill.c:		list_for_each_entry(child, &t->children, sibling) {
oom_kill.c:			if (process_shares_mm(child, p->mm))
oom_kill.c:				oc->memcg, oc->nodemask, oc->totalpages);
oom_kill.c:	mm = victim->mm;
oom_kill.c:	pr_err("Killed process %d (%s) total-vm:%lukB, anon-rss:%lukB, file-rss:%lukB, shmem-rss:%lukB\n",
oom_kill.c:		task_pid_nr(victim), victim->comm, K(victim->mm->total_vm),
oom_kill.c:		K(get_mm_counter(victim->mm, MM_ANONPAGES)),
oom_kill.c:		K(get_mm_counter(victim->mm, MM_FILEPAGES)),
oom_kill.c:		K(get_mm_counter(victim->mm, MM_SHMEMPAGES)));
oom_kill.c:	 * Kill all user processes sharing victim->mm in other thread groups, if
oom_kill.c:	 * depletion of all memory.  This prevents mm->mmap_sem livelock when an
oom_kill.c:			set_bit(MMF_OOM_SKIP, &mm->flags);
oom_kill.c:					task_pid_nr(victim), victim->comm,
oom_kill.c:					task_pid_nr(p), p->comm);
oom_kill.c:		if (unlikely(p->flags & PF_KTHREAD))
oom_kill.c:		sysctl_panic_on_oom == 2 ? "compulsory" : "system-wide");
oom_kill.c: * out_of_memory - kill the "best" process when we run out of memory
oom_kill.c:	 * The OOM killer does not compensate for IO-less reclaim.
oom_kill.c:	 * make sure exclude 0 mask - all other users should have at least
oom_kill.c:	if (oc->gfp_mask && !(oc->gfp_mask & __GFP_FS))
oom_kill.c:		oc->nodemask = NULL;
oom_kill.c:	    current->mm && !oom_unkillable_task(current, NULL, oc->nodemask) &&
oom_kill.c:	    current->signal->oom_score_adj != OOM_SCORE_ADJ_MIN) {
oom_kill.c:		oc->chosen = current;
oom_kill.c:	if (!oc->chosen && !is_sysrq_oom(oc) && !is_memcg_oom(oc)) {
oom_kill.c:	if (oc->chosen && oc->chosen != (void *)-1UL) {
oom_kill.c:	return !!oc->chosen;
oom_kill.c: * memory-hogging task. If oom_lock is held by somebody else, a parallel oom
percpu.c: * mm/percpu.c - percpu memory allocator
percpu.c: * a 1-to-1 mapping for units to possible cpus.  These units are grouped
percpu.c: *  -------------------          -------------------        ------------
percpu.c: *  -------------------  ......  -------------------  ....  ------------
percpu.c: * c1:u1, c1:u2, etc.  On NUMA machines, the mapping may be non-linear
percpu.c: * linker.  The reserved section, if non-zero, primarily manages static
percpu.c: * - define __addr_to_pcpu_ptr() and __pcpu_ptr_to_addr() to translate
percpu.c: * - use pcpu_setup_first_chunk() during percpu area initialization to
percpu.c:#include "percpu-internal.h"
percpu.c:/* the slots are sorted by free bytes left, 1-31 bytes share the same slot */
percpu.c:/* default addr <-> pcpu_ptr mapping, override in asm/percpu.h if necessary */
percpu.c:	(void __percpu *)((unsigned long)(addr) -			\
percpu.c:			 (unsigned long)pcpu_base_addr -		\
percpu.c:static const int *pcpu_unit_map __ro_after_init;		/* cpu -> unit */
percpu.c:const unsigned long *pcpu_unit_offsets __ro_after_init;	/* cpu -> unit offset */
percpu.c: * pcpu_addr_in_chunk - check if the address is served from this chunk
percpu.c:	start_addr = chunk->base_addr + chunk->start_offset;
percpu.c:	end_addr = chunk->base_addr + chunk->nr_pages * PAGE_SIZE -
percpu.c:		   chunk->end_offset;
percpu.c:	return max(highbit - PCPU_SLOT_BASE_SHIFT + 2, 1);
percpu.c:		return pcpu_nr_slots - 1;
percpu.c:	if (chunk->free_bytes < PCPU_MIN_ALLOC_SIZE || chunk->contig_bits == 0)
percpu.c:	return pcpu_size_to_slot(chunk->free_bytes);
percpu.c:	page->index = (unsigned long)pcpu;
percpu.c:	return (struct pcpu_chunk *)page->index;
percpu.c:	return (unsigned long)chunk->base_addr +
percpu.c:	return chunk->alloc_map +
percpu.c:	return off & (PCPU_BITMAP_BLOCK_BITS - 1);
percpu.c: * pcpu_next_md_free_region - finds the next hint free area
percpu.c: * block->contig_hint and performs aggregation across blocks to find the
percpu.c: * next hint.  It modifies bit_off and bits in-place to be consumed in the
percpu.c:	for (block = chunk->md_blocks + i; i < pcpu_chunk_nr_blocks(chunk);
percpu.c:			*bits += block->left_free;
percpu.c:			if (block->left_free == PCPU_BITMAP_BLOCK_BITS)
percpu.c:		*bits = block->contig_hint;
percpu.c:		if (*bits && block->contig_hint_start >= block_off &&
percpu.c:		    *bits + block->contig_hint_start < PCPU_BITMAP_BLOCK_BITS) {
percpu.c:					block->contig_hint_start);
percpu.c:		*bits = block->right_free;
percpu.c:		*bit_off = (i + 1) * PCPU_BITMAP_BLOCK_BITS - block->right_free;
percpu.c: * pcpu_next_fit_region - finds fit areas for a given allocation request
percpu.c: * allocation.  block->first_free is returned if the allocation request fits
percpu.c:	for (block = chunk->md_blocks + i; i < pcpu_chunk_nr_blocks(chunk);
percpu.c:			*bits += block->left_free;
percpu.c:			if (block->left_free == PCPU_BITMAP_BLOCK_BITS)
percpu.c:		/* check block->contig_hint */
percpu.c:		*bits = ALIGN(block->contig_hint_start, align) -
percpu.c:			block->contig_hint_start;
percpu.c:		if (block->contig_hint &&
percpu.c:		    block->contig_hint_start >= block_off &&
percpu.c:		    block->contig_hint >= *bits + alloc_bits) {
percpu.c:			*bits += alloc_bits + block->contig_hint_start -
percpu.c:				 block->first_free;
percpu.c:			*bit_off = pcpu_block_off_to_off(i, block->first_free);
percpu.c:		*bit_off = ALIGN(PCPU_BITMAP_BLOCK_BITS - block->right_free,
percpu.c:		*bits = PCPU_BITMAP_BLOCK_BITS - *bit_off;
percpu.c:	/* no valid offsets were found - fail condition */
percpu.c: * pcpu_mem_zalloc - allocate memory
percpu.c: * pcpu_mem_free - free memory
percpu.c: * pcpu_chunk_relocate - put chunk in the appropriate chunk slot
percpu.c:			list_move(&chunk->list, &pcpu_slot[nslot]);
percpu.c:			list_move_tail(&chunk->list, &pcpu_slot[nslot]);
percpu.c: * pcpu_cnt_pop_pages- counts populated backing pages in range
percpu.c:	return bitmap_weight(chunk->populated, page_end) -
percpu.c:	       bitmap_weight(chunk->populated, page_start);
percpu.c: * pcpu_chunk_update - updates the chunk metadata given a free area
percpu.c:	if (bits > chunk->contig_bits) {
percpu.c:		chunk->contig_bits_start = bit_off;
percpu.c:		chunk->contig_bits = bits;
percpu.c:	} else if (bits == chunk->contig_bits && chunk->contig_bits_start &&
percpu.c:		    __ffs(bit_off) > __ffs(chunk->contig_bits_start))) {
percpu.c:		chunk->contig_bits_start = bit_off;
percpu.c: * pcpu_chunk_refresh_hint - updates metadata about a chunk
percpu.c: *      chunk->contig_bits
percpu.c: *      chunk->contig_bits_start
percpu.c:	chunk->contig_bits = 0;
percpu.c:	bit_off = chunk->first_bit;
percpu.c:			(nr_empty_pop_pages - chunk->nr_empty_pop_pages);
percpu.c:	chunk->nr_empty_pop_pages = nr_empty_pop_pages;
percpu.c: * pcpu_block_update - updates a block given a free area
percpu.c:	int contig = end - start;
percpu.c:	block->first_free = min(block->first_free, start);
percpu.c:		block->left_free = contig;
percpu.c:		block->right_free = contig;
percpu.c:	if (contig > block->contig_hint) {
percpu.c:		block->contig_hint_start = start;
percpu.c:		block->contig_hint = contig;
percpu.c:	} else if (block->contig_hint_start && contig == block->contig_hint &&
percpu.c:		   (!start || __ffs(start) > __ffs(block->contig_hint_start))) {
percpu.c:		block->contig_hint_start = start;
percpu.c:	struct pcpu_block_md *block = chunk->md_blocks + index;
percpu.c:	block->contig_hint = 0;
percpu.c:	block->left_free = block->right_free = 0;
percpu.c:	pcpu_for_each_unpop_region(alloc_map, rs, re, block->first_free,
percpu.c: * pcpu_block_update_hint_alloc - update hint on allocation path
percpu.c:	e_index = pcpu_off_to_block_index(bit_off + bits - 1);
percpu.c:	e_off = pcpu_off_to_block_off(bit_off + bits - 1) + 1;
percpu.c:	s_block = chunk->md_blocks + s_index;
percpu.c:	e_block = chunk->md_blocks + e_index;
percpu.c:	 * block->first_free must be updated if the allocation takes its place.
percpu.c:	if (s_off == s_block->first_free)
percpu.c:		s_block->first_free = find_next_zero_bit(
percpu.c:	if (s_off >= s_block->contig_hint_start &&
percpu.c:	    s_off < s_block->contig_hint_start + s_block->contig_hint) {
percpu.c:		/* block contig hint is broken - scan to fix it */
percpu.c:		s_block->left_free = min(s_block->left_free, s_off);
percpu.c:			s_block->right_free = min_t(int, s_block->right_free,
percpu.c:					PCPU_BITMAP_BLOCK_BITS - e_off);
percpu.c:			s_block->right_free = 0;
percpu.c:		e_block->first_free = find_next_zero_bit(
percpu.c:			if (e_off > e_block->contig_hint_start) {
percpu.c:				/* contig hint is broken - scan to fix it */
percpu.c:				e_block->left_free = 0;
percpu.c:				e_block->right_free =
percpu.c:					min_t(int, e_block->right_free,
percpu.c:					      PCPU_BITMAP_BLOCK_BITS - e_off);
percpu.c:		/* update in-between md_blocks */
percpu.c:			block->contig_hint = 0;
percpu.c:			block->left_free = 0;
percpu.c:			block->right_free = 0;
percpu.c:	if (bit_off >= chunk->contig_bits_start  &&
percpu.c:	    bit_off < chunk->contig_bits_start + chunk->contig_bits)
percpu.c: * pcpu_block_update_hint_free - updates the block hints on the free path
percpu.c: * over the block metadata to update chunk->contig_bits.  chunk->contig_bits
percpu.c:	e_index = pcpu_off_to_block_index(bit_off + bits - 1);
percpu.c:	e_off = pcpu_off_to_block_off(bit_off + bits - 1) + 1;
percpu.c:	s_block = chunk->md_blocks + s_index;
percpu.c:	e_block = chunk->md_blocks + e_index;
percpu.c:	 * Check if the freed area aligns with the block->contig_hint.
percpu.c:	if (s_off == s_block->contig_hint + s_block->contig_hint_start) {
percpu.c:		start = s_block->contig_hint_start;
percpu.c:	if (e_off == e_block->contig_hint_start)
percpu.c:		end = e_block->contig_hint_start + e_block->contig_hint;
percpu.c:			block->first_free = 0;
percpu.c:			block->contig_hint_start = 0;
percpu.c:			block->contig_hint = PCPU_BITMAP_BLOCK_BITS;
percpu.c:			block->left_free = PCPU_BITMAP_BLOCK_BITS;
percpu.c:			block->right_free = PCPU_BITMAP_BLOCK_BITS;
percpu.c:				  s_block->contig_hint);
percpu.c: * pcpu_is_populated - determines if the region is populated
percpu.c:	pcpu_next_unpop(chunk->populated, &rs, &re, page_end);
percpu.c: * pcpu_find_block_fit - finds the block index to start searching
percpu.c: * -1 if no offset is found.
percpu.c:	bit_off = ALIGN(chunk->contig_bits_start, align) -
percpu.c:		  chunk->contig_bits_start;
percpu.c:	if (bit_off + alloc_bits > chunk->contig_bits)
percpu.c:		return -1;
percpu.c:	bit_off = chunk->first_bit;
percpu.c:		return -1;
percpu.c: * pcpu_alloc_area - allocates an area from a pcpu_chunk
percpu.c: * @start will be block->first_free. This is an attempt to fill the
percpu.c: * -1 if no matching area is found.
percpu.c:	size_t align_mask = (align) ? (align - 1) : 0;
percpu.c:	bit_off = bitmap_find_next_zero_area(chunk->alloc_map, end, start,
percpu.c:		return -1;
percpu.c:	bitmap_set(chunk->alloc_map, bit_off, alloc_bits);
percpu.c:	set_bit(bit_off, chunk->bound_map);
percpu.c:	bitmap_clear(chunk->bound_map, bit_off + 1, alloc_bits - 1);
percpu.c:	set_bit(bit_off + alloc_bits, chunk->bound_map);
percpu.c:	chunk->free_bytes -= alloc_bits * PCPU_MIN_ALLOC_SIZE;
percpu.c:	if (bit_off == chunk->first_bit)
percpu.c:		chunk->first_bit = find_next_zero_bit(
percpu.c:					chunk->alloc_map,
percpu.c: * pcpu_free_area - frees the corresponding offset
percpu.c:	end = find_next_bit(chunk->bound_map, pcpu_chunk_map_bits(chunk),
percpu.c:	bits = end - bit_off;
percpu.c:	bitmap_clear(chunk->alloc_map, bit_off, bits);
percpu.c:	chunk->free_bytes += bits * PCPU_MIN_ALLOC_SIZE;
percpu.c:	chunk->first_bit = min(chunk->first_bit, bit_off);
percpu.c:	for (md_block = chunk->md_blocks;
percpu.c:	     md_block != chunk->md_blocks + pcpu_chunk_nr_blocks(chunk);
percpu.c:		md_block->contig_hint = PCPU_BITMAP_BLOCK_BITS;
percpu.c:		md_block->left_free = PCPU_BITMAP_BLOCK_BITS;
percpu.c:		md_block->right_free = PCPU_BITMAP_BLOCK_BITS;
percpu.c: * pcpu_alloc_first_chunk - creates chunks that serve the first chunk
percpu.c:	start_offset = tmp_addr - aligned_addr;
percpu.c:	INIT_LIST_HEAD(&chunk->list);
percpu.c:	chunk->base_addr = (void *)aligned_addr;
percpu.c:	chunk->start_offset = start_offset;
percpu.c:	chunk->end_offset = region_size - chunk->start_offset - map_size;
percpu.c:	chunk->nr_pages = region_size >> PAGE_SHIFT;
percpu.c:	chunk->alloc_map = memblock_virt_alloc(BITS_TO_LONGS(region_bits) *
percpu.c:					       sizeof(chunk->alloc_map[0]), 0);
percpu.c:	chunk->bound_map = memblock_virt_alloc(BITS_TO_LONGS(region_bits + 1) *
percpu.c:					       sizeof(chunk->bound_map[0]), 0);
percpu.c:	chunk->md_blocks = memblock_virt_alloc(pcpu_chunk_nr_blocks(chunk) *
percpu.c:					       sizeof(chunk->md_blocks[0]), 0);
percpu.c:	chunk->immutable = true;
percpu.c:	bitmap_fill(chunk->populated, chunk->nr_pages);
percpu.c:	chunk->nr_populated = chunk->nr_pages;
percpu.c:	chunk->nr_empty_pop_pages =
percpu.c:	chunk->contig_bits = map_size / PCPU_MIN_ALLOC_SIZE;
percpu.c:	chunk->free_bytes = map_size;
percpu.c:	if (chunk->start_offset) {
percpu.c:		offset_bits = chunk->start_offset / PCPU_MIN_ALLOC_SIZE;
percpu.c:		bitmap_set(chunk->alloc_map, 0, offset_bits);
percpu.c:		set_bit(0, chunk->bound_map);
percpu.c:		set_bit(offset_bits, chunk->bound_map);
percpu.c:		chunk->first_bit = offset_bits;
percpu.c:	if (chunk->end_offset) {
percpu.c:		offset_bits = chunk->end_offset / PCPU_MIN_ALLOC_SIZE;
percpu.c:		bitmap_set(chunk->alloc_map,
percpu.c:			   pcpu_chunk_map_bits(chunk) - offset_bits,
percpu.c:			chunk->bound_map);
percpu.c:		set_bit(region_bits, chunk->bound_map);
percpu.c:					     - offset_bits, offset_bits);
percpu.c:	INIT_LIST_HEAD(&chunk->list);
percpu.c:	chunk->nr_pages = pcpu_unit_pages;
percpu.c:	chunk->alloc_map = pcpu_mem_zalloc(BITS_TO_LONGS(region_bits) *
percpu.c:					   sizeof(chunk->alloc_map[0]), gfp);
percpu.c:	if (!chunk->alloc_map)
percpu.c:	chunk->bound_map = pcpu_mem_zalloc(BITS_TO_LONGS(region_bits + 1) *
percpu.c:					   sizeof(chunk->bound_map[0]), gfp);
percpu.c:	if (!chunk->bound_map)
percpu.c:	chunk->md_blocks = pcpu_mem_zalloc(pcpu_chunk_nr_blocks(chunk) *
percpu.c:					   sizeof(chunk->md_blocks[0]), gfp);
percpu.c:	if (!chunk->md_blocks)
percpu.c:	chunk->contig_bits = region_bits;
percpu.c:	chunk->free_bytes = chunk->nr_pages * PAGE_SIZE;
percpu.c:	pcpu_mem_free(chunk->bound_map);
percpu.c:	pcpu_mem_free(chunk->alloc_map);
percpu.c:	pcpu_mem_free(chunk->bound_map);
percpu.c:	pcpu_mem_free(chunk->alloc_map);
percpu.c: * pcpu_chunk_populated - post-population bookkeeping
percpu.c:	int nr = page_end - page_start;
percpu.c:	bitmap_set(chunk->populated, page_start, nr);
percpu.c:	chunk->nr_populated += nr;
percpu.c:		chunk->nr_empty_pop_pages += nr;
percpu.c: * pcpu_chunk_depopulated - post-depopulation bookkeeping
percpu.c:	int nr = page_end - page_start;
percpu.c:	bitmap_clear(chunk->populated, page_start, nr);
percpu.c:	chunk->nr_populated -= nr;
percpu.c:	chunk->nr_empty_pop_pages -= nr;
percpu.c:	pcpu_nr_empty_pop_pages -= nr;
percpu.c: * pcpu_populate_chunk		- populate the specified range of a chunk
percpu.c: * pcpu_depopulate_chunk	- depopulate the specified range of a chunk
percpu.c: * pcpu_create_chunk		- create a new chunk
percpu.c: * pcpu_destroy_chunk		- destroy a chunk, always preceded by full depop
percpu.c: * pcpu_addr_to_page		- translate address to physical address
percpu.c: * pcpu_verify_alloc_info	- check alloc_info is acceptable during init
percpu.c:#include "percpu-km.c"
percpu.c:#include "percpu-vm.c"
percpu.c: * pcpu_chunk_addr_search - determine chunk containing specified address
percpu.c: * pcpu_alloc - the percpu allocator
percpu.c:	 * of up to PCPU_MIN_ALLOC_SIZE - 1 bytes.
percpu.c:	if (list_empty(&pcpu_slot[pcpu_nr_slots - 1])) {
percpu.c:		pcpu_chunk_relocate(chunk, -1);
percpu.c:		pcpu_for_each_unpop_region(chunk->populated, rs, re,
percpu.c:			WARN_ON(chunk->immutable);
percpu.c:	ptr = __addr_to_pcpu_ptr(chunk->base_addr + off);
percpu.c:			chunk->base_addr, off, ptr);
percpu.c:		if (!--warn_limit)
percpu.c: * __alloc_percpu_gfp - allocate dynamic percpu area
percpu.c: * Allocate zero-filled percpu area of @size bytes aligned at @align.  If
percpu.c: * __alloc_percpu - allocate dynamic percpu area
percpu.c: * __alloc_reserved_percpu - allocate reserved percpu area
percpu.c: * Allocate zero-filled percpu area of @size bytes aligned at @align
percpu.c: * pcpu_balance_workfn - manage the amount of free chunks and populated pages
percpu.c:	struct list_head *free_head = &pcpu_slot[pcpu_nr_slots - 1];
percpu.c:		WARN_ON(chunk->immutable);
percpu.c:		list_move(&chunk->list, &to_free);
percpu.c:		pcpu_for_each_pop_region(chunk->populated, rs, re, 0,
percpu.c:					 chunk->nr_pages) {
percpu.c:		nr_to_pop = clamp(PCPU_EMPTY_POP_PAGES_HIGH -
percpu.c:			nr_unpop = chunk->nr_pages - chunk->nr_populated;
percpu.c:		pcpu_for_each_unpop_region(chunk->populated, rs, re, 0,
percpu.c:					   chunk->nr_pages) {
percpu.c:			int nr = min(re - rs, nr_to_pop);
percpu.c:				nr_to_pop -= nr;
percpu.c:			pcpu_chunk_relocate(chunk, -1);
percpu.c: * free_percpu - free percpu area
percpu.c:	off = addr - chunk->base_addr;
percpu.c:	if (chunk->free_bytes == pcpu_unit_size) {
percpu.c:		list_for_each_entry(pos, &pcpu_slot[pcpu_nr_slots - 1], list)
percpu.c:	trace_percpu_free_percpu(chunk->base_addr, off, ptr);
percpu.c:	const size_t static_size = __per_cpu_end - __per_cpu_start;
percpu.c:				*can_addr = (unsigned long) (va - start);
percpu.c: * is_kernel_percpu_address - test whether address is from static percpu area
percpu.c: * Test whether @addr belongs to in-kernel static percpu area.  Module
percpu.c: * %true if @addr is from in-kernel static percpu area, %false otherwise.
percpu.c: * per_cpu_ptr_to_phys - convert translated percpu address to physical address
percpu.c: * pcpu_alloc_alloc_info - allocate percpu allocation info
percpu.c:	base_size = ALIGN(sizeof(*ai) + nr_groups * sizeof(ai->groups[0]),
percpu.c:			  __alignof__(ai->groups[0].cpu_map[0]));
percpu.c:	ai_size = base_size + nr_units * sizeof(ai->groups[0].cpu_map[0]);
percpu.c:	ai->groups[0].cpu_map = ptr;
percpu.c:		ai->groups[0].cpu_map[unit] = NR_CPUS;
percpu.c:	ai->nr_groups = nr_groups;
percpu.c:	ai->__ai_size = PFN_ALIGN(ai_size);
percpu.c: * pcpu_free_alloc_info - free percpu allocation info
percpu.c:	memblock_free_early(__pa(ai), ai->__ai_size);
percpu.c: * pcpu_dump_alloc_info - print out information about pcpu_alloc_info
percpu.c:	char empty_str[] = "--------";
percpu.c:	v = ai->nr_groups;
percpu.c:	empty_str[min_t(int, cpu_width, sizeof(empty_str) - 1)] = '\0';
percpu.c:	upa = ai->alloc_size / ai->unit_size;
percpu.c:	printk("%spcpu-alloc: s%zu r%zu d%zu u%zu alloc=%zu*%zu",
percpu.c:	       lvl, ai->static_size, ai->reserved_size, ai->dyn_size,
percpu.c:	       ai->unit_size, ai->alloc_size / ai->atom_size, ai->atom_size);
percpu.c:	for (group = 0; group < ai->nr_groups; group++) {
percpu.c:		const struct pcpu_group_info *gi = &ai->groups[group];
percpu.c:		BUG_ON(gi->nr_units % upa);
percpu.c:		for (alloc_end += gi->nr_units / upa;
percpu.c:				printk("%spcpu-alloc: ", lvl);
percpu.c:				if (gi->cpu_map[unit] != NR_CPUS)
percpu.c:						cpu_width, gi->cpu_map[unit]);
percpu.c: * pcpu_setup_first_chunk - initialize the first percpu chunk
percpu.c: * @ai->static_size is the size of static percpu area.
percpu.c: * @ai->reserved_size, if non-zero, specifies the amount of bytes to
percpu.c: * @ai->dyn_size determines the number of bytes available for dynamic
percpu.c: * allocation in the first chunk.  The area between @ai->static_size +
percpu.c: * @ai->reserved_size + @ai->dyn_size and @ai->unit_size is unused.
percpu.c: * @ai->unit_size specifies unit size and must be aligned to PAGE_SIZE
percpu.c: * and equal to or larger than @ai->static_size + @ai->reserved_size +
percpu.c: * @ai->dyn_size.
percpu.c: * @ai->atom_size is the allocation atom size and used as alignment
percpu.c: * @ai->alloc_size is the allocation size and always multiple of
percpu.c: * @ai->atom_size.  This is larger than @ai->atom_size if
percpu.c: * @ai->unit_size is larger than @ai->atom_size.
percpu.c: * @ai->nr_groups and @ai->groups describe virtual memory layout of
percpu.c: * groupings.  If @ai->nr_groups is zero, a single group containing
percpu.c: * chunk also contains a reserved region, it is served by two chunks -
percpu.c: * 0 on success, -errno on failure.
percpu.c:	size_t size_sum = ai->static_size + ai->reserved_size + ai->dyn_size;
percpu.c:	PCPU_SETUP_BUG_ON(ai->nr_groups <= 0);
percpu.c:	PCPU_SETUP_BUG_ON(!ai->static_size);
percpu.c:	PCPU_SETUP_BUG_ON(ai->unit_size < size_sum);
percpu.c:	PCPU_SETUP_BUG_ON(offset_in_page(ai->unit_size));
percpu.c:	PCPU_SETUP_BUG_ON(ai->unit_size < PCPU_MIN_UNIT_SIZE);
percpu.c:	PCPU_SETUP_BUG_ON(!IS_ALIGNED(ai->unit_size, PCPU_BITMAP_BLOCK_SIZE));
percpu.c:	PCPU_SETUP_BUG_ON(ai->dyn_size < PERCPU_DYNAMIC_EARLY_SIZE);
percpu.c:	PCPU_SETUP_BUG_ON(!ai->dyn_size);
percpu.c:	PCPU_SETUP_BUG_ON(!IS_ALIGNED(ai->reserved_size, PCPU_MIN_ALLOC_SIZE));
percpu.c:	group_offsets = memblock_virt_alloc(ai->nr_groups *
percpu.c:	group_sizes = memblock_virt_alloc(ai->nr_groups *
percpu.c:	for (group = 0, unit = 0; group < ai->nr_groups; group++, unit += i) {
percpu.c:		const struct pcpu_group_info *gi = &ai->groups[group];
percpu.c:		group_offsets[group] = gi->base_offset;
percpu.c:		group_sizes[group] = gi->nr_units * ai->unit_size;
percpu.c:		for (i = 0; i < gi->nr_units; i++) {
percpu.c:			cpu = gi->cpu_map[i];
percpu.c:			unit_off[cpu] = gi->base_offset + i * ai->unit_size;
percpu.c:	pcpu_nr_groups = ai->nr_groups;
percpu.c:	pcpu_unit_pages = ai->unit_size >> PAGE_SHIFT;
percpu.c:	pcpu_atom_size = ai->atom_size;
percpu.c:	static_size = ALIGN(ai->static_size, PCPU_MIN_ALLOC_SIZE);
percpu.c:	dyn_size = ai->dyn_size - (static_size - ai->static_size);
percpu.c:	 * If the reserved_size is non-zero, this initializes the reserved
percpu.c:	map_size = ai->reserved_size ?: dyn_size;
percpu.c:	if (ai->reserved_size) {
percpu.c:			   ai->reserved_size;
percpu.c:	pcpu_nr_empty_pop_pages = pcpu_first_chunk->nr_empty_pop_pages;
percpu.c:	pcpu_chunk_relocate(pcpu_first_chunk, -1);
percpu.c:		return -EINVAL;
percpu.c: * pcpu_build_alloc_info - build alloc_info considering distances between CPUs
percpu.c:	const size_t static_size = __per_cpu_end - __per_cpu_start;
percpu.c:	dyn_size = size_sum - static_size - reserved_size;
percpu.c:		upa--;
percpu.c:	for (upa = max_upa; upa; upa--) {
percpu.c:			wasted += this_allocs * upa - group_cnt[group];
percpu.c:		 * greater-than comparison ensures upa==1 always
percpu.c:		return ERR_PTR(-ENOMEM);
percpu.c:	cpu_map = ai->groups[0].cpu_map;
percpu.c:		ai->groups[group].cpu_map = cpu_map;
percpu.c:	ai->static_size = static_size;
percpu.c:	ai->reserved_size = reserved_size;
percpu.c:	ai->dyn_size = dyn_size;
percpu.c:	ai->unit_size = alloc_size / upa;
percpu.c:	ai->atom_size = atom_size;
percpu.c:	ai->alloc_size = alloc_size;
percpu.c:		struct pcpu_group_info *gi = &ai->groups[group];
percpu.c:		 * back-to-back.  The caller should update this to
percpu.c:		gi->base_offset = unit * ai->unit_size;
percpu.c:				gi->cpu_map[gi->nr_units++] = cpu;
percpu.c:		gi->nr_units = roundup(gi->nr_units, upa);
percpu.c:		unit += gi->nr_units;
percpu.c: * pcpu_embed_first_chunk - embed the first percpu chunk into bootmem
percpu.c: * by calling @alloc_fn and used as-is without being mapped into
percpu.c: * can result in very sparse cpu->unit mapping on NUMA machines thus
percpu.c: * 0 on success, -errno on failure.
percpu.c:	size_sum = ai->static_size + ai->reserved_size + ai->dyn_size;
percpu.c:	areas_size = PFN_ALIGN(ai->nr_groups * sizeof(void *));
percpu.c:		rc = -ENOMEM;
percpu.c:	for (group = 0; group < ai->nr_groups; group++) {
percpu.c:		struct pcpu_group_info *gi = &ai->groups[group];
percpu.c:		for (i = 0; i < gi->nr_units && cpu == NR_CPUS; i++)
percpu.c:			cpu = gi->cpu_map[i];
percpu.c:		ptr = alloc_fn(cpu, gi->nr_units * ai->unit_size, atom_size);
percpu.c:			rc = -ENOMEM;
percpu.c:	max_distance = areas[highest_group] - base;
percpu.c:	max_distance += ai->unit_size * ai->groups[highest_group].nr_units;
percpu.c:		rc = -EINVAL;
percpu.c:	for (group = 0; group < ai->nr_groups; group++) {
percpu.c:		struct pcpu_group_info *gi = &ai->groups[group];
percpu.c:		for (i = 0; i < gi->nr_units; i++, ptr += ai->unit_size) {
percpu.c:			if (gi->cpu_map[i] == NR_CPUS) {
percpu.c:				free_fn(ptr, ai->unit_size);
percpu.c:			memcpy(ptr, __per_cpu_load, ai->static_size);
percpu.c:			free_fn(ptr + size_sum, ai->unit_size - size_sum);
percpu.c:	for (group = 0; group < ai->nr_groups; group++) {
percpu.c:		ai->groups[group].base_offset = areas[group] - base;
percpu.c:		PFN_DOWN(size_sum), base, ai->static_size, ai->reserved_size,
percpu.c:		ai->dyn_size, ai->unit_size);
percpu.c:	for (group = 0; group < ai->nr_groups; group++)
percpu.c:				ai->groups[group].nr_units * ai->unit_size);
percpu.c: * pcpu_page_first_chunk - map the first chunk using PAGE_SIZE pages
percpu.c: * This is a helper to ease setting up page-remapped first percpu
percpu.c: * page-by-page into vmalloc area.
percpu.c: * 0 on success, -errno on failure.
percpu.c:	BUG_ON(ai->nr_groups != 1);
percpu.c:	upa = ai->alloc_size/ai->unit_size;
percpu.c:	if (unlikely(WARN_ON(ai->groups[0].nr_units != nr_g0_units))) {
percpu.c:		return -EINVAL;
percpu.c:	unit_pages = ai->unit_size >> PAGE_SHIFT;
percpu.c:		unsigned int cpu = ai->groups[0].cpu_map[unit];
percpu.c:	vm.size = num_possible_cpus() * ai->unit_size;
percpu.c:			(unsigned long)vm.addr + unit * ai->unit_size;
percpu.c:		 * cache for the linear mapping here - something
percpu.c:		memcpy((void *)unit_addr, __per_cpu_load, ai->static_size);
percpu.c:		unit_pages, psize_str, vm.addr, ai->static_size,
percpu.c:		ai->reserved_size, ai->dyn_size);
percpu.c:	while (--j >= 0)
percpu.c:	rc = -ENOMEM;
percpu.c: * the original non-dynamic generic percpu area setup.  This is
percpu.c: * location.  As an added bonus, in non-NUMA cases, embedding is
percpu.c: * generally a good idea TLB-wise because percpu area can piggy back
percpu.c:	delta = (unsigned long)pcpu_base_addr - (unsigned long)__per_cpu_start;
percpu.c: * UP always uses km-based percpu allocator with identity mapping.
percpu.c:	ai->dyn_size = unit_size;
percpu.c:	ai->unit_size = unit_size;
percpu.c:	ai->atom_size = unit_size;
percpu.c:	ai->alloc_size = unit_size;
percpu.c:	ai->groups[0].nr_units = 1;
percpu.c:	ai->groups[0].cpu_map[0] = 0;
internal.h:#include <linux/tracepoint-defs.h>
internal.h:	return !(vma->vm_flags & (VM_LOCKED|VM_HUGETLB|VM_PFNMAP));
internal.h: * Submit IO for the read-ahead request in file_ra_state.
internal.h:					ra->start, ra->size, ra->async_size);
internal.h: * Turn a non-refcounted page (->_refcount == 0) into refcounted with
internal.h:#define ac_classzone_idx(ac) zonelist_zone_idx(ac->preferred_zoneref)
internal.h:	if (zone->contiguous)
internal.h: * general, page_zone(page)->lock must be held by the caller to prevent the
internal.h: * If a caller does not hold page_zone(page)->lock, it must guarantee that the
internal.h: * Executable code area - executable, not writable, not stack
internal.h: * Stack area - atomatically grows in one direction
internal.h: * Data area - private, writable, not stack
internal.h:	munlock_vma_pages_range(vma, vma->vm_start, vma->vm_end);
internal.h: * we want to unconditionally remove a page from the pagecache -- e.g.,
internal.h: * is revert to lazy LRU behaviour -- semantics are not broken.
internal.h: * mlock_migrate_page - called only from migrate_misplaced_transhuge_page()
internal.h:		__mod_zone_page_state(page_zone(page), NR_MLOCK, -nr_pages);
internal.h:	return vma->vm_start + ((pgoff - vma->vm_pgoff) << PAGE_SHIFT);
internal.h:	end = start + PAGE_SIZE * (hpage_nr_pages(page) - 1);
internal.h:	VM_BUG_ON_VMA(end < vma->vm_start || start >= vma->vm_end, vma);
internal.h:	return max(start, vma->vm_start);
internal.h:	if (unlikely((offset & (MAX_ORDER_NR_PAGES - 1)) == 0)) {
internal.h:#define NODE_RECLAIM_NOSCAN	-2
internal.h:#define NODE_RECLAIM_FULL	-1
internal.h:/* The ALLOC_WMARK bits are used as an index to zone->watermark */
internal.h:#define ALLOC_WMARK_MASK	(ALLOC_NO_WATERMARKS-1)
internal.h: * Only MMU archs have async oom victim reclaim - aka oom_reaper so we
pgtable-generic.c:// SPDX-License-Identifier: GPL-2.0
pgtable-generic.c: *  mm/pgtable-generic.c
pgtable-generic.c: *  Generic pgtable methods declared in asm-generic/pgtable.h
pgtable-generic.c:#include <asm-generic/pgtable.h>
pgtable-generic.c:		set_pte_at(vma->vm_mm, address, ptep, entry);
pgtable-generic.c:	struct mm_struct *mm = (vma)->vm_mm;
pgtable-generic.c:		set_pmd_at(vma->vm_mm, address, pmdp, entry);
pgtable-generic.c:	pmd = pmdp_huge_get_and_clear(vma->vm_mm, address, pmdp);
pgtable-generic.c:	pud = pudp_huge_get_and_clear(vma->vm_mm, address, pudp);
pgtable-generic.c:		INIT_LIST_HEAD(&pgtable->lru);
pgtable-generic.c:		list_add(&pgtable->lru, &pmd_huge_pte(mm, pmdp)->lru);
pgtable-generic.c:	pmd_huge_pte(mm, pmdp) = list_first_entry_or_null(&pgtable->lru,
pgtable-generic.c:		list_del(&pgtable->lru);
pgtable-generic.c:	set_pmd_at(vma->vm_mm, address, pmdp, pmd_mknotpresent(entry));
pgtable-generic.c:	pmd = pmdp_huge_get_and_clear(vma->vm_mm, address, pmdp);
page_ext.c:// SPDX-License-Identifier: GPL-2.0
page_ext.c: * At last, enlarging struct page could cause un-wanted system behaviour change.
page_ext.c: * memory allocation at boot-time. The other is optional, init callback, which
page_ext.c: * extra memory through size in struct page_ext_operations. If it is non-zero,
page_ext.c:		if (page_ext_ops[i]->need && page_ext_ops[i]->need()) {
page_ext.c:			page_ext_ops[i]->offset = sizeof(struct page_ext) +
page_ext.c:			extra_mem += page_ext_ops[i]->size;
page_ext.c:		if (page_ext_ops[i]->init)
page_ext.c:			page_ext_ops[i]->init();
page_ext.c:	pgdat->node_page_ext = NULL;
page_ext.c:	base = NODE_DATA(page_to_nid(page))->node_page_ext;
page_ext.c:	index = pfn - round_down(node_start_pfn(page_to_nid(page)),
page_ext.c:	nr_pages = NODE_DATA(nid)->node_spanned_pages;
page_ext.c:		return -ENOMEM;
page_ext.c:	NODE_DATA(nid)->node_page_ext = base;
page_ext.c:	if (!section->page_ext)
page_ext.c:	return get_entry(section->page_ext, pfn);
page_ext.c:	if (section->page_ext)
page_ext.c:	 * The value stored in section->page_ext is (base - pfn)
page_ext.c:		return -ENOMEM;
page_ext.c:	section->page_ext = (void *)base - get_entry_size() * pfn;
page_ext.c:	if (!ms || !ms->page_ext)
page_ext.c:	base = get_entry(ms->page_ext, pfn);
page_ext.c:	ms->page_ext = NULL;
page_ext.c:	if (nid == -1) {
page_ext.c:	return -ENOMEM;
page_ext.c:		ret = online_page_ext(mn->start_pfn,
page_ext.c:				   mn->nr_pages, mn->status_change_nid);
page_ext.c:		offline_page_ext(mn->start_pfn,
page_ext.c:				mn->nr_pages, mn->status_change_nid);
page_ext.c:		offline_page_ext(mn->start_pfn,
page_ext.c:				mn->nr_pages, mn->status_change_nid);
page_ext.c:		 * page->flags of out of node pages are not initialized.  So we
page_ext.c:			 * -------------pfn-------------->
memcontrol.c:/* memcontrol.c - Memory Controller
memcontrol.c:#include <linux/page-flags.h>
memcontrol.c:#include <linux/backing-dev.h>
memcontrol.c: * Cgroups above their limits are maintained in a RB-Tree, independent of
memcontrol.c:/* for encoding cft->private value on file */
memcontrol.c:	return &memcg->vmpressure;
memcontrol.c:	return &container_of(vmpr, struct mem_cgroup, vmpressure)->css;
memcontrol.c: * This will be the memcg's index in each cache's ->memcg_params.memcg_caches.
memcontrol.c: *  but only a few kmem-limited. Or also, if we have, for instance, 200
memcontrol.c: *  memcgs, and none but the 200th is kmem-limited, we'd have to have a
memcontrol.c: * the alloc/free process all the time. In a small machine, 4 kmem-limited
memcontrol.c: * mem_cgroup_css_from_page - css of the memcg associated with a page
memcontrol.c:	memcg = page->mem_cgroup;
memcontrol.c:	return &memcg->css;
memcontrol.c: * page_cgroup_ino - return inode number of the memcg a page is charged to
memcontrol.c:	memcg = READ_ONCE(page->mem_cgroup);
memcontrol.c:	while (memcg && !(memcg->css.flags & CSS_ONLINE))
memcontrol.c:		ino = cgroup_ino(memcg->css.cgroup);
memcontrol.c:	return memcg->nodeinfo[nid];
memcontrol.c:	struct rb_node **p = &mctz->rb_root.rb_node;
memcontrol.c:	if (mz->on_tree)
memcontrol.c:	mz->usage_in_excess = new_usage_in_excess;
memcontrol.c:	if (!mz->usage_in_excess)
memcontrol.c:		if (mz->usage_in_excess < mz_node->usage_in_excess) {
memcontrol.c:			p = &(*p)->rb_left;
memcontrol.c:		else if (mz->usage_in_excess >= mz_node->usage_in_excess)
memcontrol.c:			p = &(*p)->rb_right;
memcontrol.c:		mctz->rb_rightmost = &mz->tree_node;
memcontrol.c:	rb_link_node(&mz->tree_node, parent, p);
memcontrol.c:	rb_insert_color(&mz->tree_node, &mctz->rb_root);
memcontrol.c:	mz->on_tree = true;
memcontrol.c:	if (!mz->on_tree)
memcontrol.c:	if (&mz->tree_node == mctz->rb_rightmost)
memcontrol.c:		mctz->rb_rightmost = rb_prev(&mz->tree_node);
memcontrol.c:	rb_erase(&mz->tree_node, &mctz->rb_root);
memcontrol.c:	mz->on_tree = false;
memcontrol.c:	spin_lock_irqsave(&mctz->lock, flags);
memcontrol.c:	spin_unlock_irqrestore(&mctz->lock, flags);
memcontrol.c:	unsigned long nr_pages = page_counter_read(&memcg->memory);
memcontrol.c:	unsigned long soft_limit = READ_ONCE(memcg->soft_limit);
memcontrol.c:		excess = nr_pages - soft_limit;
memcontrol.c:		 * We have to update the tree if mz is on RB-tree or
memcontrol.c:		if (excess || mz->on_tree) {
memcontrol.c:			spin_lock_irqsave(&mctz->lock, flags);
memcontrol.c:			/* if on-tree, remove it */
memcontrol.c:			if (mz->on_tree)
memcontrol.c:			 * Insert again. mz->usage_in_excess will be updated.
memcontrol.c:			spin_unlock_irqrestore(&mctz->lock, flags);
memcontrol.c:	if (!mctz->rb_rightmost)
memcontrol.c:	mz = rb_entry(mctz->rb_rightmost,
memcontrol.c:	if (!soft_limit_excess(mz->memcg) ||
memcontrol.c:	    !css_tryget_online(&mz->memcg->css))
memcontrol.c:	spin_lock_irq(&mctz->lock);
memcontrol.c:	spin_unlock_irq(&mctz->lock);
memcontrol.c: * synchronization to implement "quick" read. There are trade-off between
memcontrol.c: * he accounts memory. Even if we provide quick-and-fuzzy read, we always
memcontrol.c: * If there are kernel internal actions which can make use of some not-exact
memcontrol.c:		val += per_cpu(memcg->stat->events[event], cpu);
memcontrol.c:		__this_cpu_add(memcg->stat->count[MEMCG_RSS], nr_pages);
memcontrol.c:		__this_cpu_add(memcg->stat->count[MEMCG_CACHE], nr_pages);
memcontrol.c:			__this_cpu_add(memcg->stat->count[NR_SHMEM], nr_pages);
memcontrol.c:		__this_cpu_add(memcg->stat->count[MEMCG_RSS_HUGE], nr_pages);
memcontrol.c:		__this_cpu_inc(memcg->stat->events[PGPGIN]);
memcontrol.c:		__this_cpu_inc(memcg->stat->events[PGPGOUT]);
memcontrol.c:		nr_pages = -nr_pages; /* for event */
memcontrol.c:	__this_cpu_add(memcg->stat->nr_page_events, nr_pages);
memcontrol.c:	val = __this_cpu_read(memcg->stat->nr_page_events);
memcontrol.c:	next = __this_cpu_read(memcg->stat->targets[target]);
memcontrol.c:	if ((long)(next - val) < 0) {
memcontrol.c:		__this_cpu_write(memcg->stat->targets[target], next);
memcontrol.c:			atomic_inc(&memcg->numainfo_events);
memcontrol.c:	 * mm_update_next_owner() may clear mm->owner to NULL
memcontrol.c:			memcg = mem_cgroup_from_task(rcu_dereference(mm->owner));
memcontrol.c:	} while (!css_tryget_online(&memcg->css));
memcontrol.c: * mem_cgroup_iter - iterate over memory cgroup hierarchy
memcontrol.c: * @root itself, or %NULL after a full round-trip.
memcontrol.c: * to cancel a hierarchy walk before the round-trip is complete.
memcontrol.c:	if (!root->use_hierarchy && root != root_mem_cgroup) {
memcontrol.c:		mz = mem_cgroup_nodeinfo(root, reclaim->pgdat->node_id);
memcontrol.c:		iter = &mz->iter[reclaim->priority];
memcontrol.c:		if (prev && reclaim->generation != iter->generation)
memcontrol.c:			pos = READ_ONCE(iter->position);
memcontrol.c:			if (!pos || css_tryget(&pos->css))
memcontrol.c:			 * css reference reached zero, so iter->position will
memcontrol.c:			 * be cleared by ->css_released. However, we should not
memcontrol.c:			 * rely on this happening soon, because ->css_released
memcontrol.c:			 * is called from a work queue, and by busy-waiting we
memcontrol.c:			 * might block it. So we clear iter->position right
memcontrol.c:			(void)cmpxchg(&iter->position, pos, NULL);
memcontrol.c:		css = &pos->css;
memcontrol.c:		css = css_next_descendant_pre(css, &root->css);
memcontrol.c:			 * the hierarchy - make sure they see at least
memcontrol.c:		if (css == &root->css)
memcontrol.c:		(void)cmpxchg(&iter->position, pos, memcg);
memcontrol.c:			css_put(&pos->css);
memcontrol.c:			iter->generation++;
memcontrol.c:			reclaim->generation = iter->generation;
memcontrol.c:		css_put(&prev->css);
memcontrol.c: * mem_cgroup_iter_break - abort a hierarchy walk prematurely
memcontrol.c:		css_put(&prev->css);
memcontrol.c:				iter = &mz->iter[i];
memcontrol.c:				cmpxchg(&iter->position,
memcontrol.c: * mem_cgroup_scan_tasks - iterate over tasks of a memory cgroup hierarchy
memcontrol.c: * descendants and calls @fn for each task. If @fn returns a non-zero
memcontrol.c:		css_task_iter_start(&iter->css, 0, &it);
memcontrol.c: * mem_cgroup_page_lruvec - return lruvec for isolating/putting an LRU page
memcontrol.c:		lruvec = &pgdat->lruvec;
memcontrol.c:	memcg = page->mem_cgroup;
memcontrol.c:	 * Swapcache readahead pages are added to the LRU - and
memcontrol.c:	 * possibly migrated - before they are charged.
memcontrol.c:	lruvec = &mz->lruvec;
memcontrol.c:	 * we have to be prepared to initialize lruvec->zone here;
memcontrol.c:	if (unlikely(lruvec->pgdat != pgdat))
memcontrol.c:		lruvec->pgdat = pgdat;
memcontrol.c: * mem_cgroup_update_lru_size - account for adding or removing an lru page
memcontrol.c:	lru_size = &mz->lru_zone_size[zid][lru];
memcontrol.c:		task_memcg = get_mem_cgroup_from_mm(p->mm);
memcontrol.c:		css_get(&task_memcg->css);
memcontrol.c:	css_put(&task_memcg->css);
memcontrol.c: * mem_cgroup_margin - calculate chargeable space of a memory cgroup
memcontrol.c:	count = page_counter_read(&memcg->memory);
memcontrol.c:	limit = READ_ONCE(memcg->memory.limit);
memcontrol.c:		margin = limit - count;
memcontrol.c:		count = page_counter_read(&memcg->memsw);
memcontrol.c:		limit = READ_ONCE(memcg->memsw.limit);
memcontrol.c:			margin = min(margin, limit - count);
memcontrol.c: * moving cgroups. This is for waiting at high-memory pressure
memcontrol.c:#define K(x) ((x) << (PAGE_SHIFT-10))
memcontrol.c:	pr_cont_cgroup_path(memcg->css.cgroup);
memcontrol.c:		K((u64)page_counter_read(&memcg->memory)),
memcontrol.c:		K((u64)memcg->memory.limit), memcg->memory.failcnt);
memcontrol.c:		K((u64)page_counter_read(&memcg->memsw)),
memcontrol.c:		K((u64)memcg->memsw.limit), memcg->memsw.failcnt);
memcontrol.c:		K((u64)page_counter_read(&memcg->kmem)),
memcontrol.c:		K((u64)memcg->kmem.limit), memcg->kmem.failcnt);
memcontrol.c:		pr_cont_cgroup_path(iter->css.cgroup);
memcontrol.c:	limit = memcg->memory.limit;
memcontrol.c:		memsw_limit = memcg->memsw.limit;
memcontrol.c:		swap_limit = memcg->swap.limit;
memcontrol.c: * Always updating the nodemask is not very good - even if we have an empty
memcontrol.c:	if (!atomic_read(&memcg->numainfo_events))
memcontrol.c:	if (atomic_inc_return(&memcg->numainfo_updating) > 1)
memcontrol.c:	memcg->scan_nodes = node_states[N_MEMORY];
memcontrol.c:			node_clear(nid, memcg->scan_nodes);
memcontrol.c:	atomic_set(&memcg->numainfo_events, 0);
memcontrol.c:	atomic_set(&memcg->numainfo_updating, 0);
memcontrol.c: * Now, we use round-robin. Better algorithm is welcomed.
memcontrol.c:	node = memcg->last_scanned_node;
memcontrol.c:	node = next_node_in(node, memcg->scan_nodes);
memcontrol.c:	memcg->last_scanned_node = node;
memcontrol.c: * Check OOM-Killer is already running under our hierarchy.
memcontrol.c:		if (iter->oom_lock) {
memcontrol.c:			iter->oom_lock = true;
memcontrol.c:			iter->oom_lock = false;
memcontrol.c:		iter->oom_lock = false;
memcontrol.c:		iter->under_oom++;
memcontrol.c:		if (iter->under_oom > 0)
memcontrol.c:			iter->under_oom--;
memcontrol.c:	oom_wait_memcg = oom_wait_info->memcg;
memcontrol.c:	 * For the following lockless ->under_oom test, the only required
memcontrol.c:	if (memcg && memcg->under_oom)
memcontrol.c:	if (!current->memcg_may_oom)
memcontrol.c:	css_get(&memcg->css);
memcontrol.c:	current->memcg_in_oom = memcg;
memcontrol.c:	current->memcg_oom_gfp_mask = mask;
memcontrol.c:	current->memcg_oom_order = order;
memcontrol.c: * mem_cgroup_oom_synchronize - complete memcg OOM handling
memcontrol.c:	struct mem_cgroup *memcg = current->memcg_in_oom;
memcontrol.c:	if (locked && !memcg->oom_kill_disable) {
memcontrol.c:		mem_cgroup_out_of_memory(memcg, current->memcg_oom_gfp_mask,
memcontrol.c:					 current->memcg_oom_order);
memcontrol.c:		 * There is no guarantee that an OOM-lock contender
memcontrol.c:	current->memcg_in_oom = NULL;
memcontrol.c:	css_put(&memcg->css);
memcontrol.c: * lock_page_memcg - lock a page->mem_cgroup binding
memcontrol.c:	 * path can get away without acquiring the memcg->move_lock
memcontrol.c:	memcg = page->mem_cgroup;
memcontrol.c:	if (atomic_read(&memcg->moving_account) <= 0)
memcontrol.c:	spin_lock_irqsave(&memcg->move_lock, flags);
memcontrol.c:	if (memcg != page->mem_cgroup) {
memcontrol.c:		spin_unlock_irqrestore(&memcg->move_lock, flags);
memcontrol.c:	memcg->move_lock_task = current;
memcontrol.c:	memcg->move_lock_flags = flags;
memcontrol.c: * __unlock_page_memcg - unlock and unpin a memcg
memcontrol.c:	if (memcg && memcg->move_lock_task == current) {
memcontrol.c:		unsigned long flags = memcg->move_lock_flags;
memcontrol.c:		memcg->move_lock_task = NULL;
memcontrol.c:		memcg->move_lock_flags = 0;
memcontrol.c:		spin_unlock_irqrestore(&memcg->move_lock, flags);
memcontrol.c: * unlock_page_memcg - unlock a page->mem_cgroup binding
memcontrol.c:	__unlock_page_memcg(page->mem_cgroup);
memcontrol.c:	if (memcg == stock->cached && stock->nr_pages >= nr_pages) {
memcontrol.c:		stock->nr_pages -= nr_pages;
memcontrol.c:	struct mem_cgroup *old = stock->cached;
memcontrol.c:	if (stock->nr_pages) {
memcontrol.c:		page_counter_uncharge(&old->memory, stock->nr_pages);
memcontrol.c:			page_counter_uncharge(&old->memsw, stock->nr_pages);
memcontrol.c:		css_put_many(&old->css, stock->nr_pages);
memcontrol.c:		stock->nr_pages = 0;
memcontrol.c:	stock->cached = NULL;
memcontrol.c:	clear_bit(FLUSHING_CACHED_CHARGE, &stock->flags);
memcontrol.c:	if (stock->cached != memcg) { /* reset if necessary */
memcontrol.c:		stock->cached = memcg;
memcontrol.c:	stock->nr_pages += nr_pages;
memcontrol.c:	if (stock->nr_pages > CHARGE_BATCH)
memcontrol.c: * Drains all per-CPU charge caches for given root_memcg resp. subtree
memcontrol.c:	 * Notify other cpus that system-wide "drain" is running
memcontrol.c:	 * per-cpu data. CPU up doesn't touch memcg_stock at all.
memcontrol.c:		memcg = stock->cached;
memcontrol.c:		if (!memcg || !stock->nr_pages || !css_tryget(&memcg->css))
memcontrol.c:			css_put(&memcg->css);
memcontrol.c:		if (!test_and_set_bit(FLUSHING_CACHED_CHARGE, &stock->flags)) {
memcontrol.c:				drain_local_stock(&stock->work);
memcontrol.c:				schedule_work_on(cpu, &stock->work);
memcontrol.c:		css_put(&memcg->css);
memcontrol.c:		if (page_counter_read(&memcg->memory) <= memcg->high)
memcontrol.c:	unsigned int nr_pages = current->memcg_nr_pages_over_high;
memcontrol.c:	memcg = get_mem_cgroup_from_mm(current->mm);
memcontrol.c:	css_put(&memcg->css);
memcontrol.c:	current->memcg_nr_pages_over_high = 0;
memcontrol.c:	    page_counter_try_charge(&memcg->memsw, batch, &counter)) {
memcontrol.c:		if (page_counter_try_charge(&memcg->memory, batch, &counter))
memcontrol.c:			page_counter_uncharge(&memcg->memsw, batch);
memcontrol.c:	 * memory shortage.  Allow dying and OOM-killed tasks to
memcontrol.c:		     current->flags & PF_EXITING))
memcontrol.c:	if (unlikely(current->flags & PF_MEMALLOC))
memcontrol.c:	if (nr_retries--)
memcontrol.c:		return -ENOMEM;
memcontrol.c:	page_counter_charge(&memcg->memory, nr_pages);
memcontrol.c:		page_counter_charge(&memcg->memsw, nr_pages);
memcontrol.c:	css_get_many(&memcg->css, nr_pages);
memcontrol.c:	css_get_many(&memcg->css, batch);
memcontrol.c:		refill_stock(memcg, batch - nr_pages);
memcontrol.c:		if (page_counter_read(&memcg->memory) > memcg->high) {
memcontrol.c:				schedule_work(&memcg->high_work);
memcontrol.c:			current->memcg_nr_pages_over_high += batch;
memcontrol.c:	page_counter_uncharge(&memcg->memory, nr_pages);
memcontrol.c:		page_counter_uncharge(&memcg->memsw, nr_pages);
memcontrol.c:	css_put_many(&memcg->css, nr_pages);
memcontrol.c:		lruvec = mem_cgroup_page_lruvec(page, zone->zone_pgdat);
memcontrol.c:		lruvec = mem_cgroup_page_lruvec(page, zone->zone_pgdat);
memcontrol.c:	VM_BUG_ON_PAGE(page->mem_cgroup, page);
memcontrol.c:	 * In some cases, SwapCache and FUSE(splice_buf->radixtree), the page
memcontrol.c:	 * page->mem_cgroup at this point:
memcontrol.c:	 * - the page is uncharged
memcontrol.c:	 * - the page is off-LRU
memcontrol.c:	 * - an anonymous fault has exclusive page access, except for
memcontrol.c:	 * - a page cache insertion, a swapin fault, or a migration
memcontrol.c:	page->mem_cgroup = memcg;
memcontrol.c:	struct mem_cgroup *memcg = cw->memcg;
memcontrol.c:	struct kmem_cache *cachep = cw->cachep;
memcontrol.c:	css_put(&memcg->css);
memcontrol.c: * Enqueue the creation of a per-memcg kmem_cache.
memcontrol.c:	css_get(&memcg->css);
memcontrol.c:	cw->memcg = memcg;
memcontrol.c:	cw->cachep = cachep;
memcontrol.c:	INIT_WORK(&cw->work, memcg_kmem_cache_create_func);
memcontrol.c:	queue_work(memcg_kmem_cache_wq, &cw->work);
memcontrol.c:	current->memcg_kmem_skip_account = 1;
memcontrol.c:	current->memcg_kmem_skip_account = 0;
memcontrol.c:	if (in_interrupt() || !current->mm || (current->flags & PF_KTHREAD))
memcontrol.c: * memcg_kmem_get_cache: select the correct per-memcg cache for allocation
memcontrol.c:	if (current->memcg_kmem_skip_account)
memcontrol.c:	memcg = get_mem_cgroup_from_mm(current->mm);
memcontrol.c:	kmemcg_id = READ_ONCE(memcg->kmemcg_id);
memcontrol.c:	css_put(&memcg->css);
memcontrol.c:		css_put(&cachep->memcg_params.memcg->css);
memcontrol.c:	    !page_counter_try_charge(&memcg->kmem, nr_pages, &counter)) {
memcontrol.c:		return -ENOMEM;
memcontrol.c:	page->mem_cgroup = memcg;
memcontrol.c:	memcg = get_mem_cgroup_from_mm(current->mm);
memcontrol.c:	css_put(&memcg->css);
memcontrol.c:	struct mem_cgroup *memcg = page->mem_cgroup;
memcontrol.c:		page_counter_uncharge(&memcg->kmem, nr_pages);
memcontrol.c:	page_counter_uncharge(&memcg->memory, nr_pages);
memcontrol.c:		page_counter_uncharge(&memcg->memsw, nr_pages);
memcontrol.c:	page->mem_cgroup = NULL;
memcontrol.c:	css_put_many(&memcg->css, nr_pages);
memcontrol.c:		head[i].mem_cgroup = head->mem_cgroup;
memcontrol.c:	__this_cpu_sub(head->mem_cgroup->stat->count[MEMCG_RSS_HUGE],
memcontrol.c:	this_cpu_add(memcg->stat->count[MEMCG_SWAP], nr_entries);
memcontrol.c: * mem_cgroup_move_swap_account - move swap charge and swap_cgroup's record.
memcontrol.c: * Returns 0 on success, -EINVAL on failure.
memcontrol.c:		mem_cgroup_swap_statistics(from, -1);
memcontrol.c:	return -EINVAL;
memcontrol.c:	return -EINVAL;
memcontrol.c:	 * is depends on callers. We set our retry-count to be function
memcontrol.c:	oldusage = page_counter_read(&memcg->memory);
memcontrol.c:			ret = -EINTR;
memcontrol.c:		if (limit > memcg->memsw.limit) {
memcontrol.c:			ret = -EINVAL;
memcontrol.c:		if (limit > memcg->memory.limit)
memcontrol.c:		ret = page_counter_limit(&memcg->memory, limit);
memcontrol.c:		curusage = page_counter_read(&memcg->memory);
memcontrol.c:			retry_count--;
memcontrol.c:	oldusage = page_counter_read(&memcg->memsw);
memcontrol.c:			ret = -EINTR;
memcontrol.c:		if (limit < memcg->memory.limit) {
memcontrol.c:			ret = -EINVAL;
memcontrol.c:		if (limit > memcg->memsw.limit)
memcontrol.c:		ret = page_counter_limit(&memcg->memsw, limit);
memcontrol.c:		curusage = page_counter_read(&memcg->memsw);
memcontrol.c:			retry_count--;
memcontrol.c:	mctz = soft_limit_tree_node(pgdat->node_id);
memcontrol.c:	if (!mctz || RB_EMPTY_ROOT(&mctz->rb_root))
memcontrol.c:		reclaimed = mem_cgroup_soft_reclaim(mz->memcg, pgdat,
memcontrol.c:		spin_lock_irq(&mctz->lock);
memcontrol.c:		excess = soft_limit_excess(mz->memcg);
memcontrol.c:		spin_unlock_irq(&mctz->lock);
memcontrol.c:		css_put(&mz->memcg->css);
memcontrol.c:		css_put(&next_mz->memcg->css);
memcontrol.c:	ret = css_next_child(NULL, &memcg->css);
memcontrol.c:	/* we call try-to-free pages for make this cgroup empty */
memcontrol.c:	while (nr_retries && page_counter_read(&memcg->memory)) {
memcontrol.c:			return -EINTR;
memcontrol.c:			nr_retries--;
memcontrol.c:		return -EINVAL;
memcontrol.c:	return mem_cgroup_from_css(css)->use_hierarchy;
memcontrol.c:	struct mem_cgroup *parent_memcg = mem_cgroup_from_css(memcg->css.parent);
memcontrol.c:	if (memcg->use_hierarchy == val)
memcontrol.c:	if ((!parent_memcg || !parent_memcg->use_hierarchy) &&
memcontrol.c:			memcg->use_hierarchy = val;
memcontrol.c:			retval = -EBUSY;
memcontrol.c:		retval = -EINVAL;
memcontrol.c:			val = page_counter_read(&memcg->memory);
memcontrol.c:			val = page_counter_read(&memcg->memsw);
memcontrol.c:	switch (MEMFILE_TYPE(cft->private)) {
memcontrol.c:		counter = &memcg->memory;
memcontrol.c:		counter = &memcg->memsw;
memcontrol.c:		counter = &memcg->kmem;
memcontrol.c:		counter = &memcg->tcpmem;
memcontrol.c:	switch (MEMFILE_ATTR(cft->private)) {
memcontrol.c:		if (counter == &memcg->memory)
memcontrol.c:		if (counter == &memcg->memsw)
memcontrol.c:		return (u64)counter->limit * PAGE_SIZE;
memcontrol.c:		return (u64)counter->watermark * PAGE_SIZE;
memcontrol.c:		return counter->failcnt;
memcontrol.c:		return (u64)memcg->soft_limit * PAGE_SIZE;
memcontrol.c:	BUG_ON(memcg->kmemcg_id >= 0);
memcontrol.c:	BUG_ON(memcg->kmem_state);
memcontrol.c:	 * A memory cgroup is considered kmem-online as soon as it gets
memcontrol.c:	memcg->kmemcg_id = memcg_id;
memcontrol.c:	memcg->kmem_state = KMEM_ONLINE;
memcontrol.c:	INIT_LIST_HEAD(&memcg->kmem_caches);
memcontrol.c:	if (memcg->kmem_state != KMEM_ONLINE)
memcontrol.c:	memcg->kmem_state = KMEM_ALLOCATED;
memcontrol.c:	kmemcg_id = memcg->kmemcg_id;
memcontrol.c:	 * ordering is imposed by list_lru_node->lock taken by
memcontrol.c:	css_for_each_descendant_pre(css, &memcg->css) {
memcontrol.c:		BUG_ON(child->kmemcg_id != kmemcg_id);
memcontrol.c:		child->kmemcg_id = parent->kmemcg_id;
memcontrol.c:		if (!memcg->use_hierarchy)
memcontrol.c:	memcg_drain_all_list_lrus(kmemcg_id, parent->kmemcg_id);
memcontrol.c:	if (unlikely(memcg->kmem_state == KMEM_ONLINE))
memcontrol.c:	if (memcg->kmem_state == KMEM_ALLOCATED) {
memcontrol.c:		WARN_ON(page_counter_read(&memcg->kmem));
memcontrol.c:	ret = page_counter_limit(&memcg->kmem, limit);
memcontrol.c:	ret = page_counter_limit(&memcg->tcpmem, limit);
memcontrol.c:	if (!memcg->tcpmem_active) {
memcontrol.c:		memcg->tcpmem_active = true;
memcontrol.c:	ret = page_counter_memparse(buf, "-1", &nr_pages);
memcontrol.c:	switch (MEMFILE_ATTR(of_cft(of)->private)) {
memcontrol.c:			ret = -EINVAL;
memcontrol.c:		switch (MEMFILE_TYPE(of_cft(of)->private)) {
memcontrol.c:		memcg->soft_limit = nr_pages;
memcontrol.c:	switch (MEMFILE_TYPE(of_cft(of)->private)) {
memcontrol.c:		counter = &memcg->memory;
memcontrol.c:		counter = &memcg->memsw;
memcontrol.c:		counter = &memcg->kmem;
memcontrol.c:		counter = &memcg->tcpmem;
memcontrol.c:	switch (MEMFILE_ATTR(of_cft(of)->private)) {
memcontrol.c:		counter->failcnt = 0;
memcontrol.c:	return mem_cgroup_from_css(css)->move_charge_at_immigrate;
memcontrol.c:		return -EINVAL;
memcontrol.c:	 * No kind of locking is needed in here, because ->can_attach() will
memcontrol.c:	memcg->move_charge_at_immigrate = val;
memcontrol.c:	return -ENOSYS;
memcontrol.c:		nr = mem_cgroup_nr_lru_pages(memcg, stat->lru_mask);
memcontrol.c:		seq_printf(m, "%s=%lu", stat->name, nr);
memcontrol.c:							  stat->lru_mask);
memcontrol.c:			nr += mem_cgroup_nr_lru_pages(iter, stat->lru_mask);
memcontrol.c:		seq_printf(m, "hierarchical_%s=%lu", stat->name, nr);
memcontrol.c:					iter, nid, stat->lru_mask);
memcontrol.c:		memory = min(memory, mi->memory.limit);
memcontrol.c:		memsw = min(memsw, mi->memsw.limit);
memcontrol.c:			mz = mem_cgroup_nodeinfo(memcg, pgdat->node_id);
memcontrol.c:			rstat = &mz->lruvec.reclaim_stat;
memcontrol.c:			recent_rotated[0] += rstat->recent_rotated[0];
memcontrol.c:			recent_rotated[1] += rstat->recent_rotated[1];
memcontrol.c:			recent_scanned[0] += rstat->recent_scanned[0];
memcontrol.c:			recent_scanned[1] += rstat->recent_scanned[1];
memcontrol.c:		return -EINVAL;
memcontrol.c:	if (css->parent)
memcontrol.c:		memcg->swappiness = val;
memcontrol.c:		t = rcu_dereference(memcg->thresholds.primary);
memcontrol.c:		t = rcu_dereference(memcg->memsw_thresholds.primary);
memcontrol.c:	i = t->current_threshold;
memcontrol.c:	for (; i >= 0 && unlikely(t->entries[i].threshold > usage); i--)
memcontrol.c:		eventfd_signal(t->entries[i].eventfd, 1);
memcontrol.c:	for (; i < t->size && unlikely(t->entries[i].threshold <= usage); i++)
memcontrol.c:		eventfd_signal(t->entries[i].eventfd, 1);
memcontrol.c:	t->current_threshold = i - 1;
memcontrol.c:	if (_a->threshold > _b->threshold)
memcontrol.c:	if (_a->threshold < _b->threshold)
memcontrol.c:		return -1;
memcontrol.c:	list_for_each_entry(ev, &memcg->oom_notify, list)
memcontrol.c:		eventfd_signal(ev->eventfd, 1);
memcontrol.c:	ret = page_counter_memparse(args, "-1", &threshold);
memcontrol.c:	mutex_lock(&memcg->thresholds_lock);
memcontrol.c:		thresholds = &memcg->thresholds;
memcontrol.c:		thresholds = &memcg->memsw_thresholds;
memcontrol.c:	if (thresholds->primary)
memcontrol.c:	size = thresholds->primary ? thresholds->primary->size + 1 : 1;
memcontrol.c:		ret = -ENOMEM;
memcontrol.c:	new->size = size;
memcontrol.c:	if (thresholds->primary) {
memcontrol.c:		memcpy(new->entries, thresholds->primary->entries, (size - 1) *
memcontrol.c:	new->entries[size - 1].eventfd = eventfd;
memcontrol.c:	new->entries[size - 1].threshold = threshold;
memcontrol.c:	/* Sort thresholds. Registering of new threshold isn't time-critical */
memcontrol.c:	sort(new->entries, size, sizeof(struct mem_cgroup_threshold),
memcontrol.c:	new->current_threshold = -1;
memcontrol.c:		if (new->entries[i].threshold <= usage) {
memcontrol.c:			 * new->current_threshold will not be used until
memcontrol.c:			++new->current_threshold;
memcontrol.c:	kfree(thresholds->spare);
memcontrol.c:	thresholds->spare = thresholds->primary;
memcontrol.c:	rcu_assign_pointer(thresholds->primary, new);
memcontrol.c:	mutex_unlock(&memcg->thresholds_lock);
memcontrol.c:	mutex_lock(&memcg->thresholds_lock);
memcontrol.c:		thresholds = &memcg->thresholds;
memcontrol.c:		thresholds = &memcg->memsw_thresholds;
memcontrol.c:	if (!thresholds->primary)
memcontrol.c:	for (i = 0; i < thresholds->primary->size; i++) {
memcontrol.c:		if (thresholds->primary->entries[i].eventfd != eventfd)
memcontrol.c:	new = thresholds->spare;
memcontrol.c:	new->size = size;
memcontrol.c:	new->current_threshold = -1;
memcontrol.c:	for (i = 0, j = 0; i < thresholds->primary->size; i++) {
memcontrol.c:		if (thresholds->primary->entries[i].eventfd == eventfd)
memcontrol.c:		new->entries[j] = thresholds->primary->entries[i];
memcontrol.c:		if (new->entries[j].threshold <= usage) {
memcontrol.c:			 * new->current_threshold will not be used
memcontrol.c:			++new->current_threshold;
memcontrol.c:	thresholds->spare = thresholds->primary;
memcontrol.c:	rcu_assign_pointer(thresholds->primary, new);
memcontrol.c:		kfree(thresholds->spare);
memcontrol.c:		thresholds->spare = NULL;
memcontrol.c:	mutex_unlock(&memcg->thresholds_lock);
memcontrol.c:		return -ENOMEM;
memcontrol.c:	event->eventfd = eventfd;
memcontrol.c:	list_add(&event->list, &memcg->oom_notify);
memcontrol.c:	if (memcg->under_oom)
memcontrol.c:	list_for_each_entry_safe(ev, tmp, &memcg->oom_notify, list) {
memcontrol.c:		if (ev->eventfd == eventfd) {
memcontrol.c:			list_del(&ev->list);
memcontrol.c:	seq_printf(sf, "oom_kill_disable %d\n", memcg->oom_kill_disable);
memcontrol.c:	seq_printf(sf, "under_oom %d\n", (bool)memcg->under_oom);
memcontrol.c:	if (!css->parent || !((val == 0) || (val == 1)))
memcontrol.c:		return -EINVAL;
memcontrol.c:	memcg->oom_kill_disable = val;
memcontrol.c:	return &memcg->cgwb_list;
memcontrol.c:	return wb_domain_init(&memcg->cgwb_domain, gfp);
memcontrol.c:	wb_domain_exit(&memcg->cgwb_domain);
memcontrol.c:	wb_domain_size_changed(&memcg->cgwb_domain);
memcontrol.c:	struct mem_cgroup *memcg = mem_cgroup_from_css(wb->memcg_css);
memcontrol.c:	if (!memcg->css.parent)
memcontrol.c:	return &memcg->cgwb_domain;
memcontrol.c: * mem_cgroup_wb_stats - retrieve writeback related stats from its memcg
memcontrol.c: * @wb's memcg.  File, dirty and writeback are self-explanatory.  Headroom
memcontrol.c: * A memcg's headroom is "min(max, high) - used".  In the hierarchy, the
memcontrol.c:	struct mem_cgroup *memcg = mem_cgroup_from_css(wb->memcg_css);
memcontrol.c:		unsigned long ceiling = min(memcg->memory.limit, memcg->high);
memcontrol.c:		unsigned long used = page_counter_read(&memcg->memory);
memcontrol.c:		*pheadroom = min(*pheadroom, ceiling - min(ceiling, used));
memcontrol.c: * This is way over-engineered.  It tries to support fully configurable
memcontrol.c:	struct mem_cgroup *memcg = event->memcg;
memcontrol.c:	remove_wait_queue(event->wqh, &event->wait);
memcontrol.c:	event->unregister_event(memcg, event->eventfd);
memcontrol.c:	eventfd_signal(event->eventfd, 1);
memcontrol.c:	eventfd_ctx_put(event->eventfd);
memcontrol.c:	css_put(&memcg->css);
memcontrol.c: * Called with wqh->lock held and interrupts disabled.
memcontrol.c:	struct mem_cgroup *memcg = event->memcg;
memcontrol.c:		 * side will require wqh->lock via remove_wait_queue(),
memcontrol.c:		spin_lock(&memcg->event_list_lock);
memcontrol.c:		if (!list_empty(&event->list)) {
memcontrol.c:			list_del_init(&event->list);
memcontrol.c:			schedule_work(&event->remove);
memcontrol.c:		spin_unlock(&memcg->event_list_lock);
memcontrol.c:	event->wqh = wqh;
memcontrol.c:	add_wait_queue(wqh, &event->wait);
memcontrol.c:		return -EINVAL;
memcontrol.c:		return -EINVAL;
memcontrol.c:		return -ENOMEM;
memcontrol.c:	event->memcg = memcg;
memcontrol.c:	INIT_LIST_HEAD(&event->list);
memcontrol.c:	init_poll_funcptr(&event->pt, memcg_event_ptable_queue_proc);
memcontrol.c:	init_waitqueue_func_entry(&event->wait, memcg_event_wake);
memcontrol.c:	INIT_WORK(&event->remove, memcg_event_remove);
memcontrol.c:		ret = -EBADF;
memcontrol.c:	event->eventfd = eventfd_ctx_fileget(efile.file);
memcontrol.c:	if (IS_ERR(event->eventfd)) {
memcontrol.c:		ret = PTR_ERR(event->eventfd);
memcontrol.c:		ret = -EBADF;
memcontrol.c:	name = cfile.file->f_path.dentry->d_name.name;
memcontrol.c:		event->register_event = mem_cgroup_usage_register_event;
memcontrol.c:		event->unregister_event = mem_cgroup_usage_unregister_event;
memcontrol.c:		event->register_event = mem_cgroup_oom_register_event;
memcontrol.c:		event->unregister_event = mem_cgroup_oom_unregister_event;
memcontrol.c:		event->register_event = vmpressure_register_event;
memcontrol.c:		event->unregister_event = vmpressure_unregister_event;
memcontrol.c:		event->register_event = memsw_cgroup_usage_register_event;
memcontrol.c:		event->unregister_event = memsw_cgroup_usage_unregister_event;
memcontrol.c:		ret = -EINVAL;
memcontrol.c:	cfile_css = css_tryget_online_from_dir(cfile.file->f_path.dentry->d_parent,
memcontrol.c:	ret = -EINVAL;
memcontrol.c:	ret = event->register_event(memcg, event->eventfd, buf);
memcontrol.c:	efile.file->f_op->poll(efile.file, &event->pt);
memcontrol.c:	spin_lock(&memcg->event_list_lock);
memcontrol.c:	list_add(&event->list, &memcg->event_list);
memcontrol.c:	spin_unlock(&memcg->event_list_lock);
memcontrol.c:	eventfd_ctx_put(event->eventfd);
memcontrol.c: * Swap-out records and page cache shadow entries need to store memcg
memcontrol.c: * memory-controlled cgroups to 64k.
memcontrol.c: * even when there are much fewer than 64k cgroups - possibly none.
memcontrol.c: * Maintain a private 16-bit ID space for memcg, and allow the ID to
memcontrol.c:	VM_BUG_ON(atomic_read(&memcg->id.ref) <= 0);
memcontrol.c:	atomic_add(n, &memcg->id.ref);
memcontrol.c:	VM_BUG_ON(atomic_read(&memcg->id.ref) < n);
memcontrol.c:	if (atomic_sub_and_test(n, &memcg->id.ref)) {
memcontrol.c:		idr_remove(&mem_cgroup_idr, memcg->id.id);
memcontrol.c:		memcg->id.id = 0;
memcontrol.c:		css_put(&memcg->css);
memcontrol.c: * mem_cgroup_from_id - look up a memcg from a memcg id
memcontrol.c:		tmp = -1;
memcontrol.c:	pn->lruvec_stat = alloc_percpu(struct lruvec_stat);
memcontrol.c:	if (!pn->lruvec_stat) {
memcontrol.c:	lruvec_init(&pn->lruvec);
memcontrol.c:	pn->usage_in_excess = 0;
memcontrol.c:	pn->on_tree = false;
memcontrol.c:	pn->memcg = memcg;
memcontrol.c:	memcg->nodeinfo[node] = pn;
memcontrol.c:	struct mem_cgroup_per_node *pn = memcg->nodeinfo[node];
memcontrol.c:	free_percpu(pn->lruvec_stat);
memcontrol.c:	free_percpu(memcg->stat);
memcontrol.c:	memcg->id.id = idr_alloc(&mem_cgroup_idr, NULL,
memcontrol.c:	if (memcg->id.id < 0)
memcontrol.c:	memcg->stat = alloc_percpu(struct mem_cgroup_stat_cpu);
memcontrol.c:	if (!memcg->stat)
memcontrol.c:	INIT_WORK(&memcg->high_work, high_work_func);
memcontrol.c:	memcg->last_scanned_node = MAX_NUMNODES;
memcontrol.c:	INIT_LIST_HEAD(&memcg->oom_notify);
memcontrol.c:	mutex_init(&memcg->thresholds_lock);
memcontrol.c:	spin_lock_init(&memcg->move_lock);
memcontrol.c:	vmpressure_init(&memcg->vmpressure);
memcontrol.c:	INIT_LIST_HEAD(&memcg->event_list);
memcontrol.c:	spin_lock_init(&memcg->event_list_lock);
memcontrol.c:	memcg->socket_pressure = jiffies;
memcontrol.c:	memcg->kmemcg_id = -1;
memcontrol.c:	INIT_LIST_HEAD(&memcg->cgwb_list);
memcontrol.c:	idr_replace(&mem_cgroup_idr, memcg, memcg->id.id);
memcontrol.c:	if (memcg->id.id > 0)
memcontrol.c:		idr_remove(&mem_cgroup_idr, memcg->id.id);
memcontrol.c:	long error = -ENOMEM;
memcontrol.c:	memcg->high = PAGE_COUNTER_MAX;
memcontrol.c:	memcg->soft_limit = PAGE_COUNTER_MAX;
memcontrol.c:		memcg->swappiness = mem_cgroup_swappiness(parent);
memcontrol.c:		memcg->oom_kill_disable = parent->oom_kill_disable;
memcontrol.c:	if (parent && parent->use_hierarchy) {
memcontrol.c:		memcg->use_hierarchy = true;
memcontrol.c:		page_counter_init(&memcg->memory, &parent->memory);
memcontrol.c:		page_counter_init(&memcg->swap, &parent->swap);
memcontrol.c:		page_counter_init(&memcg->memsw, &parent->memsw);
memcontrol.c:		page_counter_init(&memcg->kmem, &parent->kmem);
memcontrol.c:		page_counter_init(&memcg->tcpmem, &parent->tcpmem);
memcontrol.c:		page_counter_init(&memcg->memory, NULL);
memcontrol.c:		page_counter_init(&memcg->swap, NULL);
memcontrol.c:		page_counter_init(&memcg->memsw, NULL);
memcontrol.c:		page_counter_init(&memcg->kmem, NULL);
memcontrol.c:		page_counter_init(&memcg->tcpmem, NULL);
memcontrol.c:		return &memcg->css;
memcontrol.c:	return &memcg->css;
memcontrol.c:	return ERR_PTR(-ENOMEM);
memcontrol.c:	atomic_set(&memcg->id.ref, 1);
memcontrol.c:	spin_lock(&memcg->event_list_lock);
memcontrol.c:	list_for_each_entry_safe(event, tmp, &memcg->event_list, list) {
memcontrol.c:		list_del_init(&event->list);
memcontrol.c:		schedule_work(&event->remove);
memcontrol.c:	spin_unlock(&memcg->event_list_lock);
memcontrol.c:	memcg->low = 0;
memcontrol.c:	if (!cgroup_subsys_on_dfl(memory_cgrp_subsys) && memcg->tcpmem_active)
memcontrol.c:	vmpressure_cleanup(&memcg->vmpressure);
memcontrol.c:	cancel_work_sync(&memcg->high_work);
memcontrol.c: * mem_cgroup_css_reset - reset the states of a mem_cgroup
memcontrol.c:	page_counter_limit(&memcg->memory, PAGE_COUNTER_MAX);
memcontrol.c:	page_counter_limit(&memcg->swap, PAGE_COUNTER_MAX);
memcontrol.c:	page_counter_limit(&memcg->memsw, PAGE_COUNTER_MAX);
memcontrol.c:	page_counter_limit(&memcg->kmem, PAGE_COUNTER_MAX);
memcontrol.c:	page_counter_limit(&memcg->tcpmem, PAGE_COUNTER_MAX);
memcontrol.c:	memcg->low = 0;
memcontrol.c:	memcg->high = PAGE_COUNTER_MAX;
memcontrol.c:	memcg->soft_limit = PAGE_COUNTER_MAX;
memcontrol.c:	while (count--) {
memcontrol.c:		entry->val = ent.val;
memcontrol.c:	if (!vma->vm_file) /* anonymous vma */
memcontrol.c:	mapping = vma->vm_file->f_mapping;
memcontrol.c:	/* page is moved even if it's not RSS of this task(page-faulted). */
memcontrol.c: * mem_cgroup_move_account - move account of the page
memcontrol.c:	 * page->mem_cgroup of its source page while we change it.
memcontrol.c:	ret = -EBUSY;
memcontrol.c:	ret = -EINVAL;
memcontrol.c:	if (page->mem_cgroup != from)
memcontrol.c:	spin_lock_irqsave(&from->move_lock, flags);
memcontrol.c:		__this_cpu_sub(from->stat->count[NR_FILE_MAPPED], nr_pages);
memcontrol.c:		__this_cpu_add(to->stat->count[NR_FILE_MAPPED], nr_pages);
memcontrol.c:	 * move_lock grabbed above and caller set from->moving_account, so
memcontrol.c:			__this_cpu_sub(from->stat->count[NR_FILE_DIRTY],
memcontrol.c:			__this_cpu_add(to->stat->count[NR_FILE_DIRTY],
memcontrol.c:		__this_cpu_sub(from->stat->count[NR_WRITEBACK], nr_pages);
memcontrol.c:		__this_cpu_add(to->stat->count[NR_WRITEBACK], nr_pages);
memcontrol.c:	 * It is safe to change page->mem_cgroup here because the page
memcontrol.c:	 * is referenced, charged, and isolated - we can't race with
memcontrol.c:	page->mem_cgroup = to;
memcontrol.c:	spin_unlock_irqrestore(&from->move_lock, flags);
memcontrol.c:	mem_cgroup_charge_statistics(from, page, compound, -nr_pages);
memcontrol.c: * get_mctgt_type - get target type of moving charge
memcontrol.c: *     move charge. if @target is not NULL, the page is stored in target->page
memcontrol.c: *     in target->ent.
memcontrol.c:		if (page->mem_cgroup == mc.from) {
memcontrol.c:				target->page = page;
memcontrol.c:	 * But we cannot move a tail-page in a THP.
memcontrol.c:			target->ent = ent;
memcontrol.c:	if (page->mem_cgroup == mc.from) {
memcontrol.c:			target->page = page;
memcontrol.c:	struct vm_area_struct *vma = walk->vma;
memcontrol.c:	pte = pte_offset_map_lock(vma->vm_mm, pmd, addr, &ptl);
memcontrol.c:	pte_unmap_unlock(pte - 1, ptl);
memcontrol.c:	down_read(&mm->mmap_sem);
memcontrol.c:	walk_page_range(0, mm->highest_vm_end,
memcontrol.c:	up_read(&mm->mmap_sem);
memcontrol.c:			page_counter_uncharge(&mc.from->memsw, mc.moved_swap);
memcontrol.c:		 * we charged both to->memory and to->memsw, so we
memcontrol.c:		 * should uncharge to->memory.
memcontrol.c:			page_counter_uncharge(&mc.to->memory, mc.moved_swap);
memcontrol.c:		css_put_many(&mc.to->css, mc.moved_swap);
memcontrol.c:	 * Multi-process migrations only happen on the default hierarchy
memcontrol.c:	move_flags = READ_ONCE(memcg->move_charge_at_immigrate);
memcontrol.c:	if (mm->owner == p) {
memcontrol.c:	struct vm_area_struct *vma = walk->vma;
memcontrol.c:					mc.precharge -= HPAGE_PMD_NR;
memcontrol.c:				mc.precharge -= HPAGE_PMD_NR;
memcontrol.c:	pte = pte_offset_map_lock(vma->vm_mm, pmd, addr, &ptl);
memcontrol.c:				mc.precharge--;
memcontrol.c:				mc.precharge--;
memcontrol.c:	pte_unmap_unlock(pte - 1, ptl);
memcontrol.c:	 * for already started RCU-only updates to finish.
memcontrol.c:	atomic_inc(&mc.from->moving_account);
memcontrol.c:	if (unlikely(!down_read_trylock(&mc.mm->mmap_sem))) {
memcontrol.c:		 * to move enough charges, but moving charge is a best-effort
memcontrol.c:	walk_page_range(0, mc.mm->highest_vm_end, &mem_cgroup_move_charge_walk);
memcontrol.c:	up_read(&mc.mm->mmap_sem);
memcontrol.c:	atomic_dec(&mc.from->moving_account);
memcontrol.c:		root_mem_cgroup->use_hierarchy = true;
memcontrol.c:		root_mem_cgroup->use_hierarchy = false;
memcontrol.c:	return (u64)page_counter_read(&memcg->memory) * PAGE_SIZE;
memcontrol.c:	unsigned long low = READ_ONCE(memcg->low);
memcontrol.c:	memcg->low = low;
memcontrol.c:	unsigned long high = READ_ONCE(memcg->high);
memcontrol.c:	memcg->high = high;
memcontrol.c:	nr_pages = page_counter_read(&memcg->memory);
memcontrol.c:		try_to_free_mem_cgroup_pages(memcg, nr_pages - high,
memcontrol.c:	unsigned long max = READ_ONCE(memcg->memory.limit);
memcontrol.c:	xchg(&memcg->memory.limit, max);
memcontrol.c:		unsigned long nr_pages = page_counter_read(&memcg->memory);
memcontrol.c:			err = -EINTR;
memcontrol.c:			if (!try_to_free_mem_cgroup_pages(memcg, nr_pages - max,
memcontrol.c:				nr_reclaims--;
memcontrol.c:	 * 1) generic big picture -> specifics and details
memcontrol.c:	 * 2) reflecting userspace activity -> reflecting kernel heuristics
memcontrol.c: * mem_cgroup_low - check if memory consumption is below the normal range
memcontrol.c: * @root: the top ancestor of the sub-tree being checked
memcontrol.c:		if (page_counter_read(&memcg->memory) >= memcg->low)
memcontrol.c: * mem_cgroup_try_charge - try charging a page
memcontrol.c: * After page->mapping has been set up, the caller must finalize the
memcontrol.c:		if (compound_head(page)->mem_cgroup)
memcontrol.c:			if (memcg && !css_tryget_online(&memcg->css))
memcontrol.c:	css_put(&memcg->css);
memcontrol.c: * mem_cgroup_commit_charge - commit a page charge
memcontrol.c: * after page->mapping has been set up.  This must happen atomically
memcontrol.c:	VM_BUG_ON_PAGE(!page->mapping, page);
memcontrol.c: * mem_cgroup_cancel_charge - cancel a page charge
memcontrol.c:	unsigned long nr_pages = ug->nr_anon + ug->nr_file + ug->nr_kmem;
memcontrol.c:	if (!mem_cgroup_is_root(ug->memcg)) {
memcontrol.c:		page_counter_uncharge(&ug->memcg->memory, nr_pages);
memcontrol.c:			page_counter_uncharge(&ug->memcg->memsw, nr_pages);
memcontrol.c:		if (!cgroup_subsys_on_dfl(memory_cgrp_subsys) && ug->nr_kmem)
memcontrol.c:			page_counter_uncharge(&ug->memcg->kmem, ug->nr_kmem);
memcontrol.c:		memcg_oom_recover(ug->memcg);
memcontrol.c:	__this_cpu_sub(ug->memcg->stat->count[MEMCG_RSS], ug->nr_anon);
memcontrol.c:	__this_cpu_sub(ug->memcg->stat->count[MEMCG_CACHE], ug->nr_file);
memcontrol.c:	__this_cpu_sub(ug->memcg->stat->count[MEMCG_RSS_HUGE], ug->nr_huge);
memcontrol.c:	__this_cpu_sub(ug->memcg->stat->count[NR_SHMEM], ug->nr_shmem);
memcontrol.c:	__this_cpu_add(ug->memcg->stat->events[PGPGOUT], ug->pgpgout);
memcontrol.c:	__this_cpu_add(ug->memcg->stat->nr_page_events, nr_pages);
memcontrol.c:	memcg_check_events(ug->memcg, ug->dummy_page);
memcontrol.c:	if (!mem_cgroup_is_root(ug->memcg))
memcontrol.c:		css_put_many(&ug->memcg->css, nr_pages);
memcontrol.c:	if (!page->mem_cgroup)
memcontrol.c:	 * page->mem_cgroup at this point, we have fully
memcontrol.c:	if (ug->memcg != page->mem_cgroup) {
memcontrol.c:		if (ug->memcg) {
memcontrol.c:		ug->memcg = page->mem_cgroup;
memcontrol.c:			ug->nr_huge += nr_pages;
memcontrol.c:			ug->nr_anon += nr_pages;
memcontrol.c:			ug->nr_file += nr_pages;
memcontrol.c:				ug->nr_shmem += nr_pages;
memcontrol.c:		ug->pgpgout++;
memcontrol.c:		ug->nr_kmem += 1 << compound_order(page);
memcontrol.c:	ug->dummy_page = page;
memcontrol.c:	page->mem_cgroup = NULL;
memcontrol.c:	 * Note that the list can be a single page->lru; hence the
memcontrol.c:	 * do-while loop instead of a simple list_for_each_entry().
memcontrol.c:	next = page_list->next;
memcontrol.c:		next = page->lru.next;
memcontrol.c: * mem_cgroup_uncharge - uncharge a page
memcontrol.c:	/* Don't touch page->lru of any random page, pre-check: */
memcontrol.c:	if (!page->mem_cgroup)
memcontrol.c: * mem_cgroup_uncharge_list - uncharge a list of page
memcontrol.c: * mem_cgroup_migrate - charge a page's replacement
memcontrol.c: * Both pages must be locked, @newpage->mapping must be set up.
memcontrol.c:	if (newpage->mem_cgroup)
memcontrol.c:	memcg = oldpage->mem_cgroup;
memcontrol.c:	/* Force-charge the new page. The old one will be freed soon */
memcontrol.c:	page_counter_charge(&memcg->memory, nr_pages);
memcontrol.c:		page_counter_charge(&memcg->memsw, nr_pages);
memcontrol.c:	css_get_many(&memcg->css, nr_pages);
memcontrol.c:	if (sk->sk_memcg) {
memcontrol.c:		css_get(&sk->sk_memcg->css);
memcontrol.c:	if (!cgroup_subsys_on_dfl(memory_cgrp_subsys) && !memcg->tcpmem_active)
memcontrol.c:	if (css_tryget_online(&memcg->css))
memcontrol.c:		sk->sk_memcg = memcg;
memcontrol.c:	if (sk->sk_memcg)
memcontrol.c:		css_put(&sk->sk_memcg->css);
memcontrol.c: * mem_cgroup_charge_skmem - charge socket memory
memcontrol.c:		if (page_counter_try_charge(&memcg->tcpmem, nr_pages, &fail)) {
memcontrol.c:			memcg->tcpmem_pressure = 0;
memcontrol.c:		page_counter_charge(&memcg->tcpmem, nr_pages);
memcontrol.c:		memcg->tcpmem_pressure = 1;
memcontrol.c:	this_cpu_add(memcg->stat->count[MEMCG_SOCK], nr_pages);
memcontrol.c: * mem_cgroup_uncharge_skmem - uncharge socket memory
memcontrol.c: * @memcg - memcg to uncharge
memcontrol.c: * @nr_pages - number of pages to uncharge
memcontrol.c:		page_counter_uncharge(&memcg->tcpmem, nr_pages);
memcontrol.c:	this_cpu_sub(memcg->stat->count[MEMCG_SOCK], nr_pages);
memcontrol.c: * context because of lock dependencies (cgroup_lock -> cpu hotplug) but
memcontrol.c:		INIT_WORK(&per_cpu_ptr(&memcg_stock, cpu)->work,
memcontrol.c:		rtpn->rb_root = RB_ROOT;
memcontrol.c:		rtpn->rb_rightmost = NULL;
memcontrol.c:		spin_lock_init(&rtpn->lock);
memcontrol.c:	while (!atomic_inc_not_zero(&memcg->id.ref)) {
memcontrol.c: * mem_cgroup_swapout - transfer a memsw charge to swap
memcontrol.c:	memcg = page->mem_cgroup;
memcontrol.c:		mem_cgroup_id_get_many(swap_memcg, nr_entries - 1);
memcontrol.c:	page->mem_cgroup = NULL;
memcontrol.c:		page_counter_uncharge(&memcg->memory, nr_entries);
memcontrol.c:			page_counter_charge(&swap_memcg->memsw, nr_entries);
memcontrol.c:		page_counter_uncharge(&memcg->memsw, nr_entries);
memcontrol.c:	 * mapping->tree_lock lock which is taken with interrupts-off. It is
memcontrol.c:	 * only synchronisation we have for udpating the per-CPU variables.
memcontrol.c:				     -nr_entries);
memcontrol.c:		css_put_many(&memcg->css, nr_entries);
memcontrol.c: * mem_cgroup_try_charge_swap - try charging swap space for a page
memcontrol.c: * Returns 0 on success, -ENOMEM on failure.
memcontrol.c:	memcg = page->mem_cgroup;
memcontrol.c:	    !page_counter_try_charge(&memcg->swap, nr_pages, &counter)) {
memcontrol.c:		return -ENOMEM;
memcontrol.c:		mem_cgroup_id_get_many(memcg, nr_pages - 1);
memcontrol.c: * mem_cgroup_uncharge_swap - uncharge swap space
memcontrol.c:				page_counter_uncharge(&memcg->swap, nr_pages);
memcontrol.c:				page_counter_uncharge(&memcg->memsw, nr_pages);
memcontrol.c:		mem_cgroup_swap_statistics(memcg, -nr_pages);
memcontrol.c:				      READ_ONCE(memcg->swap.limit) -
memcontrol.c:				      page_counter_read(&memcg->swap));
memcontrol.c:	memcg = page->mem_cgroup;
memcontrol.c:		if (page_counter_read(&memcg->swap) * 2 >= memcg->swap.limit)
memcontrol.c:	return (u64)page_counter_read(&memcg->swap) * PAGE_SIZE;
memcontrol.c:	unsigned long max = READ_ONCE(memcg->swap.limit);
memcontrol.c:	err = page_counter_limit(&memcg->swap, max);
zswap.c: * zswap.c - zswap driver file
zswap.c: * RAM-based memory pool.  This can result in a significant I/O reduction on
zswap.c:#include <linux/page-flags.h>
zswap.c: * rbnode - links the entry into red-black tree for the appropriate swap type
zswap.c: * offset - the swap offset for the entry.  Index into the red-black tree.
zswap.c: * refcount - the number of outstanding reference to the entry. This is needed
zswap.c: * length - the length in bytes of the compressed page data.  Needed during
zswap.c: * pool - the zswap_pool the entry's data is in
zswap.c: * handle - zpool allocation handle that stores the compressed page data
zswap.c: * - the rbtree
zswap.c: * - the refcount field of each entry in the tree
zswap.c:/* RCU-protected iteration */
zswap.c:	pr_debug("%s pool %s/%s\n", msg, (p)->tfm_name,		\
zswap.c:		 zpool_get_type((p)->zpool))
zswap.c:		total += zpool_get_total_size(pool->zpool);
zswap.c:	entry->refcount = 1;
zswap.c:	RB_CLEAR_NODE(&entry->rbnode);
zswap.c:	struct rb_node *node = root->rb_node;
zswap.c:		if (entry->offset > offset)
zswap.c:			node = node->rb_left;
zswap.c:		else if (entry->offset < offset)
zswap.c:			node = node->rb_right;
zswap.c: * the existing entry is stored in dupentry and the function returns -EEXIST
zswap.c:	struct rb_node **link = &root->rb_node, *parent = NULL;
zswap.c:		if (myentry->offset > entry->offset)
zswap.c:			link = &(*link)->rb_left;
zswap.c:		else if (myentry->offset < entry->offset)
zswap.c:			link = &(*link)->rb_right;
zswap.c:			return -EEXIST;
zswap.c:	rb_link_node(&entry->rbnode, parent, link);
zswap.c:	rb_insert_color(&entry->rbnode, root);
zswap.c:	if (!RB_EMPTY_NODE(&entry->rbnode)) {
zswap.c:		rb_erase(&entry->rbnode, root);
zswap.c:		RB_CLEAR_NODE(&entry->rbnode);
zswap.c:	zpool_free(entry->pool->zpool, entry->handle);
zswap.c:	zswap_pool_put(entry->pool);
zswap.c:	entry->refcount++;
zswap.c:	int refcount = --entry->refcount;
zswap.c:		zswap_rb_erase(&tree->rbroot, entry);
zswap.c:* per-cpu code
zswap.c:		return -ENOMEM;
zswap.c:	if (WARN_ON(*per_cpu_ptr(pool->tfm, cpu)))
zswap.c:	tfm = crypto_alloc_comp(pool->tfm_name, 0, 0);
zswap.c:		       pool->tfm_name, PTR_ERR(tfm));
zswap.c:		return -ENOMEM;
zswap.c:	*per_cpu_ptr(pool->tfm, cpu) = tfm;
zswap.c:	tfm = *per_cpu_ptr(pool->tfm, cpu);
zswap.c:	*per_cpu_ptr(pool->tfm, cpu) = NULL;
zswap.c:/* type and compressor must be null-terminated */
zswap.c:		if (strcmp(pool->tfm_name, compressor))
zswap.c:		if (strcmp(zpool_get_type(pool->zpool), type))
zswap.c:	pool->zpool = zpool_create_pool(type, name, gfp, &zswap_zpool_ops);
zswap.c:	if (!pool->zpool) {
zswap.c:	pr_debug("using %s zpool\n", zpool_get_type(pool->zpool));
zswap.c:	strlcpy(pool->tfm_name, compressor, sizeof(pool->tfm_name));
zswap.c:	pool->tfm = alloc_percpu(struct crypto_comp *);
zswap.c:	if (!pool->tfm) {
zswap.c:				       &pool->node);
zswap.c:	pr_debug("using %s compressor\n", pool->tfm_name);
zswap.c:	kref_init(&pool->kref);
zswap.c:	INIT_LIST_HEAD(&pool->list);
zswap.c:	free_percpu(pool->tfm);
zswap.c:	if (pool->zpool)
zswap.c:		zpool_destroy_pool(pool->zpool);
zswap.c:	cpuhp_state_remove_instance(CPUHP_MM_ZSWP_POOL_PREPARE, &pool->node);
zswap.c:	free_percpu(pool->tfm);
zswap.c:	zpool_destroy_pool(pool->zpool);
zswap.c:	return kref_get_unless_zero(&pool->kref);
zswap.c:	WARN_ON(kref_get_unless_zero(&pool->kref));
zswap.c:	list_del_rcu(&pool->list);
zswap.c:	INIT_WORK(&pool->work, __zswap_pool_release);
zswap.c:	schedule_work(&pool->work);
zswap.c:	kref_put(&pool->kref, __zswap_pool_empty);
zswap.c:/* val must be a null-terminated string */
zswap.c:		return -ENODEV;
zswap.c:	if (!strcmp(s, *(char **)kp->arg) && zswap_has_pool)
zswap.c:	/* if this is load-time (pre-init) param setting,
zswap.c:			return -ENOENT;
zswap.c:			return -ENOENT;
zswap.c:		return -EINVAL;
zswap.c:		list_del_rcu(&pool->list);
zswap.c:		ret = -EINVAL;
zswap.c:		list_add_rcu(&pool->list, &zswap_pools);
zswap.c:		/* add the possibly pre-existing pool to the end of the pools
zswap.c:		list_add_tail_rcu(&pool->list, &zswap_pools);
zswap.c:		return -ENODEV;
zswap.c:		return -ENODEV;
zswap.c:	swpentry = zhdr->swpentry; /* here */
zswap.c:	spin_lock(&tree->lock);
zswap.c:	entry = zswap_entry_find_get(&tree->rbroot, offset);
zswap.c:		spin_unlock(&tree->lock);
zswap.c:	spin_unlock(&tree->lock);
zswap.c:	BUG_ON(offset != entry->offset);
zswap.c:		ret = -ENOMEM;
zswap.c:		ret = -EEXIST;
zswap.c:		src = (u8 *)zpool_map_handle(entry->pool->zpool, entry->handle,
zswap.c:		tfm = *get_cpu_ptr(entry->pool->tfm);
zswap.c:		ret = crypto_comp_decompress(tfm, src, entry->length,
zswap.c:		put_cpu_ptr(entry->pool->tfm);
zswap.c:		zpool_unmap_handle(entry->pool->zpool, entry->handle);
zswap.c:	spin_lock(&tree->lock);
zswap.c:	if (entry == zswap_rb_search(&tree->rbroot, offset))
zswap.c:	spin_unlock(&tree->lock);
zswap.c:	spin_lock(&tree->lock);
zswap.c:	spin_unlock(&tree->lock);
zswap.c:		return -ENOENT;
zswap.c:	ret = zpool_shrink(pool->zpool, 1, NULL);
zswap.c:		ret = -EINVAL;
zswap.c:		ret = -ENODEV;
zswap.c:			ret = -ENOMEM;
zswap.c:		ret = -ENOMEM;
zswap.c:	entry->pool = zswap_pool_current_get();
zswap.c:	if (!entry->pool) {
zswap.c:		ret = -EINVAL;
zswap.c:	tfm = *get_cpu_ptr(entry->pool->tfm);
zswap.c:	put_cpu_ptr(entry->pool->tfm);
zswap.c:		ret = -EINVAL;
zswap.c:	ret = zpool_malloc(entry->pool->zpool, len,
zswap.c:	if (ret == -ENOSPC) {
zswap.c:	zhdr = zpool_map_handle(entry->pool->zpool, handle, ZPOOL_MM_RW);
zswap.c:	zhdr->swpentry = swp_entry(type, offset);
zswap.c:	zpool_unmap_handle(entry->pool->zpool, handle);
zswap.c:	entry->offset = offset;
zswap.c:	entry->handle = handle;
zswap.c:	entry->length = dlen;
zswap.c:	spin_lock(&tree->lock);
zswap.c:		ret = zswap_rb_insert(&tree->rbroot, entry, &dupentry);
zswap.c:		if (ret == -EEXIST) {
zswap.c:			zswap_rb_erase(&tree->rbroot, dupentry);
zswap.c:	} while (ret == -EEXIST);
zswap.c:	spin_unlock(&tree->lock);
zswap.c:	zswap_pool_put(entry->pool);
zswap.c: * return -1 on entry not found or error
zswap.c:	spin_lock(&tree->lock);
zswap.c:	entry = zswap_entry_find_get(&tree->rbroot, offset);
zswap.c:		spin_unlock(&tree->lock);
zswap.c:		return -1;
zswap.c:	spin_unlock(&tree->lock);
zswap.c:	src = (u8 *)zpool_map_handle(entry->pool->zpool, entry->handle,
zswap.c:	tfm = *get_cpu_ptr(entry->pool->tfm);
zswap.c:	ret = crypto_comp_decompress(tfm, src, entry->length, dst, &dlen);
zswap.c:	put_cpu_ptr(entry->pool->tfm);
zswap.c:	zpool_unmap_handle(entry->pool->zpool, entry->handle);
zswap.c:	spin_lock(&tree->lock);
zswap.c:	spin_unlock(&tree->lock);
zswap.c:	spin_lock(&tree->lock);
zswap.c:	entry = zswap_rb_search(&tree->rbroot, offset);
zswap.c:		spin_unlock(&tree->lock);
zswap.c:	zswap_rb_erase(&tree->rbroot, entry);
zswap.c:	spin_unlock(&tree->lock);
zswap.c:	spin_lock(&tree->lock);
zswap.c:	rbtree_postorder_for_each_entry_safe(entry, n, &tree->rbroot, rbnode)
zswap.c:	tree->rbroot = RB_ROOT;
zswap.c:	spin_unlock(&tree->lock);
zswap.c:	tree->rbroot = RB_ROOT;
zswap.c:	spin_lock_init(&tree->lock);
zswap.c:		return -ENODEV;
zswap.c:		return -ENOMEM;
zswap.c:		pr_info("loaded using pool %s/%s\n", pool->tfm_name,
zswap.c:			zpool_get_type(pool->zpool));
zswap.c:		list_add(&pool->list, &zswap_pools);
zswap.c:	/* if built-in, we aren't unloaded on failure; don't allow use */
zswap.c:	return -ENOMEM;
page_owner.c:// SPDX-License-Identifier: GPL-2.0
page_owner.c:		return -EINVAL;
page_owner.c:		__clear_bit(PAGE_EXT_OWNER, &page_ext->flags);
page_owner.c:	if (!trace->nr_entries)
page_owner.c:	for (i = 0; i < trace->nr_entries; i++) {
page_owner.c:		if (trace->entries[i] == ip)
page_owner.c:	    trace.entries[trace.nr_entries-1] == ULONG_MAX)
page_owner.c:		trace.nr_entries--;
page_owner.c:	page_owner->handle = handle;
page_owner.c:	page_owner->order = order;
page_owner.c:	page_owner->gfp_mask = gfp_mask;
page_owner.c:	page_owner->last_migrate_reason = -1;
page_owner.c:	__set_bit(PAGE_EXT_OWNER, &page_ext->flags);
page_owner.c:	page_owner->last_migrate_reason = reason;
page_owner.c:	page_owner->order = 0;
page_owner.c:	new_page_owner->order = old_page_owner->order;
page_owner.c:	new_page_owner->gfp_mask = old_page_owner->gfp_mask;
page_owner.c:	new_page_owner->last_migrate_reason =
page_owner.c:		old_page_owner->last_migrate_reason;
page_owner.c:	new_page_owner->handle = old_page_owner->handle;
page_owner.c:	__set_bit(PAGE_EXT_OWNER, &new_ext->flags);
page_owner.c:	unsigned long pfn = zone->zone_start_pfn, block_end_pfn;
page_owner.c:	unsigned long end_pfn = pfn + zone->spanned_pages;
page_owner.c:	pfn = zone->zone_start_pfn;
page_owner.c:					pfn += (1UL << freepage_order) - 1;
page_owner.c:			if (!test_bit(PAGE_EXT_OWNER, &page_ext->flags))
page_owner.c:					page_owner->gfp_mask);
page_owner.c:			pfn += (1UL << page_owner->order) - 1;
page_owner.c:	seq_printf(m, "Node %d, zone %8s ", pgdat->node_id, zone->name);
page_owner.c:		return -ENOMEM;
page_owner.c:			page_owner->order, page_owner->gfp_mask,
page_owner.c:			&page_owner->gfp_mask);
page_owner.c:	page_mt  = gfpflags_to_migratetype(page_owner->gfp_mask);
page_owner.c:	ret += snprintf(kbuf + ret, count - ret,
page_owner.c:			page->flags, &page->flags);
page_owner.c:	ret += snprint_stack_trace(kbuf + ret, count - ret, &trace, 0);
page_owner.c:	if (page_owner->last_migrate_reason != -1) {
page_owner.c:		ret += snprintf(kbuf + ret, count - ret,
page_owner.c:			migrate_reason_names[page_owner->last_migrate_reason]);
page_owner.c:	ret += snprintf(kbuf + ret, count - ret, "\n");
page_owner.c:		ret = -EFAULT;
page_owner.c:	return -ENOMEM;
page_owner.c:	gfp_mask = page_owner->gfp_mask;
page_owner.c:	if (!test_bit(PAGE_EXT_OWNER, &page_ext->flags)) {
page_owner.c:	handle = READ_ONCE(page_owner->handle);
page_owner.c:		 page_owner->order, migratetype_names[mt], gfp_mask, &gfp_mask);
page_owner.c:	if (page_owner->last_migrate_reason != -1)
page_owner.c:			migrate_reason_names[page_owner->last_migrate_reason]);
page_owner.c:		return -EINVAL;
page_owner.c:	while (!pfn_valid(pfn) && (pfn & (MAX_ORDER_NR_PAGES - 1)) != 0)
page_owner.c:		if ((pfn & (MAX_ORDER_NR_PAGES - 1)) == 0 && !pfn_valid(pfn)) {
page_owner.c:			pfn += MAX_ORDER_NR_PAGES - 1;
page_owner.c:				pfn += (1UL << freepage_order) - 1;
page_owner.c:		if (!test_bit(PAGE_EXT_OWNER, &page_ext->flags))
page_owner.c:		 * Access to page_ext->handle isn't synchronous so we should
page_owner.c:		handle = READ_ONCE(page_owner->handle);
page_owner.c:		*ppos = (pfn - min_low_pfn) + 1;
page_owner.c:	unsigned long pfn = zone->zone_start_pfn, block_end_pfn;
page_owner.c:	unsigned long end_pfn = pfn + zone->spanned_pages;
page_owner.c:	pfn = zone->zone_start_pfn;
page_owner.c:			 * To avoid having to grab zone->lock, be a little
page_owner.c:					pfn += (1UL << order) - 1;
page_owner.c:			if (test_bit(PAGE_EXT_OWNER, &page_ext->flags))
page_owner.c:		pgdat->node_id, zone->name, count);
page_owner.c:	struct zone *node_zones = pgdat->node_zones;
page_owner.c:	for (zone = node_zones; zone - node_zones < MAX_NR_ZONES; ++zone) {
slob.c:// SPDX-License-Identifier: GPL-2.0
slob.c: * will require 4 bytes on 32-bit and 8 bytes on 64-bit.
slob.c: * and within each page, there is a singly-linked list of free blocks
slob.c: * sufficient free blocks (using a next-fit-like approach) followed by
slob.c: * a first-fit scan of the page. Deallocation inserts objects back
slob.c: * address-ordered first fit.
slob.c: * from kmalloc are prepended with a 4-byte header with the kmalloc size.
slob.c: * 4-byte alignment unless the SLAB_HWCACHE_ALIGN flag is set, in which
slob.c: * case the low-level allocator will fragment blocks to create the proper
slob.c: * alignment. Again, objects of page-size or greater are allocated by
slob.c: * space overhead, and compound pages aren't needed for multi-page
slob.c: * or offset of next block if -ve (in SLOB_UNITs).
slob.c:	list_add(&sp->lru, list);
slob.c:	list_del(&sp->lru);
slob.c:	slobidx_t offset = next - base;
slob.c:		s[0].units = -offset;
slob.c:	if (s->units > 0)
slob.c:		return s->units;
slob.c:		next = -s[0].units;
slob.c:	if (current->reclaim_state)
slob.c:		current->reclaim_state->reclaimed_slab += 1 << order;
slob.c:	for (prev = NULL, cur = sp->freelist; ; prev = cur, cur = slob_next(cur)) {
slob.c:			delta = aligned - cur;
slob.c:				set_slob(aligned, avail - delta, next);
slob.c:					sp->freelist = next;
slob.c:					sp->freelist = cur + units;
slob.c:				set_slob(cur + units, avail - units, next);
slob.c:			sp->units -= units;
slob.c:			if (!sp->units)
slob.c:		if (sp->units < SLOB_UNITS(size))
slob.c:		prev = sp->lru.prev;
slob.c:		if (prev != slob_list->prev &&
slob.c:				slob_list->next != prev->next)
slob.c:			list_move_tail(slob_list, prev->next);
slob.c:		sp->units = SLOB_UNITS(PAGE_SIZE);
slob.c:		sp->freelist = b;
slob.c:		INIT_LIST_HEAD(&sp->lru);
slob.c:	if (sp->units + units == SLOB_UNITS(PAGE_SIZE)) {
slob.c:		sp->units = units;
slob.c:		sp->freelist = b;
slob.c:	sp->units += units;
slob.c:	if (b < (slob_t *)sp->freelist) {
slob.c:		if (b + units == sp->freelist) {
slob.c:			units += slob_units(sp->freelist);
slob.c:			sp->freelist = slob_next(sp->freelist);
slob.c:		set_slob(b, units, sp->freelist);
slob.c:		sp->freelist = b;
slob.c:		prev = sp->freelist;
slob.c:	if (size < PAGE_SIZE - align) {
slob.c:		unsigned int *m = (unsigned int *)(block - align);
slob.c:	m = (unsigned int *)(block - align);
slob.c:		c->size += sizeof(struct slob_rcu);
slob.c:	c->flags = flags;
slob.c:	if (c->size < PAGE_SIZE) {
slob.c:		b = slob_alloc(c->size, flags, c->align, node);
slob.c:		trace_kmem_cache_alloc_node(_RET_IP_, b, c->object_size,
slob.c:					    SLOB_UNITS(c->size) * SLOB_UNIT,
slob.c:		b = slob_new_pages(flags, get_order(c->size), node);
slob.c:		trace_kmem_cache_alloc_node(_RET_IP_, b, c->object_size,
slob.c:					    PAGE_SIZE << get_order(c->size),
slob.c:	if (b && c->ctor)
slob.c:		c->ctor(b);
slob.c:	kmemleak_alloc_recursive(b, c->size, 1, c->flags, flags);
slob.c:	void *b = (void *)slob_rcu - (slob_rcu->size - sizeof(struct slob_rcu));
slob.c:	__kmem_cache_free(b, slob_rcu->size);
slob.c:	kmemleak_free_recursive(b, c->flags);
slob.c:	if (unlikely(c->flags & SLAB_TYPESAFE_BY_RCU)) {
slob.c:		slob_rcu = b + (c->size - sizeof(struct slob_rcu));
slob.c:		slob_rcu->size = c->size;
slob.c:		call_rcu(&slob_rcu->head, kmem_rcu_free);
slob.c:		__kmem_cache_free(b, c->size);
vmpressure.c: * rate-limit tunable for the "low" level notification, and also for
vmpressure.c: * prio <= DEF_PRIORITY - 2 : kswapd becomes somewhat overwhelmed
vmpressure.c:	pressure = scale - (reclaimed * scale / scanned);
vmpressure.c:	mutex_lock(&vmpr->events_lock);
vmpressure.c:	list_for_each_entry(ev, &vmpr->events, node) {
vmpressure.c:		if (ancestor && ev->mode == VMPRESSURE_LOCAL)
vmpressure.c:		if (signalled && ev->mode == VMPRESSURE_NO_PASSTHROUGH)
vmpressure.c:		if (level < ev->level)
vmpressure.c:		eventfd_signal(ev->efd, 1);
vmpressure.c:	mutex_unlock(&vmpr->events_lock);
vmpressure.c:	spin_lock(&vmpr->sr_lock);
vmpressure.c:	 * vmpr->reclaimed is in sync.
vmpressure.c:	scanned = vmpr->tree_scanned;
vmpressure.c:		spin_unlock(&vmpr->sr_lock);
vmpressure.c:	reclaimed = vmpr->tree_reclaimed;
vmpressure.c:	vmpr->tree_scanned = 0;
vmpressure.c:	vmpr->tree_reclaimed = 0;
vmpressure.c:	spin_unlock(&vmpr->sr_lock);
vmpressure.c: * vmpressure() - Account memory pressure through scanned/reclaimed ratio
vmpressure.c: * only in-kernel users are notified.
vmpressure.c:	 * Indirect reclaim (kswapd) sets sc->gfp_mask to GFP_KERNEL, so
vmpressure.c:		spin_lock(&vmpr->sr_lock);
vmpressure.c:		scanned = vmpr->tree_scanned += scanned;
vmpressure.c:		vmpr->tree_reclaimed += reclaimed;
vmpressure.c:		spin_unlock(&vmpr->sr_lock);
vmpressure.c:		schedule_work(&vmpr->work);
vmpressure.c:		/* For now, no users for root-level efficiency */
vmpressure.c:		spin_lock(&vmpr->sr_lock);
vmpressure.c:		scanned = vmpr->scanned += scanned;
vmpressure.c:		reclaimed = vmpr->reclaimed += reclaimed;
vmpressure.c:			spin_unlock(&vmpr->sr_lock);
vmpressure.c:		vmpr->scanned = vmpr->reclaimed = 0;
vmpressure.c:		spin_unlock(&vmpr->sr_lock);
vmpressure.c:			memcg->socket_pressure = jiffies + HZ;
vmpressure.c: * vmpressure_prio() - Account memory pressure through reclaimer priority level
vmpressure.c:	return -1;
vmpressure.c:	return -1;
vmpressure.c: * vmpressure_register_event() - Bind vmpressure notifications to an eventfd
vmpressure.c: * @eventfd. The @args parameter is a comma-delimited string that denotes a
vmpressure.c:	enum vmpressure_levels level = -1;
vmpressure.c:		ret = -ENOMEM;
vmpressure.c:	if (level == -1) {
vmpressure.c:		ret = -EINVAL;
vmpressure.c:		if (mode == -1) {
vmpressure.c:			ret = -EINVAL;
vmpressure.c:		ret = -ENOMEM;
vmpressure.c:	ev->efd = eventfd;
vmpressure.c:	ev->level = level;
vmpressure.c:	ev->mode = mode;
vmpressure.c:	mutex_lock(&vmpr->events_lock);
vmpressure.c:	list_add(&ev->node, &vmpr->events);
vmpressure.c:	mutex_unlock(&vmpr->events_lock);
vmpressure.c: * vmpressure_unregister_event() - Unbind eventfd from vmpressure
vmpressure.c:	mutex_lock(&vmpr->events_lock);
vmpressure.c:	list_for_each_entry(ev, &vmpr->events, node) {
vmpressure.c:		if (ev->efd != eventfd)
vmpressure.c:		list_del(&ev->node);
vmpressure.c:	mutex_unlock(&vmpr->events_lock);
vmpressure.c: * vmpressure_init() - Initialize vmpressure control structure
vmpressure.c:	spin_lock_init(&vmpr->sr_lock);
vmpressure.c:	mutex_init(&vmpr->events_lock);
vmpressure.c:	INIT_LIST_HEAD(&vmpr->events);
vmpressure.c:	INIT_WORK(&vmpr->work, vmpressure_work_fn);
vmpressure.c: * vmpressure_cleanup() - shuts down vmpressure control structure
vmpressure.c:	flush_work(&vmpr->work);
zbud.c:#define CHUNK_SHIFT	(PAGE_SHIFT - NCHUNKS_ORDER)
zbud.c:#define NCHUNKS		((PAGE_SIZE - ZHDR_SIZE_ALIGNED) >> CHUNK_SHIFT)
zbud.c: * struct zbud_pool - stores metadata for each zbud pool
zbud.c: * struct zbud_header - zbud page metadata occupying the first chunk of each
zbud.c:	if (pool->zpool && pool->zpool_ops && pool->zpool_ops->evict)
zbud.c:		return pool->zpool_ops->evict(pool->zpool, handle);
zbud.c:		return -ENOENT;
zbud.c:		pool->zpool = zpool;
zbud.c:		pool->zpool_ops = zpool_ops;
zbud.c:	int ret = -EINVAL;
zbud.c:MODULE_ALIAS("zpool-zbud");
zbud.c:	return (size + CHUNK_SIZE - 1) >> CHUNK_SHIFT;
zbud.c:	zhdr->first_chunks = 0;
zbud.c:	zhdr->last_chunks = 0;
zbud.c:	INIT_LIST_HEAD(&zhdr->buddy);
zbud.c:	INIT_LIST_HEAD(&zhdr->lru);
zbud.c:	zhdr->under_reclaim = 0;
zbud.c:		handle += PAGE_SIZE - (zhdr->last_chunks  << CHUNK_SHIFT);
zbud.c:	return NCHUNKS - zhdr->first_chunks - zhdr->last_chunks;
zbud.c: * zbud_create_pool() - create a new zbud pool
zbud.c: * @ops:	user-defined operations for the zbud pool
zbud.c:	spin_lock_init(&pool->lock);
zbud.c:		INIT_LIST_HEAD(&pool->unbuddied[i]);
zbud.c:	INIT_LIST_HEAD(&pool->buddied);
zbud.c:	INIT_LIST_HEAD(&pool->lru);
zbud.c:	pool->pages_nr = 0;
zbud.c:	pool->ops = ops;
zbud.c: * zbud_destroy_pool() - destroys an existing zbud pool
zbud.c: * zbud_alloc() - allocates a region of a given size
zbud.c: * Return: 0 if success and handle is set, otherwise -EINVAL if the size or
zbud.c: * gfp arguments are invalid or -ENOMEM if the pool was unable to allocate
zbud.c:		return -EINVAL;
zbud.c:	if (size > PAGE_SIZE - ZHDR_SIZE_ALIGNED - CHUNK_SIZE)
zbud.c:		return -ENOSPC;
zbud.c:	spin_lock(&pool->lock);
zbud.c:		if (!list_empty(&pool->unbuddied[i])) {
zbud.c:			zhdr = list_first_entry(&pool->unbuddied[i],
zbud.c:			list_del(&zhdr->buddy);
zbud.c:			if (zhdr->first_chunks == 0)
zbud.c:	spin_unlock(&pool->lock);
zbud.c:		return -ENOMEM;
zbud.c:	spin_lock(&pool->lock);
zbud.c:	pool->pages_nr++;
zbud.c:		zhdr->first_chunks = chunks;
zbud.c:		zhdr->last_chunks = chunks;
zbud.c:	if (zhdr->first_chunks == 0 || zhdr->last_chunks == 0) {
zbud.c:		list_add(&zhdr->buddy, &pool->unbuddied[freechunks]);
zbud.c:		list_add(&zhdr->buddy, &pool->buddied);
zbud.c:	if (!list_empty(&zhdr->lru))
zbud.c:		list_del(&zhdr->lru);
zbud.c:	list_add(&zhdr->lru, &pool->lru);
zbud.c:	spin_unlock(&pool->lock);
zbud.c: * zbud_free() - frees the allocation associated with the given handle
zbud.c:	spin_lock(&pool->lock);
zbud.c:	if ((handle - ZHDR_SIZE_ALIGNED) & ~PAGE_MASK)
zbud.c:		zhdr->last_chunks = 0;
zbud.c:		zhdr->first_chunks = 0;
zbud.c:	if (zhdr->under_reclaim) {
zbud.c:		spin_unlock(&pool->lock);
zbud.c:	list_del(&zhdr->buddy);
zbud.c:	if (zhdr->first_chunks == 0 && zhdr->last_chunks == 0) {
zbud.c:		list_del(&zhdr->lru);
zbud.c:		pool->pages_nr--;
zbud.c:		list_add(&zhdr->buddy, &pool->unbuddied[freechunks]);
zbud.c:	spin_unlock(&pool->lock);
zbud.c: * zbud_reclaim_page() - evicts allocations from a pool page and frees it
zbud.c: * the user-defined eviction handler with the pool and handle as arguments.
zbud.c: * non-zero. zbud_reclaim_page() will add the zbud page back to the
zbud.c: * Returns: 0 if page is successfully freed, otherwise -EINVAL if there are
zbud.c: * no pages to evict or an eviction handler is not registered, -EAGAIN if
zbud.c:	spin_lock(&pool->lock);
zbud.c:	if (!pool->ops || !pool->ops->evict || list_empty(&pool->lru) ||
zbud.c:		spin_unlock(&pool->lock);
zbud.c:		return -EINVAL;
zbud.c:		zhdr = list_last_entry(&pool->lru, struct zbud_header, lru);
zbud.c:		list_del(&zhdr->lru);
zbud.c:		list_del(&zhdr->buddy);
zbud.c:		zhdr->under_reclaim = true;
zbud.c:		if (zhdr->first_chunks)
zbud.c:		if (zhdr->last_chunks)
zbud.c:		spin_unlock(&pool->lock);
zbud.c:			ret = pool->ops->evict(pool, first_handle);
zbud.c:			ret = pool->ops->evict(pool, last_handle);
zbud.c:		spin_lock(&pool->lock);
zbud.c:		zhdr->under_reclaim = false;
zbud.c:		if (zhdr->first_chunks == 0 && zhdr->last_chunks == 0) {
zbud.c:			pool->pages_nr--;
zbud.c:			spin_unlock(&pool->lock);
zbud.c:		} else if (zhdr->first_chunks == 0 ||
zbud.c:				zhdr->last_chunks == 0) {
zbud.c:			list_add(&zhdr->buddy, &pool->unbuddied[freechunks]);
zbud.c:			list_add(&zhdr->buddy, &pool->buddied);
zbud.c:		list_add(&zhdr->lru, &pool->lru);
zbud.c:	spin_unlock(&pool->lock);
zbud.c:	return -EAGAIN;
zbud.c: * zbud_map() - maps the allocation associated with the given handle
zbud.c: * zbud_unmap() - maps the allocation associated with the given handle
zbud.c: * zbud_get_pool_size() - gets the zbud pool size in pages
zbud.c:	return pool->pages_nr;
mm_init.c: * mm_init.c - Memory initialisation verification and debugging
mm_init.c:			zonelist = &pgdat->node_zonelists[listid];
mm_init.c:			zone = &pgdat->node_zones[zoneid];
mm_init.c:				zone->name);
mm_init.c:				pr_cont("%d:%s ", zone->node, zone->name);
mm_init.c:				pr_cont("0:%s ", zone->name);
mm_init.c:	width = shift - SECTIONS_WIDTH - NODES_WIDTH - ZONES_WIDTH - LAST_CPUPID_SHIFT;
mm_init.c:		"Node/Zone ID: %lu -> %lu\n",
mm_init.c:		"location: %d -> %d layout %d -> %d unused %d -> %d page-flags\n",
mm_init.c:		shift -= SECTIONS_WIDTH;
mm_init.c:		shift -= NODES_WIDTH;
mm_init.c:		shift -= ZONES_WIDTH;
mm_init.c:		return -ENOMEM;
kasan/report.c: * Some code borrowed from https://github.com/xairy/kasan-prototype by
kasan/report.c:	return (info->access_addr >=
kasan/report.c:	const char *bug_type = "unknown-crash";
kasan/report.c:	info->first_bad_addr = find_first_bad_addr(info->access_addr,
kasan/report.c:						info->access_size);
kasan/report.c:	shadow_addr = (u8 *)kasan_mem_to_shadow(info->first_bad_addr);
kasan/report.c:	if (*shadow_addr > 0 && *shadow_addr <= KASAN_SHADOW_SCALE_SIZE - 1)
kasan/report.c:	case 0 ... KASAN_SHADOW_SCALE_SIZE - 1:
kasan/report.c:		bug_type = "out-of-bounds";
kasan/report.c:		bug_type = "slab-out-of-bounds";
kasan/report.c:		bug_type = "global-out-of-bounds";
kasan/report.c:		bug_type = "stack-out-of-bounds";
kasan/report.c:		bug_type = "use-after-free";
kasan/report.c:		bug_type = "use-after-scope";
kasan/report.c:	const char *bug_type = "unknown-crash";
kasan/report.c:	if ((unsigned long)info->access_addr < PAGE_SIZE)
kasan/report.c:		bug_type = "null-ptr-deref";
kasan/report.c:	else if ((unsigned long)info->access_addr < TASK_SIZE)
kasan/report.c:		bug_type = "user-memory-access";
kasan/report.c:		bug_type = "wild-memory-access";
kasan/report.c:		bug_type, (void *)info->ip);
kasan/report.c:		info->is_write ? "Write" : "Read", info->access_size,
kasan/report.c:		info->access_addr, current->comm, task_pid_nr(current));
kasan/report.c:	pr_err("%s by task %u:\n", prefix, track->pid);
kasan/report.c:	if (track->stack) {
kasan/report.c:		depot_fetch_stack(track->stack, &trace);
kasan/report.c:		object, cache->name, cache->object_size);
kasan/report.c:		rel_bytes = object_addr - access_addr;
kasan/report.c:	} else if (access_addr >= object_addr + cache->object_size) {
kasan/report.c:		rel_bytes = access_addr - (object_addr + cache->object_size);
kasan/report.c:		rel_bytes = access_addr - object_addr;
kasan/report.c:	       " %d-byte region [%p, %p)\n",
kasan/report.c:		rel_bytes, rel_type, cache->object_size, (void *)object_addr,
kasan/report.c:		(void *)(object_addr + cache->object_size));
kasan/report.c:	if (cache->flags & SLAB_KASAN) {
kasan/report.c:		print_track(&alloc_info->alloc_track, "Allocated");
kasan/report.c:		print_track(&alloc_info->free_track, "Freed");
kasan/report.c:		struct kmem_cache *cache = page->slab_cache;
kasan/report.c:	return 3 + (BITS_PER_LONG/8)*2 + (shadow - row)*2 +
kasan/report.c:		(shadow - row) / SHADOW_BYTES_PER_BLOCK + 1;
kasan/report.c:		- SHADOW_ROWS_AROUND_ADDR * SHADOW_BYTES_PER_ROW;
kasan/report.c:	for (i = -SHADOW_ROWS_AROUND_ADDR; i <= SHADOW_ROWS_AROUND_ADDR; i++) {
kasan/report.c:	pr_err("BUG: KASAN: double-free or invalid-free in %pS\n", ip);
kasan/report.c:		print_address_description((void *)info->access_addr);
kasan/report.c:		print_shadow_for_address(info->first_bad_addr);
kasan/report.c:	if (current->kasan_depth)
nommu.c: *  See Documentation/nommu-mmap.txt
nommu.c: *  Copyright (c) 2004-2008 David Howells <dhowells@redhat.com>
nommu.c: *  Copyright (c) 2000-2003 David McCullough <davidm@snapgear.com>
nommu.c: *  Copyright (c) 2000-2001 D Jeff Dionne <jeff@uClinux.org>
nommu.c: *  Copyright (c) 2007-2010 Paul Mundt <lethal@linux-sh.org>
nommu.c:#include <linux/backing-dev.h>
nommu.c:	 * PAGE_SIZE for 0-order pages.
nommu.c:		vma = find_vma(current->mm, (unsigned long)objp);
nommu.c:			return vma->vm_end - vma->vm_start;
nommu.c:		if ((vma->vm_flags & (VM_IO | VM_PFNMAP)) ||
nommu.c:		    !(vm_flags & vma->vm_flags))
nommu.c:	return i ? : -EFAULT;
nommu.c: * - this is potentially dodgy as we may end incrementing the page count of a
nommu.c: * - don't permit access to VMAs that don't support it, such as I/O mappings
nommu.c:	return __get_user_pages(current, current->mm, start, nr_pages,
nommu.c:	down_read(&mm->mmap_sem);
nommu.c:	up_read(&mm->mmap_sem);
nommu.c:	return __get_user_pages_unlocked(current, current->mm, start, nr_pages,
nommu.c: * follow_pfn - look up PFN at a user virtual address
nommu.c: * Returns zero and the pfn at @pfn on success, -ve otherwise.
nommu.c:	if (!(vma->vm_flags & (VM_IO | VM_PFNMAP)))
nommu.c:		return -EINVAL;
nommu.c:		down_write(&current->mm->mmap_sem);
nommu.c:		vma = find_vma(current->mm, (unsigned long)ret);
nommu.c:			vma->vm_flags |= VM_USERMAP;
nommu.c:		up_write(&current->mm->mmap_sem);
nommu.c:		count = -(unsigned long) buf;
nommu.c:		count = -(unsigned long) addr;
nommu.c: *	vmalloc  -  allocate virtually contiguous memory
nommu.c: *	vzalloc - allocate virtually contiguous memory with zero fill
nommu.c: * vmalloc_node - allocate memory on a specific node
nommu.c: * vzalloc_node - allocate memory on a specific node with zero fill
nommu.c: *	vmalloc_exec  -  allocate virtually contiguous, executable memory
nommu.c: *	Kernel-internal function to allocate enough pages to cover @size
nommu.c: * vmalloc_32  -  allocate virtually contiguous memory (32bit addressable)
nommu.c: * vmalloc_32_user - allocate zeroed virtually contiguous 32bit memory
nommu.c:	 * We'll have to sort out the ZONE_DMA bits for 64-bit,
nommu.c: *	alloc_vm_area - allocate a range of kernel address space
nommu.c:	return -EINVAL;
nommu.c: *  like trying to un-brk an area that has already been mapped
nommu.c:	struct mm_struct *mm = current->mm;
nommu.c:	if (brk < mm->start_brk || brk > mm->context.end_brk)
nommu.c:		return mm->brk;
nommu.c:	if (mm->brk == brk)
nommu.c:		return mm->brk;
nommu.c:	if (brk <= mm->brk) {
nommu.c:		mm->brk = brk;
nommu.c:	 * Ok, looks good - let it rip.
nommu.c:	flush_icache_range(mm->brk, brk);
nommu.c:	return mm->brk = brk;
nommu.c: * - the caller must hold the region lock
nommu.c:	BUG_ON(last->vm_end <= last->vm_start);
nommu.c:	BUG_ON(last->vm_top < last->vm_end);
nommu.c:		BUG_ON(region->vm_end <= region->vm_start);
nommu.c:		BUG_ON(region->vm_top < region->vm_end);
nommu.c:		BUG_ON(region->vm_start < last->vm_top);
nommu.c:		if (region->vm_start < pregion->vm_start)
nommu.c:			p = &(*p)->rb_left;
nommu.c:		else if (region->vm_start > pregion->vm_start)
nommu.c:			p = &(*p)->rb_right;
nommu.c:	rb_link_node(&region->vm_rb, parent, p);
nommu.c:	rb_insert_color(&region->vm_rb, &nommu_region_tree);
nommu.c:	rb_erase(&region->vm_rb, &nommu_region_tree);
nommu.c: * - the caller must hold the region semaphore for writing, which this releases
nommu.c: * - the region may not have been added to the tree yet, in which case vm_top
nommu.c:	if (--region->vm_usage == 0) {
nommu.c:		if (region->vm_top > region->vm_start)
nommu.c:		if (region->vm_file)
nommu.c:			fput(region->vm_file);
nommu.c:		if (region->vm_flags & VM_MAPPED_COPY)
nommu.c:			free_page_series(region->vm_start, region->vm_top);
nommu.c:	struct mm_struct *mm = vma->vm_mm;
nommu.c:	long start = vma->vm_start & PAGE_MASK;
nommu.c:	while (start < vma->vm_end) {
nommu.c: * - should be called with mm->mmap_sem held writelocked
nommu.c:	BUG_ON(!vma->vm_region);
nommu.c:	mm->map_count++;
nommu.c:	vma->vm_mm = mm;
nommu.c:	protect_vma(vma, vma->vm_flags);
nommu.c:	if (vma->vm_file) {
nommu.c:		mapping = vma->vm_file->f_mapping;
nommu.c:		vma_interval_tree_insert(vma, &mapping->i_mmap);
nommu.c:	p = &mm->mm_rb.rb_node;
nommu.c:		if (vma->vm_start < pvma->vm_start)
nommu.c:			p = &(*p)->rb_left;
nommu.c:		else if (vma->vm_start > pvma->vm_start) {
nommu.c:			p = &(*p)->rb_right;
nommu.c:		} else if (vma->vm_end < pvma->vm_end)
nommu.c:			p = &(*p)->rb_left;
nommu.c:		else if (vma->vm_end > pvma->vm_end) {
nommu.c:			p = &(*p)->rb_right;
nommu.c:			p = &(*p)->rb_left;
nommu.c:			p = &(*p)->rb_right;
nommu.c:	rb_link_node(&vma->vm_rb, parent, p);
nommu.c:	rb_insert_color(&vma->vm_rb, &mm->mm_rb);
nommu.c:	struct mm_struct *mm = vma->vm_mm;
nommu.c:	mm->map_count--;
nommu.c:		if (curr->vmacache.vmas[i] == vma) {
nommu.c:	if (vma->vm_file) {
nommu.c:		mapping = vma->vm_file->f_mapping;
nommu.c:		vma_interval_tree_remove(vma, &mapping->i_mmap);
nommu.c:	rb_erase(&vma->vm_rb, &mm->mm_rb);
nommu.c:	if (vma->vm_prev)
nommu.c:		vma->vm_prev->vm_next = vma->vm_next;
nommu.c:		mm->mmap = vma->vm_next;
nommu.c:	if (vma->vm_next)
nommu.c:		vma->vm_next->vm_prev = vma->vm_prev;
nommu.c:	if (vma->vm_ops && vma->vm_ops->close)
nommu.c:		vma->vm_ops->close(vma);
nommu.c:	if (vma->vm_file)
nommu.c:		fput(vma->vm_file);
nommu.c:	put_nommu_region(vma->vm_region);
nommu.c: * - should be called with mm->mmap_sem at least held readlocked
nommu.c:	for (vma = mm->mmap; vma; vma = vma->vm_next) {
nommu.c:		if (vma->vm_start > addr)
nommu.c:		if (vma->vm_end > addr) {
nommu.c: * - we don't extend stack VMAs under NOMMU conditions
nommu.c: * - not supported under NOMMU conditions
nommu.c:	return -ENOMEM;
nommu.c: * - should be called with mm->mmap_sem at least held readlocked
nommu.c:	for (vma = mm->mmap; vma; vma = vma->vm_next) {
nommu.c:		if (vma->vm_start < addr)
nommu.c:		if (vma->vm_start > addr)
nommu.c:		if (vma->vm_end == end) {
nommu.c:		return -EINVAL;
nommu.c:		return -EINVAL;
nommu.c:		return -EINVAL;
nommu.c:		return -ENOMEM;
nommu.c:		return -EOVERFLOW;
nommu.c:		if (!file->f_op->mmap)
nommu.c:			return -ENODEV;
nommu.c:		 * - we support chardevs that provide their own "memory"
nommu.c:		 * - we support files/blockdevs that are memory backed
nommu.c:		if (file->f_op->mmap_capabilities) {
nommu.c:			capabilities = file->f_op->mmap_capabilities(file);
nommu.c:			switch (file_inode(file)->i_mode & S_IFMT) {
nommu.c:				return -EINVAL;
nommu.c:		if (!file->f_op->get_unmapped_area)
nommu.c:		if (!(file->f_mode & FMODE_CAN_READ))
nommu.c:		if (!(file->f_mode & FMODE_READ))
nommu.c:			return -EACCES;
nommu.c:			    !(file->f_mode & FMODE_WRITE))
nommu.c:				return -EACCES;
nommu.c:			    (file->f_mode & FMODE_WRITE))
nommu.c:				return -EACCES;
nommu.c:				return -EAGAIN;
nommu.c:				return -ENODEV;
nommu.c:				return -ENODEV;
nommu.c:					return -EINVAL;
nommu.c:		if (path_noexec(&file->f_path)) {
nommu.c:				return -EPERM;
nommu.c:			if (current->personality & READ_IMPLIES_EXEC) {
nommu.c:		    (current->personality & READ_IMPLIES_EXEC))
nommu.c:	/* vm_flags |= mm->def_flags; */
nommu.c:		/* attempt to share read-only copies of mapped file chunks */
nommu.c:		 * if possible - used for chardevs, ramfs/tmpfs/shmfs and
nommu.c:	 * it's being traced - otherwise breakpoints set in it may interfere
nommu.c:	if ((flags & MAP_PRIVATE) && current->ptrace)
nommu.c:	ret = call_mmap(vma->vm_file, vma);
nommu.c:		vma->vm_region->vm_top = vma->vm_region->vm_end;
nommu.c:	if (ret != -ENOSYS)
nommu.c:	/* getting -ENOSYS indicates that direct mmap isn't possible (as
nommu.c:	return -ENODEV;
nommu.c:	 * - VM_MAYSHARE will be set if it may attempt to share
nommu.c:		ret = call_mmap(vma->vm_file, vma);
nommu.c:			BUG_ON(!(vma->vm_flags & VM_MAYSHARE));
nommu.c:			vma->vm_region->vm_top = vma->vm_region->vm_end;
nommu.c:		if (ret != -ENOSYS)
nommu.c:	 * - note that this may not return a page-aligned address if the object
nommu.c:	/* we don't want to allocate a power-of-2 sized page set */
nommu.c:	if (sysctl_nr_trim_pages && total - point >= sysctl_nr_trim_pages)
nommu.c:	region->vm_flags = vma->vm_flags |= VM_MAPPED_COPY;
nommu.c:	region->vm_start = (unsigned long) base;
nommu.c:	region->vm_end   = region->vm_start + len;
nommu.c:	region->vm_top   = region->vm_start + (total << PAGE_SHIFT);
nommu.c:	vma->vm_start = region->vm_start;
nommu.c:	vma->vm_end   = region->vm_start + len;
nommu.c:	if (vma->vm_file) {
nommu.c:		fpos = vma->vm_pgoff;
nommu.c:		ret = kernel_read(vma->vm_file, base, len, &fpos);
nommu.c:			memset(base + ret, 0, len - ret);
nommu.c:	free_page_series(region->vm_start, region->vm_top);
nommu.c:	region->vm_start = vma->vm_start = 0;
nommu.c:	region->vm_end   = vma->vm_end = 0;
nommu.c:	region->vm_top   = 0;
nommu.c:	       len, current->pid, current->comm);
nommu.c:	return -ENOMEM;
nommu.c:	region->vm_usage = 1;
nommu.c:	region->vm_flags = vm_flags;
nommu.c:	region->vm_pgoff = pgoff;
nommu.c:	INIT_LIST_HEAD(&vma->anon_vma_chain);
nommu.c:	vma->vm_flags = vm_flags;
nommu.c:	vma->vm_pgoff = pgoff;
nommu.c:		region->vm_file = get_file(file);
nommu.c:		vma->vm_file = get_file(file);
nommu.c:	 * - we can only share with a superset match on most regular files
nommu.c:	 * - shared mappings on character devices and memory backed files are
nommu.c:		pglen = (len + PAGE_SIZE - 1) >> PAGE_SHIFT;
nommu.c:			if (!(pregion->vm_flags & VM_MAYSHARE))
nommu.c:			if (file_inode(pregion->vm_file) !=
nommu.c:			if (pregion->vm_pgoff >= pgend)
nommu.c:			rpglen = pregion->vm_end - pregion->vm_start;
nommu.c:			rpglen = (rpglen + PAGE_SIZE - 1) >> PAGE_SHIFT;
nommu.c:			rpgend = pregion->vm_pgoff + rpglen;
nommu.c:			if ((pregion->vm_pgoff != pgoff || rpglen != pglen) &&
nommu.c:			    !(pgoff >= pregion->vm_pgoff && pgend <= rpgend)) {
nommu.c:			pregion->vm_usage++;
nommu.c:			vma->vm_region = pregion;
nommu.c:			start = pregion->vm_start;
nommu.c:			start += (pgoff - pregion->vm_pgoff) << PAGE_SHIFT;
nommu.c:			vma->vm_start = start;
nommu.c:			vma->vm_end = start + len;
nommu.c:			if (pregion->vm_flags & VM_MAPPED_COPY)
nommu.c:				vma->vm_flags |= VM_MAPPED_COPY;
nommu.c:					vma->vm_region = NULL;
nommu.c:					vma->vm_start = 0;
nommu.c:					vma->vm_end = 0;
nommu.c:					pregion->vm_usage--;
nommu.c:			fput(region->vm_file);
nommu.c:		 * - this is the hook for quasi-memory character devices to
nommu.c:			addr = file->f_op->get_unmapped_area(file, addr, len,
nommu.c:				if (ret != -ENOSYS)
nommu.c:				ret = -ENODEV;
nommu.c:				vma->vm_start = region->vm_start = addr;
nommu.c:				vma->vm_end = region->vm_end = addr + len;
nommu.c:	vma->vm_region = region;
nommu.c:	 * - the region is filled in if NOMMU_MAP_DIRECT is still set
nommu.c:	if (file && vma->vm_flags & VM_SHARED)
nommu.c:	if (!vma->vm_file && !(flags & MAP_UNINITIALIZED))
nommu.c:		memset((void *)region->vm_start, 0,
nommu.c:		       region->vm_end - region->vm_start);
nommu.c:	result = vma->vm_start;
nommu.c:	current->mm->total_vm += len >> PAGE_SHIFT;
nommu.c:	add_vma_to_mm(current->mm, vma);
nommu.c:	if (vma->vm_flags & VM_EXEC && !region->vm_icache_flushed) {
nommu.c:		flush_icache_range(region->vm_start, region->vm_end);
nommu.c:		region->vm_icache_flushed = true;
nommu.c:	if (region->vm_file)
nommu.c:		fput(region->vm_file);
nommu.c:	if (vma->vm_file)
nommu.c:		fput(vma->vm_file);
nommu.c:	ret = -EINVAL;
nommu.c:			len, current->pid);
nommu.c:	return -ENOMEM;
nommu.c:			len, current->pid);
nommu.c:	return -ENOMEM;
nommu.c:	unsigned long retval = -EBADF;
nommu.c:		return -EFAULT;
nommu.c:		return -EINVAL;
nommu.c:	if (vma->vm_file)
nommu.c:		return -ENOMEM;
nommu.c:	if (mm->map_count >= sysctl_max_map_count)
nommu.c:		return -ENOMEM;
nommu.c:		return -ENOMEM;
nommu.c:		return -ENOMEM;
nommu.c:	*region = *vma->vm_region;
nommu.c:	new->vm_region = region;
nommu.c:	npages = (addr - vma->vm_start) >> PAGE_SHIFT;
nommu.c:		region->vm_top = region->vm_end = new->vm_end = addr;
nommu.c:		region->vm_start = new->vm_start = addr;
nommu.c:		region->vm_pgoff = new->vm_pgoff += npages;
nommu.c:	if (new->vm_ops && new->vm_ops->open)
nommu.c:		new->vm_ops->open(new);
nommu.c:	delete_nommu_region(vma->vm_region);
nommu.c:		vma->vm_region->vm_start = vma->vm_start = addr;
nommu.c:		vma->vm_region->vm_pgoff = vma->vm_pgoff += npages;
nommu.c:		vma->vm_region->vm_end = vma->vm_end = addr;
nommu.c:		vma->vm_region->vm_top = addr;
nommu.c:	add_nommu_region(vma->vm_region);
nommu.c:	add_nommu_region(new->vm_region);
nommu.c:	if (from > vma->vm_start)
nommu.c:		vma->vm_end = from;
nommu.c:		vma->vm_start = to;
nommu.c:	region = vma->vm_region;
nommu.c:	BUG_ON(region->vm_usage != 1);
nommu.c:	if (from > region->vm_start) {
nommu.c:		to = region->vm_top;
nommu.c:		region->vm_top = region->vm_end = from;
nommu.c:		region->vm_start = to;
nommu.c: * - under NOMMU conditions the chunk to be unmapped must be backed by a single
nommu.c:		return -EINVAL;
nommu.c:			pr_warn("munmap of memory not mmapped by process %d (%s): 0x%lx-0x%lx\n",
nommu.c:					current->pid, current->comm,
nommu.c:					start, start + len - 1);
nommu.c:		return -EINVAL;
nommu.c:	/* we're allowed to split an anonymous VMA but not a file-backed one */
nommu.c:	if (vma->vm_file) {
nommu.c:			if (start > vma->vm_start)
nommu.c:				return -EINVAL;
nommu.c:			if (end == vma->vm_end)
nommu.c:			vma = vma->vm_next;
nommu.c:		return -EINVAL;
nommu.c:		if (start == vma->vm_start && end == vma->vm_end)
nommu.c:		if (start < vma->vm_start || end > vma->vm_end)
nommu.c:			return -EINVAL;
nommu.c:			return -EINVAL;
nommu.c:		if (end != vma->vm_end && offset_in_page(end))
nommu.c:			return -EINVAL;
nommu.c:		if (start != vma->vm_start && end != vma->vm_end) {
nommu.c:	struct mm_struct *mm = current->mm;
nommu.c:	down_write(&mm->mmap_sem);
nommu.c:	up_write(&mm->mmap_sem);
nommu.c:	mm->total_vm = 0;
nommu.c:	while ((vma = mm->mmap)) {
nommu.c:		mm->mmap = vma->vm_next;
nommu.c:	return -ENOMEM;
nommu.c:		return (unsigned long) -EINVAL;
nommu.c:		return -EINVAL;
nommu.c:		return (unsigned long) -EINVAL;
nommu.c:	vma = find_vma_exact(current->mm, addr, old_len);
nommu.c:		return (unsigned long) -EINVAL;
nommu.c:	if (vma->vm_end != vma->vm_start + old_len)
nommu.c:		return (unsigned long) -EFAULT;
nommu.c:	if (vma->vm_flags & VM_MAYSHARE)
nommu.c:		return (unsigned long) -EPERM;
nommu.c:	if (new_len > vma->vm_region->vm_end - vma->vm_region->vm_start)
nommu.c:		return (unsigned long) -ENOMEM;
nommu.c:	/* all checks complete - do it */
nommu.c:	vma->vm_end = vma->vm_start + new_len;
nommu.c:	return vma->vm_start;
nommu.c:	down_write(&current->mm->mmap_sem);
nommu.c:	up_write(&current->mm->mmap_sem);
nommu.c:		return -EINVAL;
nommu.c:	vma->vm_flags |= VM_IO | VM_PFNMAP | VM_DONTEXPAND | VM_DONTDUMP;
nommu.c:	unsigned long vm_len = vma->vm_end - vma->vm_start;
nommu.c:	pfn += vma->vm_pgoff;
nommu.c:	return io_remap_pfn_range(vma, vma->vm_start, pfn, vm_len, vma->vm_page_prot);
nommu.c:	unsigned int size = vma->vm_end - vma->vm_start;
nommu.c:	if (!(vma->vm_flags & VM_USERMAP))
nommu.c:		return -EINVAL;
nommu.c:	vma->vm_start = (unsigned long)(addr + (pgoff << PAGE_SHIFT));
nommu.c:	vma->vm_end = vma->vm_start + size;
nommu.c:	return -ENOMEM;
nommu.c:	down_read(&mm->mmap_sem);
nommu.c:		if (addr + len >= vma->vm_end)
nommu.c:			len = vma->vm_end - addr;
nommu.c:		if (write && vma->vm_flags & VM_MAYWRITE)
nommu.c:		else if (!write && vma->vm_flags & VM_MAYREAD)
nommu.c:	up_read(&mm->mmap_sem);
nommu.c: * @access_remote_vm - access another process' address space
nommu.c: * - source/target buffer must be kernel space
nommu.c: * nommu_shrink_inode_mappings - Shrink the shared mappings on an inode
nommu.c:	high = (size + PAGE_SIZE - 1) >> PAGE_SHIFT;
nommu.c:	i_mmap_lock_read(inode->i_mapping);
nommu.c:	vma_interval_tree_foreach(vma, &inode->i_mapping->i_mmap, low, high) {
nommu.c:		/* found one - only interested if it's shared out of the page
nommu.c:		if (vma->vm_flags & VM_SHARED) {
nommu.c:			i_mmap_unlock_read(inode->i_mapping);
nommu.c:			return -ETXTBSY; /* not quite true, but near enough */
nommu.c:	/* reduce any regions that overlap the dead zone - if in existence,
nommu.c:	vma_interval_tree_foreach(vma, &inode->i_mapping->i_mmap, 0, ULONG_MAX) {
nommu.c:		if (!(vma->vm_flags & VM_SHARED))
nommu.c:		region = vma->vm_region;
nommu.c:		r_size = region->vm_top - region->vm_start;
nommu.c:		r_top = (region->vm_pgoff << PAGE_SHIFT) + r_size;
nommu.c:			region->vm_top -= r_top - newsize;
nommu.c:			if (region->vm_end > region->vm_top)
nommu.c:				region->vm_end = region->vm_top;
nommu.c:	i_mmap_unlock_read(inode->i_mapping);
nommu.c:	free_kbytes = global_zone_page_state(NR_FREE_PAGES) << (PAGE_SHIFT - 10);
nommu.c:	free_kbytes = global_zone_page_state(NR_FREE_PAGES) << (PAGE_SHIFT - 10);
list_lru.c:	list_add(&lru->list, &list_lrus);
list_lru.c:	list_del(&lru->list);
list_lru.c:	return !!lru->node[0].memcg_lrus;
list_lru.c:	lockdep_assert_held(&nlru->lock);
list_lru.c:	if (nlru->memcg_lrus && idx >= 0)
list_lru.c:		return nlru->memcg_lrus->lru[idx];
list_lru.c:	return &nlru->lru;
list_lru.c:	return page->mem_cgroup;
list_lru.c:	if (!nlru->memcg_lrus)
list_lru.c:		return &nlru->lru;
list_lru.c:		return &nlru->lru;
list_lru.c:	return &nlru->lru;
list_lru.c:	return &nlru->lru;
list_lru.c:	struct list_lru_node *nlru = &lru->node[nid];
list_lru.c:	spin_lock(&nlru->lock);
list_lru.c:		list_add_tail(item, &l->list);
list_lru.c:		l->nr_items++;
list_lru.c:		nlru->nr_items++;
list_lru.c:		spin_unlock(&nlru->lock);
list_lru.c:	spin_unlock(&nlru->lock);
list_lru.c:	struct list_lru_node *nlru = &lru->node[nid];
list_lru.c:	spin_lock(&nlru->lock);
list_lru.c:		l->nr_items--;
list_lru.c:		nlru->nr_items--;
list_lru.c:		spin_unlock(&nlru->lock);
list_lru.c:	spin_unlock(&nlru->lock);
list_lru.c:	list->nr_items--;
list_lru.c:	list->nr_items--;
list_lru.c:	struct list_lru_node *nlru = &lru->node[nid];
list_lru.c:	spin_lock(&nlru->lock);
list_lru.c:	count = l->nr_items;
list_lru.c:	spin_unlock(&nlru->lock);
list_lru.c:	nlru = &lru->node[nid];
list_lru.c:	return nlru->nr_items;
list_lru.c:	struct list_lru_node *nlru = &lru->node[nid];
list_lru.c:	spin_lock(&nlru->lock);
list_lru.c:	list_for_each_safe(item, n, &l->list) {
list_lru.c:		--*nr_to_walk;
list_lru.c:		ret = isolate(item, l, &nlru->lock, cb_arg);
list_lru.c:			assert_spin_locked(&nlru->lock);
list_lru.c:			nlru->nr_items--;
list_lru.c:			list_move_tail(item, &l->list);
list_lru.c:			assert_spin_locked(&nlru->lock);
list_lru.c:	spin_unlock(&nlru->lock);
list_lru.c:	isolated += __list_lru_walk_one(lru, nid, -1, isolate, cb_arg,
list_lru.c:	INIT_LIST_HEAD(&l->list);
list_lru.c:	l->nr_items = 0;
list_lru.c:		kfree(memcg_lrus->lru[i]);
list_lru.c:		memcg_lrus->lru[i] = l;
list_lru.c:	__memcg_destroy_list_lru_node(memcg_lrus, begin, i - 1);
list_lru.c:	return -ENOMEM;
list_lru.c:	nlru->memcg_lrus = kvmalloc(size * sizeof(void *), GFP_KERNEL);
list_lru.c:	if (!nlru->memcg_lrus)
list_lru.c:		return -ENOMEM;
list_lru.c:	if (__memcg_init_list_lru_node(nlru->memcg_lrus, 0, size)) {
list_lru.c:		kvfree(nlru->memcg_lrus);
list_lru.c:		return -ENOMEM;
list_lru.c:	__memcg_destroy_list_lru_node(nlru->memcg_lrus, 0, memcg_nr_cache_ids);
list_lru.c:	kvfree(nlru->memcg_lrus);
list_lru.c:	old = nlru->memcg_lrus;
list_lru.c:		return -ENOMEM;
list_lru.c:		return -ENOMEM;
list_lru.c:	 * Since list_lru_{add,del} may be called under an IRQ-safe lock,
list_lru.c:	 * we have to use IRQ-safe primitives here to avoid deadlock.
list_lru.c:	spin_lock_irq(&nlru->lock);
list_lru.c:	nlru->memcg_lrus = new;
list_lru.c:	spin_unlock_irq(&nlru->lock);
list_lru.c:	__memcg_destroy_list_lru_node(nlru->memcg_lrus, old_size, new_size);
list_lru.c:		if (memcg_init_list_lru_node(&lru->node[i]))
list_lru.c:	for (i = i - 1; i >= 0; i--) {
list_lru.c:		if (!lru->node[i].memcg_lrus)
list_lru.c:		memcg_destroy_list_lru_node(&lru->node[i]);
list_lru.c:	return -ENOMEM;
list_lru.c:		memcg_destroy_list_lru_node(&lru->node[i]);
list_lru.c:		if (memcg_update_list_lru_node(&lru->node[i],
list_lru.c:	for (i = i - 1; i >= 0; i--) {
list_lru.c:		if (!lru->node[i].memcg_lrus)
list_lru.c:		memcg_cancel_update_list_lru_node(&lru->node[i],
list_lru.c:	return -ENOMEM;
list_lru.c:		memcg_cancel_update_list_lru_node(&lru->node[i],
list_lru.c:	 * Since list_lru_{add,del} may be called under an IRQ-safe lock,
list_lru.c:	 * we have to use IRQ-safe primitives here to avoid deadlock.
list_lru.c:	spin_lock_irq(&nlru->lock);
list_lru.c:	list_splice_init(&src->list, &dst->list);
list_lru.c:	dst->nr_items += src->nr_items;
list_lru.c:	src->nr_items = 0;
list_lru.c:	spin_unlock_irq(&nlru->lock);
list_lru.c:		memcg_drain_list_lru_node(&lru->node[i], src_idx, dst_idx);
list_lru.c:	size_t size = sizeof(*lru->node) * nr_node_ids;
list_lru.c:	int err = -ENOMEM;
list_lru.c:	lru->node = kzalloc(size, GFP_KERNEL);
list_lru.c:	if (!lru->node)
list_lru.c:		spin_lock_init(&lru->node[i].lock);
list_lru.c:			lockdep_set_class(&lru->node[i].lock, key);
list_lru.c:		init_one_lru(&lru->node[i].lru);
list_lru.c:		kfree(lru->node);
list_lru.c:		lru->node = NULL;
list_lru.c:	if (!lru->node)
list_lru.c:	kfree(lru->node);
list_lru.c:	lru->node = NULL;
kasan/kasan.h:/* SPDX-License-Identifier: GPL-2.0 */
kasan/kasan.h:#define KASAN_SHADOW_MASK       (KASAN_SHADOW_SCALE_SIZE - 1)
kasan/kasan.h:	return (void *)(((unsigned long)shadow_addr - KASAN_SHADOW_OFFSET)
Makefile:# SPDX-License-Identifier: GPL-2.0
Makefile:# These files are disabled because they produce non-interesting and/or
Makefile:KCOV_INSTRUMENT_debug-pagealloc.o := n
Makefile:mmu-y			:= nommu.o
Makefile:mmu-$(CONFIG_MMU)	:= gup.o highmem.o memory.o mincore.o \
Makefile:			   page_vma_mapped.o pagewalk.o pgtable-generic.o \
Makefile:mmu-$(CONFIG_MMU)	+= process_vm_access.o
Makefile:obj-y			:= filemap.o mempool.o oom_kill.o \
Makefile:			   maccess.o page_alloc.o page-writeback.o \
Makefile:			   util.o mmzone.o vmstat.o backing-dev.o \
Makefile:			   debug.o $(mmu-y)
Makefile:obj-y += init-mm.o
Makefile:	obj-y		+= nobootmem.o
Makefile:	obj-y		+= bootmem.o
Makefile:obj-$(CONFIG_ADVISE_SYSCALLS)	+= fadvise.o
Makefile:	obj-$(CONFIG_ADVISE_SYSCALLS)	+= madvise.o
Makefile:obj-$(CONFIG_HAVE_MEMBLOCK) += memblock.o
Makefile:obj-$(CONFIG_SWAP)	+= page_io.o swap_state.o swapfile.o
Makefile:obj-$(CONFIG_FRONTSWAP)	+= frontswap.o
Makefile:obj-$(CONFIG_ZSWAP)	+= zswap.o
Makefile:obj-$(CONFIG_HAS_DMA)	+= dmapool.o
Makefile:obj-$(CONFIG_HUGETLBFS)	+= hugetlb.o
Makefile:obj-$(CONFIG_NUMA) 	+= mempolicy.o
Makefile:obj-$(CONFIG_SPARSEMEM)	+= sparse.o
Makefile:obj-$(CONFIG_SPARSEMEM_VMEMMAP) += sparse-vmemmap.o
Makefile:obj-$(CONFIG_SLOB) += slob.o
Makefile:obj-$(CONFIG_MMU_NOTIFIER) += mmu_notifier.o
Makefile:obj-$(CONFIG_KSM) += ksm.o
Makefile:obj-$(CONFIG_PAGE_POISONING) += page_poison.o
Makefile:obj-$(CONFIG_SLAB) += slab.o
Makefile:obj-$(CONFIG_SLUB) += slub.o
Makefile:obj-$(CONFIG_KASAN)	+= kasan/
Makefile:obj-$(CONFIG_FAILSLAB) += failslab.o
Makefile:obj-$(CONFIG_MEMORY_HOTPLUG) += memory_hotplug.o
Makefile:obj-$(CONFIG_MEMTEST)		+= memtest.o
Makefile:obj-$(CONFIG_MIGRATION) += migrate.o
Makefile:obj-$(CONFIG_QUICKLIST) += quicklist.o
Makefile:obj-$(CONFIG_TRANSPARENT_HUGEPAGE) += huge_memory.o khugepaged.o
Makefile:obj-$(CONFIG_PAGE_COUNTER) += page_counter.o
Makefile:obj-$(CONFIG_MEMCG) += memcontrol.o vmpressure.o
Makefile:obj-$(CONFIG_MEMCG_SWAP) += swap_cgroup.o
Makefile:obj-$(CONFIG_CGROUP_HUGETLB) += hugetlb_cgroup.o
Makefile:obj-$(CONFIG_MEMORY_FAILURE) += memory-failure.o
Makefile:obj-$(CONFIG_HWPOISON_INJECT) += hwpoison-inject.o
Makefile:obj-$(CONFIG_DEBUG_KMEMLEAK) += kmemleak.o
Makefile:obj-$(CONFIG_DEBUG_KMEMLEAK_TEST) += kmemleak-test.o
Makefile:obj-$(CONFIG_DEBUG_RODATA_TEST) += rodata_test.o
Makefile:obj-$(CONFIG_PAGE_OWNER) += page_owner.o
Makefile:obj-$(CONFIG_CLEANCACHE) += cleancache.o
Makefile:obj-$(CONFIG_MEMORY_ISOLATION) += page_isolation.o
Makefile:obj-$(CONFIG_ZPOOL)	+= zpool.o
Makefile:obj-$(CONFIG_ZBUD)	+= zbud.o
Makefile:obj-$(CONFIG_ZSMALLOC)	+= zsmalloc.o
Makefile:obj-$(CONFIG_Z3FOLD)	+= z3fold.o
Makefile:obj-$(CONFIG_GENERIC_EARLY_IOREMAP) += early_ioremap.o
Makefile:obj-$(CONFIG_CMA)	+= cma.o
Makefile:obj-$(CONFIG_MEMORY_BALLOON) += balloon_compaction.o
Makefile:obj-$(CONFIG_PAGE_EXTENSION) += page_ext.o
Makefile:obj-$(CONFIG_CMA_DEBUGFS) += cma_debug.o
Makefile:obj-$(CONFIG_USERFAULTFD) += userfaultfd.o
Makefile:obj-$(CONFIG_IDLE_PAGE_TRACKING) += page_idle.o
Makefile:obj-$(CONFIG_FRAME_VECTOR) += frame_vector.o
Makefile:obj-$(CONFIG_DEBUG_PAGE_REF) += debug_page_ref.o
Makefile:obj-$(CONFIG_HARDENED_USERCOPY) += usercopy.o
Makefile:obj-$(CONFIG_PERCPU_STATS) += percpu-stats.o
Makefile:obj-$(CONFIG_HMM) += hmm.o
kasan/kasan.c: * Some code borrowed from https://github.com/xairy/kasan-prototype by
kasan/kasan.c:	current->kasan_depth++;
kasan/kasan.c:	current->kasan_depth--;
kasan/kasan.c:	memset(shadow_start, value, shadow_end - shadow_start);
kasan/kasan.c:	size_t size = sp - base;
kasan/kasan.c:	void *base = (void *)((unsigned long)watermark & ~(THREAD_SIZE - 1));
kasan/kasan.c:	kasan_unpoison_shadow(base, watermark - base);
kasan/kasan.c: * watermark value, as is sometimes required prior to hand-crafted asm function
kasan/kasan.c:	size_t size = watermark - sp;
kasan/kasan.c:	 * Access crosses 8(shadow size)-byte boundary. Such access maps
kasan/kasan.c:	if (unlikely(((addr + size - 1) & KASAN_SHADOW_MASK) < size - 1))
kasan/kasan.c:		return *shadow_addr || memory_is_poisoned_1(addr + size - 1);
kasan/kasan.c:	return memory_is_poisoned_1(addr + size - 1);
kasan/kasan.c:	/* Unaligned 16-bytes access maps into 3 shadow bytes. */
kasan/kasan.c:		size--;
kasan/kasan.c:	if (end - start <= 16)
kasan/kasan.c:		return bytes_is_nonzero(start, end - start);
kasan/kasan.c:		prefix = 8 - prefix;
kasan/kasan.c:	words = (end - start) / 8;
kasan/kasan.c:		words--;
kasan/kasan.c:	return bytes_is_nonzero(start, (end - start) % 8);
kasan/kasan.c:			kasan_mem_to_shadow((void *)addr + size - 1) + 1);
kasan/kasan.c:		unsigned long last_byte = addr + size - 1;
kasan/kasan.c:		object_size <= 64        - 16   ? 16 :
kasan/kasan.c:		object_size <= 128       - 32   ? 32 :
kasan/kasan.c:		object_size <= 512       - 64   ? 64 :
kasan/kasan.c:		object_size <= 4096      - 128  ? 128 :
kasan/kasan.c:		object_size <= (1 << 14) - 256  ? 256 :
kasan/kasan.c:		object_size <= (1 << 15) - 512  ? 512 :
kasan/kasan.c:		object_size <= (1 << 16) - 1024 ? 1024 : 2048;
kasan/kasan.c:	cache->kasan_info.alloc_meta_offset = *size;
kasan/kasan.c:	if (cache->flags & SLAB_TYPESAFE_BY_RCU || cache->ctor ||
kasan/kasan.c:	    cache->object_size < sizeof(struct kasan_free_meta)) {
kasan/kasan.c:		cache->kasan_info.free_meta_offset = *size;
kasan/kasan.c:	redzone_adjust = optimal_redzone(cache->object_size) -
kasan/kasan.c:		(*size - cache->object_size);
kasan/kasan.c:	*size = min(KMALLOC_MAX_SIZE, max(*size, cache->object_size +
kasan/kasan.c:					optimal_redzone(cache->object_size)));
kasan/kasan.c:	if (*size <= cache->kasan_info.alloc_meta_offset ||
kasan/kasan.c:			*size <= cache->kasan_info.free_meta_offset) {
kasan/kasan.c:		cache->kasan_info.alloc_meta_offset = 0;
kasan/kasan.c:		cache->kasan_info.free_meta_offset = 0;
kasan/kasan.c:	return (cache->kasan_info.alloc_meta_offset ?
kasan/kasan.c:		(cache->kasan_info.free_meta_offset ?
kasan/kasan.c:	kasan_unpoison_shadow(object, cache->object_size);
kasan/kasan.c:			round_up(cache->object_size, KASAN_SHADOW_SCALE_SIZE),
kasan/kasan.c:	if (!trace->nr_entries)
kasan/kasan.c:	for (i = 0; i < trace->nr_entries; i++)
kasan/kasan.c:		if (in_irqentry_text(trace->entries[i])) {
kasan/kasan.c:			trace->nr_entries = i + 1;
kasan/kasan.c:	    trace.entries[trace.nr_entries-1] == ULONG_MAX)
kasan/kasan.c:		trace.nr_entries--;
kasan/kasan.c:	track->pid = current->pid;
kasan/kasan.c:	track->stack = save_stack(flags);
kasan/kasan.c:	return (void *)object + cache->kasan_info.alloc_meta_offset;
kasan/kasan.c:	return (void *)object + cache->kasan_info.free_meta_offset;
kasan/kasan.c:	if (!(cache->flags & SLAB_KASAN))
kasan/kasan.c:	kasan_kmalloc(cache, object, cache->object_size, flags);
kasan/kasan.c:	unsigned long size = cache->object_size;
kasan/kasan.c:	if (unlikely(cache->flags & SLAB_TYPESAFE_BY_RCU))
kasan/kasan.c:	if (unlikely(cache->flags & SLAB_TYPESAFE_BY_RCU))
kasan/kasan.c:	if (unlikely(!(cache->flags & SLAB_KASAN)))
kasan/kasan.c:	set_track(&get_alloc_info(cache, object)->free_track, GFP_NOWAIT);
kasan/kasan.c:	redzone_end = round_up((unsigned long)object + cache->object_size,
kasan/kasan.c:	kasan_poison_shadow((void *)redzone_start, redzone_end - redzone_start,
kasan/kasan.c:	if (cache->flags & SLAB_KASAN)
kasan/kasan.c:		set_track(&get_alloc_info(cache, object)->alloc_track, flags);
kasan/kasan.c:	kasan_poison_shadow((void *)redzone_start, redzone_end - redzone_start,
kasan/kasan.c:		kasan_kmalloc(page->slab_cache, object, size, flags);
kasan/kasan.c:		kasan_poison_slab_free(page->slab_cache, ptr);
kasan/kasan.c:		return -EINVAL;
kasan/kasan.c:		find_vm_area(addr)->flags |= VM_KASAN;
kasan/kasan.c:	return -ENOMEM;
kasan/kasan.c:	if (vm->flags & VM_KASAN)
kasan/kasan.c:		vfree(kasan_mem_to_shadow(vm->addr));
kasan/kasan.c:	size_t aligned_size = round_up(global->size, KASAN_SHADOW_SCALE_SIZE);
kasan/kasan.c:	kasan_unpoison_shadow(global->beg, global->size);
kasan/kasan.c:	kasan_poison_shadow(global->beg + aligned_size,
kasan/kasan.c:		global->size_with_redzone - aligned_size,
kasan/kasan.c:	 * Addr is KASAN_SHADOW_SCALE_SIZE-aligned and the object is surrounded
kasan/kasan.c:	 * arch-specific, the last one depends on HUGETLB_PAGE.  So let's abuse
kasan/kasan.c:	nr_shadow_pages = mem_data->nr_pages >> KASAN_SHADOW_SCALE_SHIFT;
kasan/kasan.c:	start_kaddr = (unsigned long)pfn_to_kaddr(mem_data->start_pfn);
kasan/kasan.c:	if (WARN_ON(mem_data->nr_pages % KASAN_SHADOW_SCALE_SIZE) ||
kasan/kasan.c:					pfn_to_nid(mem_data->start_pfn),
kasan/kasan.c:		 * Non-NULL result of the find_vm_area() will tell us if
gup.c:	 * be zero-filled if handle_mm_fault() actually did handle it.
gup.c:	if ((flags & FOLL_DUMP) && (!vma->vm_ops || !vma->vm_ops->fault))
gup.c:		return ERR_PTR(-EFAULT);
gup.c:		return -EFAULT;
gup.c:			set_pte_at(vma->vm_mm, address, pte, entry);
gup.c:	return -EEXIST;
gup.c:	struct mm_struct *mm = vma->vm_mm;
gup.c:			page = ERR_PTR(-EFAULT);
gup.c:	if ((flags & FOLL_MLOCK) && (vma->vm_flags & VM_LOCKED)) {
gup.c:		/* Do not mlock pte-mapped THP */
gup.c:		 * handle it now - vmscan will handle it later if and
gup.c:		if (page->mapping && trylock_page(page)) {
gup.c:			 * need to check for file-cache page truncation.
gup.c:	struct mm_struct *mm = vma->vm_mm;
gup.c:	if (pmd_huge(*pmd) && vma->vm_flags & VM_HUGETLB) {
gup.c:				ret = -EBUSY;
gup.c:	*page_mask = HPAGE_PMD_NR - 1;
gup.c:	struct mm_struct *mm = vma->vm_mm;
gup.c:	if (pud_huge(*pud) && vma->vm_flags & VM_HUGETLB) {
gup.c: * follow_page_mask - look up a page descriptor from a user-virtual address
gup.c:	struct mm_struct *mm = vma->vm_mm;
gup.c:	int ret = -EFAULT;
gup.c:	/* user gate pages are read-only */
gup.c:		return -EFAULT;
gup.c:		return -EFAULT;
gup.c: * If it is, *@nonblocking will be set to 0 and -EBUSY returned.
gup.c:		return -ENOENT;
gup.c:			tsk->maj_flt++;
gup.c:			tsk->min_flt++;
gup.c:		return -EBUSY;
gup.c:	if ((ret & VM_FAULT_WRITE) && !(vma->vm_flags & VM_WRITE))
gup.c:	vm_flags_t vm_flags = vma->vm_flags;
gup.c:		return -EFAULT;
gup.c:		return -EFAULT;
gup.c:				return -EFAULT;
gup.c:			 * set a breakpoint in a read-only mapping of an
gup.c:				return -EFAULT;
gup.c:			return -EFAULT;
gup.c:			return -EFAULT;
gup.c:		return -EFAULT;
gup.c: * __get_user_pages() - pin user pages in memory
gup.c: * were pinned, returns -errno. Each page returned must be released
gup.c:		if (!vma || start >= vma->vm_end) {
gup.c:				return i ? : -EFAULT;
gup.c:			return i ? i : -ERESTARTSYS;
gup.c:			case -EFAULT:
gup.c:			case -ENOMEM:
gup.c:			case -EHWPOISON:
gup.c:			case -EBUSY:
gup.c:			case -ENOENT:
gup.c:		} else if (PTR_ERR(page) == -EEXIST) {
gup.c:		nr_pages -= page_increm;
gup.c:	if (!(vm_flags & vma->vm_flags))
gup.c: * fixup_user_fault() - manually resolve a user page fault
gup.c: * section), this returns -EFAULT, and we want to resolve the user fault before
gup.c: * same semantics wrt the @mm->mmap_sem as does filemap_fault().
gup.c:	if (!vma || address < vma->vm_start)
gup.c:		return -EFAULT;
gup.c:		return -EFAULT;
gup.c:		down_read(&mm->mmap_sem);
gup.c:			tsk->maj_flt++;
gup.c:			tsk->min_flt++;
gup.c:			nr_pages -= ret;
gup.c:		down_read(&mm->mmap_sem);
gup.c:		nr_pages--;
gup.c:		up_read(&mm->mmap_sem);
gup.c: *      down_read(&mm->mmap_sem);
gup.c: *      up_read(&mm->mmap_sem);
gup.c: *      down_read(&mm->mmap_sem);
gup.c: *          up_read(&mm->mmap_sem);
gup.c:	return __get_user_pages_locked(current, current->mm, start, nr_pages,
gup.c: * is set implicitly if "pages" is non-NULL.
gup.c:	down_read(&mm->mmap_sem);
gup.c:		up_read(&mm->mmap_sem);
gup.c: *      down_read(&mm->mmap_sem);
gup.c: *      up_read(&mm->mmap_sem);
gup.c:	return __get_user_pages_unlocked(current, current->mm, start, nr_pages,
gup.c: * get_user_pages_remote() - pin user pages in memory
gup.c: * were pinned, returns -errno. Each page returned must be released
gup.c: * get_user_pages is typically used for fewer-copy IO operations, to get a
gup.c: * less-flexible calling convention where we assume that the task
gup.c:	return __get_user_pages_locked(current, current->mm, start, nr_pages,
gup.c: * longterm elevated page reference counts. For example, filesystem-dax
gup.c:		return -EINVAL;
gup.c:			return -ENOMEM;
gup.c:	rc = -EOPNOTSUPP;
gup.c: * populate_vma_page_range() -  populate a range of pages in the vma.
gup.c: * vma->vm_mm->mmap_sem must be held.
gup.c: * If @nonblocking is non-NULL, it must held for read only and may be
gup.c:	struct mm_struct *mm = vma->vm_mm;
gup.c:	unsigned long nr_pages = (end - start) / PAGE_SIZE;
gup.c:	VM_BUG_ON_VMA(start < vma->vm_start, vma);
gup.c:	VM_BUG_ON_VMA(end   > vma->vm_end, vma);
gup.c:	VM_BUG_ON_MM(!rwsem_is_locked(&mm->mmap_sem), mm);
gup.c:	if (vma->vm_flags & VM_LOCKONFAULT)
gup.c:	if ((vma->vm_flags & (VM_WRITE | VM_SHARED)) == VM_WRITE)
gup.c:	if (vma->vm_flags & (VM_READ | VM_WRITE | VM_EXEC))
gup.c: * __mm_populate - populate and/or mlock pages within a range of address space.
gup.c:	struct mm_struct *mm = current->mm;
gup.c:			down_read(&mm->mmap_sem);
gup.c:		} else if (nstart >= vma->vm_end)
gup.c:			vma = vma->vm_next;
gup.c:		if (!vma || vma->vm_start >= end)
gup.c:		nend = min(end, vma->vm_end);
gup.c:		if (vma->vm_flags & (VM_IO | VM_PFNMAP))
gup.c:		if (nstart < vma->vm_start)
gup.c:			nstart = vma->vm_start;
gup.c:		up_read(&mm->mmap_sem);
gup.c: * get_dump_page() - pin user page in memory while writing it to core dump
gup.c: * Returns NULL on any kind of failure - a hole must then be inserted into
gup.c: * NULL wherever the ZERO_PAGE, or an anonymous pte_none, has been found -
gup.c:	if (__get_user_pages(current, current->mm, addr, 1,
gup.c:	while ((*nr) - nr_start) {
gup.c:		struct page *page = pages[--(*nr)];
gup.c:		*nr -= refs;
gup.c:		*nr -= refs;
gup.c:		while (refs--)
gup.c:		*nr -= refs;
gup.c:		*nr -= refs;
gup.c:		while (refs--)
gup.c:		*nr -= refs;
gup.c:		*nr -= refs;
gup.c:		while (refs--)
gup.c:	pgdp = pgd_offset(current->mm, addr);
gup.c: * Like get_user_pages_fast() except it's IRQ-safe in that it won't fall back to
gup.c: * the regular GUP. It will only return non-negative values.
gup.c:	 * freed from under us. See mmu_gather_tlb in asm-generic/tlb.h
gup.c: * get_user_pages_fast() - pin user pages in memory
gup.c: * Attempt to pin user pages in memory without taking mm->mmap_sem.
gup.c: * were pinned, returns -errno.
gup.c:		return -EFAULT;
gup.c:		ret = get_user_pages_unlocked(start, nr_pages - nr, pages,
kasan/kasan_init.c: *   - It used as early shadow memory. The entire shadow region populated
kasan/kasan_init.c: *   - Latter it reused it as zero shadow to cover large ranges of memory
kasan/kasan_init.c:		if (IS_ALIGNED(addr, PMD_SIZE) && end - addr >= PMD_SIZE) {
kasan/kasan_init.c:		if (IS_ALIGNED(addr, PUD_SIZE) && end - addr >= PUD_SIZE) {
kasan/kasan_init.c:		if (IS_ALIGNED(addr, P4D_SIZE) && end - addr >= P4D_SIZE) {
kasan/kasan_init.c: * kasan_populate_zero_shadow - populate shadow memory region with
kasan/kasan_init.c: * @shadow_start - start of the memory range to populate
kasan/kasan_init.c: * @shadow_end   - end of the memory range to populate
kasan/kasan_init.c:		if (IS_ALIGNED(addr, PGDIR_SIZE) && end - addr >= PGDIR_SIZE) {
kasan/kasan_init.c:			 * 3,2 - level page tables where we don't have
kasan/kasan_init.c:			 * With 5level-fixup.h, pgd_populate() is not nop and
kasan/kasan_init.c:			 * unless 5-level paging enabled.
kasan/kasan_init.c:			 * The ifndef can be dropped once all KASAN-enabled
kasan/kasan_init.c:			 * architectures will switch to pgtable-nop4d.h.
kasan/quarantine.c: * Each queue is a signle-linked list, which also stores the total size of
kasan/quarantine.c:	return !q->head;
kasan/quarantine.c:	q->head = q->tail = NULL;
kasan/quarantine.c:	q->bytes = 0;
kasan/quarantine.c:		q->head = qlink;
kasan/quarantine.c:		q->tail->next = qlink;
kasan/quarantine.c:	q->tail = qlink;
kasan/quarantine.c:	qlink->next = NULL;
kasan/quarantine.c:	q->bytes += size;
kasan/quarantine.c:	to->tail->next = from->head;
kasan/quarantine.c:	to->tail = from->tail;
kasan/quarantine.c:	to->bytes += from->bytes;
kasan/quarantine.c: * The object quarantine consists of per-cpu queues and a global queue,
kasan/quarantine.c:/* Round-robin FIFO array of batches. */
kasan/quarantine.c:	return virt_to_head_page(qlink)->slab_cache;
kasan/quarantine.c:	return ((void *)free_info) - cache->kasan_info.free_meta_offset;
kasan/quarantine.c:	qlink = q->head;
kasan/quarantine.c:		struct qlist_node *next = qlink->next;
kasan/quarantine.c:	 * beginning which ensures that it either sees the objects in per-cpu
kasan/quarantine.c:	qlist_put(q, &info->quarantine_link, cache->size);
kasan/quarantine.c:	if (unlikely(q->bytes > QUARANTINE_PERCPU_SIZE)) {
kasan/quarantine.c:	 * the installed memory to quarantine minus per-cpu queue limits.
kasan/quarantine.c:		0 : total_size - percpu_quarantines;
kasan/quarantine.c:		WRITE_ONCE(quarantine_size, quarantine_size - to_free.bytes);
kasan/quarantine.c:	curr = from->head;
kasan/quarantine.c:		struct qlist_node *next = curr->next;
kasan/quarantine.c:			qlist_put(to, curr, obj_cache->size);
kasan/quarantine.c:			qlist_put(from, curr, obj_cache->size);
kasan/quarantine.c:	 * per-cpu list to the global quarantine in quarantine_put(),
kmemleak-test.c: * mm/kmemleak-test.c
kmemleak-test.c: * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
kmemleak-test.c:			return -ENOMEM;
kmemleak-test.c:		INIT_LIST_HEAD(&elem->list);
kmemleak-test.c:		list_add_tail(&elem->list, &test_list);
kmemleak-test.c:		list_del(&elem->list);
kasan/Makefile:# SPDX-License-Identifier: GPL-2.0
kasan/Makefile:CFLAGS_REMOVE_kasan.o = -pg
kasan/Makefile:CFLAGS_kasan.o := $(call cc-option, -fno-conserve-stack -fno-stack-protector)
kasan/Makefile:obj-y := kasan.o report.o kasan_init.o quarantine.o
mmzone.c:// SPDX-License-Identifier: GPL-2.0
mmzone.c:	int nid = next_online_node(pgdat->node_id);
mmzone.c: * next_zone - helper magic for for_each_zone()
mmzone.c:	pg_data_t *pgdat = zone->zone_pgdat;
mmzone.c:	if (zone < pgdat->node_zones + MAX_NR_ZONES - 1)
mmzone.c:			zone = pgdat->node_zones;
mmzone.c:				(z->zone && !zref_in_nodemask(z, nodes)))
mmzone.c:		INIT_LIST_HEAD(&lruvec->lists[lru]);
mmzone.c:		old_flags = flags = page->flags;
mmzone.c:	} while (unlikely(cmpxchg(&page->flags, old_flags, flags) != old_flags));
